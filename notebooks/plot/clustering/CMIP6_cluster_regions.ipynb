{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7716ffb9-a564-44c7-87d4-2d4caeecdb1c",
   "metadata": {},
   "source": [
    "# CMIP6 Cluster Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5a4f3-40bb-459a-88f0-f180c8b3ee8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854af429-43a1-489d-8513-dca8c1466663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== Packages ==========\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import dask\n",
    "import matplotlib.cm\n",
    "from matplotlib import rcParams\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from cftime import DatetimeNoLeap\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams[\"mathtext.default\"] = 'regular'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5daa16-4811-4773-b98d-6b69626ff6cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb57b43-bbbe-414b-8075-8cda5d1ea4ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9326b-048c-48c2-84e0-5af87d06a120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Helper function to open the dataset ========\n",
    "def open_dataset(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ca849-c1f4-431c-ae0f-e6a3fb6ba92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to open and merge datasets\n",
    "def open_and_merge_datasets(folder, model, experiment_id, variables):\n",
    "    filepaths = []\n",
    "    for var in variables:\n",
    "        path = f'../../data/CMIP6/{experiment_id}/{folder}/{var}'\n",
    "        fp = glob.glob(os.path.join(path, f'CMIP.{model}.{experiment_id}.{var}_regridded.nc'))\n",
    "        if fp:\n",
    "            filepaths.append(fp[0])\n",
    "        else:\n",
    "            #print(f\"No file found for variable '{var}' in model '{model}'.\")\n",
    "            print(fp)\n",
    "\n",
    "    datasets = [xr.open_dataset(fp) for fp in filepaths]\n",
    "    ds = xr.merge(datasets)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9148cef-f033-40ac-aeef-dec5ccdb277d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0755d44-865c-439e-b702-f2ccb30e1010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_var(ds_dict, var):\n",
    "    for name, ds in ds_dict.items():\n",
    "        ds_dict[name] = ds.drop(var)\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba91dd1-a3c7-4ef4-9b93-4116470f0d84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_period(ds_dict, start_year=None, end_year=None, period=None, yearly_sum=False):\n",
    "    '''\n",
    "    Helper function to select periods and optionally compute yearly sums.\n",
    "    \n",
    "    Parameters:\n",
    "    ds_dict (dict): Dictionary with xarray datasets.\n",
    "    start_year (int): The start year of the period.\n",
    "    end_year (int): The end year of the period.\n",
    "    period (int, list, str, None): Single month (int), list of months (list), multiple seasons (str) to select,\n",
    "                                   or None to not select any specific period.\n",
    "    yearly_sum (bool): If True, compute the yearly sum over the selected period.\n",
    "    '''\n",
    "    \n",
    "    # Create a deep copy of the original ds_dict to avoid modifying it directly\n",
    "    ds_dict_copy = copy.deepcopy(ds_dict)\n",
    "\n",
    "    # Define season to month mapping for northern hemisphere\n",
    "    seasons_to_months = {\n",
    "        'nh_winter': [12, 1, 2],\n",
    "        'nh_spring': [3, 4, 5],\n",
    "        'nh_summer': [6, 7, 8],\n",
    "        'nh_fall': [9, 10, 11]\n",
    "    }\n",
    "    \n",
    "    # Define month name mapping\n",
    "    month_names = {\n",
    "        1: 'J', 2: 'F', 3: 'M', 4: 'A', 5: 'M', 6: 'J',\n",
    "        7: 'J', 8: 'A', 9: 'S', 10: 'O', 11: 'N', 12: 'D'\n",
    "    }\n",
    "\n",
    "    # Define number of days per month (assuming 28 days for February)\n",
    "    days_per_month = {\n",
    "        1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30,\n",
    "        7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31\n",
    "    }\n",
    "\n",
    "    months = []\n",
    "\n",
    "    # If no specific period is selected, all data will be used.\n",
    "    if period is None:\n",
    "        period_name = 'whole_year'\n",
    "        months = list(range(1, 13))  # All months\n",
    "    elif isinstance(period, int):\n",
    "        period_name = month_names[period]\n",
    "        months = [period]\n",
    "    elif isinstance(period, str):\n",
    "        # Check if the input is a single season or multiple seasons\n",
    "        if 'and' in period:\n",
    "            seasons = period.lower().split('and')\n",
    "            period_name = ''\n",
    "            for season in seasons:\n",
    "                season = season.strip()\n",
    "                months.extend(seasons_to_months.get(season, []))\n",
    "                period_name += ''.join(month_names[m] for m in seasons_to_months.get(season, []))\n",
    "        else:\n",
    "            months = seasons_to_months.get(period.lower(), [])\n",
    "            period_name = ''.join(month_names[m] for m in months)\n",
    "    elif isinstance(period, list):\n",
    "        period_name = ''.join(month_names[m] for m in period if m in month_names)\n",
    "        months = period\n",
    "    else:\n",
    "        raise ValueError(\"Period must be None, an integer, a string representing a single season, \"\n",
    "                         \"a string with multiple seasons separated by 'and', or a list of integers.\")\n",
    "\n",
    "    for k, ds in ds_dict_copy.items():\n",
    "        if start_year and end_year:\n",
    "            start_date = f'{start_year}-01-01'\n",
    "            end_date = f'{end_year}-12-31'\n",
    "            ds = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # If months are specified, select those months\n",
    "        if months:\n",
    "            month_mask = ds['time.month'].isin(months)\n",
    "            ds = ds.where(month_mask, drop=True)\n",
    "\n",
    "        # Store the original attributes of each variable\n",
    "        original_attrs = {var: ds[var].attrs for var in ds.data_vars}\n",
    "\n",
    "        # If yearly_sum is True, sum over 'time' dimension to get yearly sum\n",
    "        if yearly_sum: # does only make sense for accumulative variables e.g. pr or tran\n",
    "            attrs = ds.attrs\n",
    "            # Multiply each value by the number of days in the respective month\n",
    "            days = ds['time'].dt.days_in_month\n",
    "            ds = (ds * days).resample(time='AS').sum(dim='time')\n",
    "            sum_type = 'yearly_sum'\n",
    "            ds.attrs = attrs\n",
    "        else:\n",
    "            sum_type = 'monthly_mean'\n",
    "\n",
    "        # Reassign the original attributes back to each variable\n",
    "        for var in ds.data_vars:\n",
    "            ds[var].attrs = original_attrs[var]\n",
    "\n",
    "        ds_dict_copy[k] = ds\n",
    "        ds_dict_copy[k].attrs['months'] = period_name\n",
    "        ds_dict_copy[k].attrs['yearly_sum'] = sum_type\n",
    "\n",
    "    return ds_dict_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a690d-ac2f-4875-96dc-66e8424a4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Standardize ========\n",
    "def standardize(ds_dict):\n",
    "    '''\n",
    "    Helper function to standardize datasets of a dictionary\n",
    "    '''\n",
    "    ds_dict_stand = {}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        ds_stand = (ds - ds.mean()) / ds.std()\n",
    "\n",
    "        # Preserve variable attributes from the original dataset\n",
    "        for var in ds.variables:\n",
    "            if var in ds_stand.variables:\n",
    "                ds_stand[var].attrs = ds[var].attrs\n",
    "\n",
    "        ds_stand.attrs = attrs\n",
    "        ds_dict_stand[name] = ds_stand\n",
    "        \n",
    "    return ds_dict_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090c127-204b-4d69-9e33-5769b28e4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_args_and_get_info(ds_dict, variable):\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if not isinstance(variable, str):\n",
    "        raise TypeError('variable must be a string.')\n",
    "        \n",
    "    # Dictionary to store plot titles for each statistic\n",
    "    titles = {\"mean\": \"Mean\", \"std\": \"Standard deviation of yearly means\", \"min\": \"Minimum\", \"max\": \"Maximum\", \"median\": \"Median\", \"time\": \"Time\", \"space\": \"Space\"}\n",
    "    freq = {\"mon\": \"Monthly\"}\n",
    "    \n",
    "    long_name = {\n",
    "        'Precipitation': 'Precipitation',\n",
    "        'Total Runoff': 'Total Runoff',\n",
    "        'Vapor Pressure Deficit': 'Vapor Pressure Deficit',\n",
    "        'Evaporation Including Sublimation and Transpiration': 'Evapotranspiration',\n",
    "        'Transpiration': 'Transpiration',\n",
    "        'Leaf Area Index': 'Leaf Area Index',\n",
    "        'Carbon Mass Flux out of Atmosphere Due to Gross Primary Production on Land [kgC m-2 s-1]': 'Gross Primary Production',\n",
    "        'Total Liquid Soil Moisture Content of 1 m Column': '1 m Soil Moisture',\n",
    "        'Total Liquid Soil Moisture Content of 2 m Column': '2 m Soil Moisture',\n",
    "        'Runoff - Precipitation': 'Runoff - Precipitation',\n",
    "        'Transpiration - Precipitation': 'Transpiration - Precipitation',\n",
    "        '(Runoff + Transpiration) - Precipitation':  '(Runoff + Transpiration) - Precipitation',\n",
    "        'ET - Precipitation':  'ET - Precipitation', \n",
    "        'Negative Runoff': 'Negative Runoff',\n",
    "    }\n",
    "   \n",
    "    # Data information\n",
    "    var_long_name = ds_dict[list(ds_dict.keys())[0]][variable].long_name\n",
    "    period = f\"{ds_dict[list(ds_dict.keys())[0]].attrs['period'][0]}-{ds_dict[list(ds_dict.keys())[0]].attrs['period'][1]}\"\n",
    "    experiment_id =  ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    unit = ds_dict[list(ds_dict.keys())[0]][variable].units\n",
    "    statistic_dim = ds_dict[list(ds_dict.keys())[0]].statistic_dimension\n",
    "    statistic = ds_dict[list(ds_dict.keys())[0]].attrs['statistic']\n",
    "    frequency = freq[ds_dict[list(ds_dict.keys())[0]].frequency]\n",
    "\n",
    "    return var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a73aa-e566-4d11-8dd0-c6de7e0cc432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_ensemble(ds_dict_change):\n",
    "    for key in ['Ensemble mean', 'Ensemble median']:\n",
    "        if key in ds_dict_change:\n",
    "            ds_dict_change.pop(key)\n",
    "\n",
    "    # Drop 'member_id' coordinate if it exists in any of the datasets\n",
    "    for ds_key in ds_dict_change:\n",
    "        if 'member_id' in ds_dict_change[ds_key].coords:\n",
    "            ds_dict_change[ds_key] = ds_dict_change[ds_key].drop('member_id')\n",
    "\n",
    "    combined = xr.concat(ds_dict_change.values(), dim='ensemble')\n",
    "    ds_dict_change['Ensemble median'] = getattr(combined, 'median')(dim='ensemble')\n",
    "    \n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e4851-532d-4dd6-af51-15e12b7761f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regionmask\n",
    "\n",
    "def apply_region_mask(ds_dict):\n",
    "    \"\"\"\n",
    "    Applies the AR6 land region mask to datasets in the provided dictionary and adds a region dimension.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary where keys are the same as in the input dictionary,\n",
    "              and each value is an xarray Dataset with a region dimension added to each variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    land_regions = regionmask.defined_regions.ar6.land\n",
    "    ds_masked_dict = {}\n",
    "    \n",
    "    for ds_name, ds in ds_dict.items():\n",
    "        ds_out = xr.Dataset()  # Initiate an empty Dataset for the masked data\n",
    "        \n",
    "        # Get attributes\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        for var in ds:\n",
    "            # Get the binary mask\n",
    "            mask = land_regions.mask_3D(ds[var])\n",
    "            \n",
    "            var_attrs = ds[var].attrs\n",
    "\n",
    "            # Multiply the original data with the mask to get the masked data\n",
    "            masked_var = ds[var] * mask\n",
    "\n",
    "            # Replace 0s with NaNs, if desired\n",
    "            masked_var = masked_var.where(masked_var != 0)\n",
    "\n",
    "            # Add the masked variable to the output Dataset\n",
    "            ds_out[var] = masked_var\n",
    "            \n",
    "            ds_out[var].attrs = var_attrs\n",
    "            \n",
    "        # Add the attributes\n",
    "        ds_out.attrs = attrs\n",
    "\n",
    "        # Add the Dataset to the output dictionary\n",
    "        ds_masked_dict[ds_name] = ds_out\n",
    "\n",
    "    return ds_masked_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fedf68-0e05-43a4-a705-ba2e9b163a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_numeric(data):\n",
    "    try:\n",
    "        _ = data.astype(float)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def compute_change(ds_dict_hist, ds_dict_fut, var_rel_change=None):\n",
    "    ds_dict_change = {}\n",
    "\n",
    "    for name, ds_hist in ds_dict_hist.items():\n",
    "        if name in ds_dict_fut:\n",
    "            ds_future = ds_dict_fut[name]\n",
    "            common_vars = set(ds_hist.data_vars).intersection(ds_future.data_vars)\n",
    "\n",
    "            ds_change = ds_hist.copy(deep=True)\n",
    "            \n",
    "            if var_rel_change == 'all':\n",
    "                var_rel_change = common_vars\n",
    "                \n",
    "            for var in common_vars:\n",
    "                if is_numeric(ds_hist[var].data) and is_numeric(ds_future[var].data):\n",
    "                    if var_rel_change is not None and var in var_rel_change:\n",
    "                        # Compute relative change where ds_hist is not zero\n",
    "                        rel_change = (ds_future[var] - ds_hist[var]) / ds_hist[var].where(ds_hist[var] != 0) * 100\n",
    "                        ds_change[var].data = rel_change.data\n",
    "                        ds_change[var].attrs['units'] = '%'\n",
    "                    else:\n",
    "                        # Compute absolute change\n",
    "                        abs_change = ds_future[var] - ds_hist[var]\n",
    "                        ds_change[var].data = abs_change.data\n",
    "\n",
    "            ds_change.attrs = ds_future.attrs\n",
    "            ds_dict_change[name] = ds_change\n",
    "\n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a2736-eb0f-466e-b547-4d293e8a5af5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2f45a-7c54-4e4b-a62d-c43186adb78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_spatial_mean(ds_dict):\n",
    "    ds_dict_mean = {}\n",
    "    for key, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        for var in list(ds.data_vars.keys()):\n",
    "            var_attrs = ds[var].attrs\n",
    "            \n",
    "            ds_dict_mean[key][var] = ds.mean(['lon', 'lat'])\n",
    "            ds_dict_mean[key][var].attrs = var_attrs\n",
    "        \n",
    "        ds_dict_mean[key].attrs = attrs\n",
    "        \n",
    "    return ds_dict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766cd9d-393f-4db9-b0a3-744388ad88c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic_single(ds, statistic, dimension, yearly_mean=True):\n",
    "    if dimension == \"time\":\n",
    "        stat_ds = getattr(ds, statistic)(\"time\", keep_attrs=True, skipna=True)\n",
    "        stat_ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "    if dimension == \"space\":\n",
    "        # Assign the period attribute before grouping by year\n",
    "        ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "        if yearly_mean:\n",
    "            ds = ds.groupby('time.year').mean('time', keep_attrs=True, skipna=True)\n",
    "            ds.attrs['mean'] = 'yearly mean'\n",
    "            \n",
    "        \n",
    "        #get the weights, apply on data, and compute statistic\n",
    "        weights = np.cos(np.deg2rad(ds.lat))\n",
    "        weights.name = \"weights\"\n",
    "        ds_weighted = ds.weighted(weights)\n",
    "        stat_ds = getattr(ds_weighted, statistic)((\"lon\", \"lat\"), keep_attrs=True, skipna=True)\n",
    "    \n",
    "    stat_ds.attrs['statistic'] = statistic\n",
    "    stat_ds.attrs['statistic_dimension'] = dimension\n",
    "\n",
    "    return stat_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadad33e-375e-4f6f-94b1-918dce7e2f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic(ds_dict, statistic, dimension, start_year=None, end_year=None, yearly_mean=True):\n",
    "    \"\"\"\n",
    "    Computes the specified statistic for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        statistic (str): The statistic to compute, which can be one of 'mean', 'std', 'min', 'var', or 'median'.\n",
    "        dimension (str): The dimension to compute over, which can be 'time' or 'space'.\n",
    "        start_year (str, optional): The start year of the period to compute the statistic over.\n",
    "        end_year (str, optional): The end year of the period to compute the statistic over.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with computed statistic for each dataset.\n",
    "    \"\"\"\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if statistic not in [\"mean\", \"std\", \"min\", \"max\", \"var\", \"median\"]:\n",
    "        raise ValueError(f\"Invalid statistic '{statistic}' specified.\")\n",
    "    if dimension not in [\"time\", \"space\"]:\n",
    "        raise ValueError(f\"Invalid dimension '{dimension}' specified.\")\n",
    "\n",
    "    # Select period\n",
    "    if start_year is not None and end_year is not None:\n",
    "        select_period(ds_dict, start_year=start_year, end_year=end_year)\n",
    "        \n",
    "        \n",
    "    # Use multiprocessing to compute the statistic for each dataset in parallel\n",
    "    with mp.Pool() as pool:\n",
    "        results = pool.starmap(compute_statistic_single, [(ds, statistic, dimension, yearly_mean) for ds in ds_dict.values()])\n",
    "\n",
    "    return dict(zip(ds_dict.keys(), results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d176a36-95a5-4a18-b46f-a311e49243f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def precompute_metrics(ds_dict, variables, metrics=['pearson']):\n",
    "    # Initialize the results dictionary\n",
    "    results_dict = {metric: {} for metric in metrics}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        # Create a DataFrame with all the variables\n",
    "        df = pd.DataFrame({var: ds[var].values.flatten() for var in variables})\n",
    "        \n",
    "        # Define all pairs of variables\n",
    "        pairs = list(permutations(variables, 2))  # <-- Change here\n",
    "        args = [(df, var1, var2, metrics) for var1, var2 in pairs]\n",
    "\n",
    "        # Use a multiprocessing pool to compute the metrics for all pairs\n",
    "        with Pool() as p:\n",
    "            results = p.map(compute_metrics_for_pair, args)\n",
    "        \n",
    "        # Store the results in the results_dict\n",
    "        for var1, var2, metric_dict in results:\n",
    "            for metric, value in metric_dict.items():\n",
    "                # Ensure the keys exist in the dictionary\n",
    "                results_dict[metric].setdefault(name, {}).setdefault(f'{var1}_{var2}', value)\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9381cb-6944-4552-b6c7-aac1b6a3c6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_stats(ds_dict):\n",
    "    \"\"\"\n",
    "    Compute yearly mean of each variable in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds_dict (dict): The input dictionary of xarray.Dataset.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are the dataset names and the values are another dictionary.\n",
    "          This inner dictionary has keys as variable names and values as DataArray of yearly means.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    for model, ds in ds_dict.items():\n",
    "        # Compute the yearly mean\n",
    "        yearly_ds = ds.resample(time='1Y').mean()\n",
    "\n",
    "        stats[model] = {}\n",
    "        for var in yearly_ds.data_vars:\n",
    "            # Compute the spatial mean\n",
    "            spatial_mean = yearly_ds[var].mean(dim=['lat', 'lon'])\n",
    "            \n",
    "            # Store the yearly mean values\n",
    "            stats[model][var] = spatial_mean\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43015f97-e54e-4f20-aa0c-3e10f0c8d748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_yearly_means(ds_dict):\n",
    "    yearly_means_dict = {}\n",
    "\n",
    "    # For each dataset, compute the yearly mean over the 'time', 'lat', and 'lon' dimensions\n",
    "    for name, ds in ds_dict.items():  \n",
    "        ds_yearly = ds.groupby('time.year').mean('time')    \n",
    "        \n",
    "        yearly_means_dict[name] = ds_yearly\n",
    "\n",
    "    return yearly_means_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbbc45-b47c-461b-99db-d20d9fa0625d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_yearly_regional_means(ds_dict_region):\n",
    "    yearly_means_dict = {}\n",
    "\n",
    "    # For each dataset, compute the yearly mean over the 'time', 'lat', and 'lon' dimensions\n",
    "    for region, ds_dict in ds_dict_region.items():\n",
    "        yearly_means_dict[region] = {}\n",
    "        for ds_name, ds in ds_dict.items():\n",
    "            # Compute the yearly mean\n",
    "            ds_yearly = ds.groupby('time.year').mean('time')\n",
    "            \n",
    "            # Create weights\n",
    "            weights = np.cos(np.deg2rad(ds.lat))\n",
    "            # Apply the weights and calculate the spatial mean\n",
    "            ds_weighted = ds_yearly.weighted(weights)\n",
    "            yearly_means_dict[region][ds_name] = ds_weighted.mean(('lat', 'lon'))\n",
    "\n",
    "    return yearly_means_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b71e4e-b89b-4169-bac4-97f010c2f2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_spatial_mean(ds_dict):\n",
    "    ds_dict_mean = {}\n",
    "    \n",
    "    for key, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        # Initialize a new Dataset for this key\n",
    "        ds_dict_mean[key] = xr.Dataset()\n",
    "        \n",
    "        for var in list(ds.data_vars.keys()):\n",
    "            var_attrs = ds[var].attrs\n",
    "            \n",
    "            ds_dict_mean[key][var] = ds[var].mean(['lon', 'lat'])\n",
    "            ds_dict_mean[key][var].attrs = var_attrs\n",
    "        \n",
    "        ds_dict_mean[key].attrs = attrs\n",
    "        \n",
    "    return ds_dict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22691ecc-6183-484e-ac91-4c35668756da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_spatial_mean_std(ds_dict):\n",
    "    ds_stats = {}\n",
    "    \n",
    "    for key, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        # Initialize a new Dataset for spatial statistics\n",
    "        ds_stats[key] = xr.Dataset()\n",
    "        \n",
    "        for var in list(ds.data_vars.keys()):\n",
    "            var_attrs = ds[var].attrs\n",
    "            \n",
    "            # Calculate the mean and standard deviation, skipping NaN values\n",
    "            ds_stats[key][f'{var}'] = ds[var].mean(['lon', 'lat'], skipna=True)\n",
    "            ds_stats[key][f'{var}'].attrs = var_attrs\n",
    "            \n",
    "            # Use a minimum count of 2 for standard deviation to ensure degrees of freedom > 0\n",
    "            ds_stats[key][f'{var}_std'] = ds[var].std(['lon', 'lat'], skipna=True)\n",
    "            ds_stats[key][f'{var}_std'].attrs = var_attrs\n",
    "        \n",
    "        ds_stats[key].attrs = attrs\n",
    "        \n",
    "    return ds_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354754c-b13c-4650-b31b-7ae9e9e424ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_bgws(ds_dict):\n",
    "\n",
    "    for model, ds in ds_dict.items():\n",
    "        bgws = (ds['mrro']-ds['tran'])/ds['pr']\n",
    "\n",
    "        # Replace infinite values with NaN\n",
    "        bgws = xr.where(np.isinf(bgws), float('nan'), bgws)\n",
    "\n",
    "        # Set all values above 2 and below -2 to NaN\n",
    "        bgws = xr.where(bgws > 2, float('nan'), bgws)\n",
    "        bgws = xr.where(bgws < -2, float('nan'), bgws)\n",
    "\n",
    "        ds_dict[model]['bgws'] = bgws\n",
    "        ds_dict[model]['bgws'].attrs = {'long_name': 'Blue Green Water Share',\n",
    "                             'units': ''}\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173177e7-1327-46be-9952-4ba5d9d45155",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343c372-5fab-4276-9c97-171ecae7340b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "#variables=['tas', 'pr', 'vpd', 'evspsbl', 'mrro', 'lmrso_1m', 'lmrso_2m', 'tran', 'lai', 'gpp', 'EI', 'wue']\n",
    "variables=['tas', 'pr', 'vpd', 'evspsbl', 'mrro', 'mrso', 'tran', 'lai', 'gpp']\n",
    "#variables=['pr', 'evspsbl', 'mrro', 'tran']\n",
    "\n",
    "folder='preprocessed'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# Create dictionary using a dictionary comprehension and Dask\n",
    "experiment_id = 'historical'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','GISS-E2-1-G','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','TaiESM1','UKESM1-0-LL']\n",
    "ds_dict_hist = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, variables) for model in source_id})[0]\n",
    "\n",
    "experiment_id = 'ssp126'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','GISS-E2-1-G','MIROC-ES2L', 'MPI-ESM1-2-LR','UKESM1-0-LL']\n",
    "ds_dict_ssp126 = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, variables) for model in source_id})[0]\n",
    "\n",
    "\n",
    "experiment_id = 'ssp370'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','GISS-E2-1-G','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','TaiESM1','UKESM1-0-LL']\n",
    "ds_dict_ssp370 = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, variables) for model in source_id})[0]\n",
    "\n",
    "\n",
    "experiment_id = 'ssp585'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','GISS-E2-1-G','MIROC-ES2L', 'MPI-ESM1-2-LR','UKESM1-0-LL']\n",
    "ds_dict_ssp585 = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, variables) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f749cba-efb8-4532-9e4b-a7b880530fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= Have a look into the data ==============\n",
    "#print(ds_dict_ssp126.keys())\n",
    "#ds_dict_hist[list(ds_dict_region_change.keys())[8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f417b3-2b33-4081-b1d8-6d3a1c21d269",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a9eff-675e-464e-9236-d82dc7a38d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'nh_winter': [12, 1, 2],'nh_spring': [3, 4, 5],'nh_summer': [6, 7, 8], 'nh_fall': [9, 10, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29262fe-a12e-44ff-894e-6d851abac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_period = select_period(ds_dict_hist, start_year=1985, end_year=2014, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938cc065-ebae-4275-9032-c60801fbe35f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp126_period = select_period(ds_dict_ssp126, start_year=2071, end_year=2100, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a12260-1df3-4232-bf91-196fb8144269",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period = select_period(ds_dict_ssp370, start_year=2071, end_year=2100, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b5ac7-d295-4756-be5a-dab0f661373e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp585_period = select_period(ds_dict_ssp585, start_year=2071, end_year=2100, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de9ead-0d36-4f06-9430-9efeae7cc9bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b6390-f4b2-41d0-951d-c6bae78ce192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_hist_period_metric = compute_statistic(ds_dict_hist_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984dd44-1f18-4f2c-aceb-465d8473c124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp126_period_metric = compute_statistic(ds_dict_ssp126_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a9c21-8094-4700-b972-36e3f720d629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_ssp370_period_metric = compute_statistic(ds_dict_ssp370_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f412cbe-95ef-4315-b9b5-ea06ffe0506f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp585_period_metric = compute_statistic(ds_dict_ssp585_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6bc0a-0837-4359-bd61-a6ad7408853e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute BGWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e99e6-709a-410c-abaa-c9a087e0346f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric = compute_bgws(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b0b69-d505-4785-98ad-9aa38437c43c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_ = compute_bgws(ds_dict_hist_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eeca7e-4c4b-4b89-9d47-047ab44e6d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp126_period_metric = compute_bgws(ds_dict_ssp126_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d56759-5317-4815-936e-9b352c10589a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric = compute_bgws(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b5860-687e-4325-a8d7-fe3aff147d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp585_period_metric = compute_bgws(ds_dict_ssp585_period_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913e646-e441-4989-b975-a12023b0baeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef8139-5a2c-43ce-902a-c19a7112355b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric_regions = {}\n",
    "ds_dict_hist_period_metric_regions = apply_region_mask(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c29b14-48cd-46b0-8026-20412c286ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_ensemble_regions = {}\n",
    "ds_dict_hist_period_ensemble_regions = apply_region_mask(ds_dict_hist_period_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b41f3-524b-40f6-93bf-16298c93737c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp126_period_metric_regions = {}\n",
    "ds_dict_ssp126_period_metric_regions = apply_region_mask(ds_dict_ssp126_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbb35e-e9ae-4879-b94d-98d345b56487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric_regions = {}\n",
    "ds_dict_ssp370_period_metric_regions = apply_region_mask(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61624353-4e9b-405b-b2c3-464c9158764e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp585_period_metric_regions = {}\n",
    "ds_dict_ssp585_period_metric_regions = apply_region_mask(ds_dict_ssp585_period_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e5240-ef4c-4649-a512-499bf54b78fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute regional mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78adb029-82f4-4f31-84f6-b0d4ab17ef0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute spatial mean of regional data\n",
    "ds_dict_hist_period_metric_regional_mean = {}\n",
    "ds_dict_hist_period_metric_regional_mean = calculate_spatial_mean(ds_dict_hist_period_metric_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a51d5-c473-4da7-8980-7ecde725bb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_ensemble_regional_mean = {}\n",
    "ds_dict_hist_period_ensemble_regional_mean = calculate_spatial_mean(ds_dict_hist_period_ensemble_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc77cc-5fad-4e66-93b5-9e982a44e91b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_ensemble_regional_mean = ds_dict_hist_period_ensemble_regional_mean['Ensemble mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5d4cd-8cf7-4fb7-bfd6-b384a5880d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric_regional_mean_std = {}\n",
    "ds_dict_hist_period_metric_regional_mean_std = calculate_spatial_mean_std(ds_dict_hist_period_metric_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527e66f-0aa1-4e2e-a34f-126e0d650b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute spatial mean of regional data\n",
    "ds_dict_ssp126_period_metric_regional_mean = {}\n",
    "ds_dict_ssp126_period_metric_regional_mean = calculate_spatial_mean(ds_dict_ssp126_period_metric_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f73e8-df66-4029-beea-f682896e98b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute spatial mean of regional data\n",
    "ds_dict_ssp370_period_metric_regional_mean = {}\n",
    "ds_dict_ssp370_period_metric_regional_mean = calculate_spatial_mean(ds_dict_ssp370_period_metric_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8ce6a-3188-442b-8584-b6f723e2d8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute spatial mean of regional data\n",
    "ds_dict_ssp585_period_metric_regional_mean = {}\n",
    "ds_dict_ssp585_period_metric_regional_mean = calculate_spatial_mean(ds_dict_ssp585_period_metric_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f7294-77df-425c-885e-65f1e9ffb1d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test BGWS robustness regionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43561937-279d-43c5-b18c-0c041418a12c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def calculate_ensemble_metrics(ds_dict_regional_mean, variable='bgws'):\n",
    "    \"\"\"\n",
    "    Calculate the ensemble mean, its uncertainty (95% confidence interval), SEM, other spread metrics, \n",
    "    and the percentage of models agreeing on the sign of the mean for the 'bgws' variable in each region.\n",
    "    :param ds_dict_regional_mean: Dictionary of xarray Datasets with regional mean values, keys are model names.\n",
    "    :param variable: The variable to analyze, default is 'bgws'.\n",
    "    :return: DataFrame with regions, ensemble mean, confidence interval, SEM, spread metrics, and sign agreement.\n",
    "    \"\"\"\n",
    "    ensemble_data = []\n",
    "\n",
    "    # Combining datasets into a single dataset\n",
    "    combined_ds = xr.concat([ds[variable] for ds in ds_dict_regional_mean.values()], dim='model')\n",
    "\n",
    "    # Calculate ensemble mean and spread metrics\n",
    "    ensemble_mean = combined_ds.mean(dim='model')\n",
    "    std_dev = combined_ds.std(dim='model')\n",
    "    iqr = combined_ds.quantile(0.75, dim='model') - combined_ds.quantile(0.25, dim='model')\n",
    "\n",
    "    # Calculate confidence interval (95%)\n",
    "    sem = std_dev / np.sqrt(len(ds_dict_regional_mean))  # Standard Error of the Mean\n",
    "    confidence_interval = stats.t.interval(0.95, len(ds_dict_regional_mean) - 1, loc=ensemble_mean, scale=sem)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    for region_number, region_name, mean, ci, std, iqr_val in zip(ensemble_mean.region.values, ensemble_mean.names.values, ensemble_mean.values, np.transpose(confidence_interval), std_dev.values, iqr.values):\n",
    "        ensemble_data.append({\n",
    "            'Region Number': region_number, \n",
    "            'Region': region_name, \n",
    "            'Ensemble Mean': mean, \n",
    "            'Lower CI': ci[0], \n",
    "            'Upper CI': ci[1],\n",
    "            'Standard Deviation': std,\n",
    "            'IQR': iqr_val\n",
    "        })\n",
    "\n",
    "    ensemble_df = pd.DataFrame(ensemble_data)\n",
    "\n",
    "    # Sort DataFrame\n",
    "    sorted_df = ensemble_df.sort_values(by=['Standard Deviation'], ascending=[True])\n",
    "\n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f913181-c4ac-46bc-ba3d-a126ce4272b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensemble_metrics_df = calculate_ensemble_metrics(ds_dict_hist_period_metric_regional_mean, 'bgws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12191b-44b7-4095-95d1-3aa3b221e92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensemble_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e629616-ec56-493e-bf47-692bb8fdc44b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Split positive and negative BGWS regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca6826-c2e1-4416-b0f1-5ff5901ecf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ensemble_metric(ds_dict, average='mean'):\n",
    "    \"\"\"\n",
    "    Compute ensemble mean or median for a dictionary of xarray Datasets and preserve attributes.\n",
    "    \n",
    "    :param ds_dict: Dictionary of xarray Datasets.\n",
    "    :param average: Type of ensemble statistic to compute ('mean' or 'median').\n",
    "    :return: xarray Dataset with ensemble statistic and preserved attributes.\n",
    "    \"\"\"\n",
    "    # Remove existing ensemble mean or median if present\n",
    "    for key in ['Ensemble mean', 'Ensemble median']:\n",
    "        ds_dict.pop(key, None)  # Use pop with None as default to avoid KeyError\n",
    "\n",
    "    # Concatenate datasets along a new 'ensemble' dimension\n",
    "    combined = xr.concat(ds_dict.values(), dim='ensemble')\n",
    "\n",
    "    # Compute ensemble statistic\n",
    "    ensemble_metric_ds = getattr(combined, average)(dim='ensemble')\n",
    "\n",
    "    # Copy attributes from the original dataset\n",
    "    first_ds = next(iter(ds_dict.values()))\n",
    "    ensemble_metric_ds.attrs = first_ds.attrs\n",
    "\n",
    "    # Copy attributes for each variable\n",
    "    for var in ensemble_metric_ds.variables:\n",
    "        if var in first_ds.variables:\n",
    "            ensemble_metric_ds[var].attrs = first_ds[var].attrs\n",
    "\n",
    "    # Assign ensemble statistic as a global attribute\n",
    "    ensemble_metric_ds.attrs['ensemble_stat'] = average\n",
    "\n",
    "    return ensemble_metric_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca8dc4-2fbf-4416-a77a-7f08c72ebeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mean_ds = compute_ensemble_metric(ds_dict_hist_period_metric_regional_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320a744-4ffa-4d39-bd81-a5ee606ac0b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dataset_by_bgws_sign(dataset, variable='bgws'):\n",
    "    \"\"\"\n",
    "    Split the dataset into two based on the sign of the specified variable's values.\n",
    "\n",
    "    :param dataset: xarray Dataset.\n",
    "    :param variable: The variable to check the sign (default is 'bgws').\n",
    "    :return: Two xarray Datasets - one with positive and the other with negative values of the variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split dataset based on the sign of the 'bgws' variable\n",
    "    positive_ds = dataset.where(dataset[variable] > 0, drop=True)\n",
    "    negative_ds = dataset.where(dataset[variable] < 0, drop=True)\n",
    "\n",
    "    return positive_ds, negative_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f42a2-24e5-4c84-9868-c710a1b9663a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_bgws_ds, negative_bgws_ds = split_dataset_by_bgws_sign(ensemble_mean_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5211c2-21b9-4b2c-84ce-176aafe882cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cluster regions based on BGWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691650ce-32b1-4a54-9495-c5be1df897a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae722f4-ac4b-4100-b69d-78133da87c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_regions_gmm(ds, max_clusters=10, important_vars=None, importance_factor=1):\n",
    "    \"\"\"\n",
    "    Clusters regions using a Gaussian Mixture Model based on their numeric variables in a single dataset.\n",
    "    Important variables can be given increased weight by duplicating them.\n",
    "    Data is standardized before clustering. Returns a DataFrame with region names and their corresponding cluster numbers.\n",
    "\n",
    "    :param ds: An xarray.Dataset object.\n",
    "    :param max_clusters: The maximum number of clusters to test for the elbow method.\n",
    "    :param important_vars: List of variables to be given increased importance.\n",
    "    :param importance_factor: Factor to determine how many times to duplicate important variables.\n",
    "    :return: A pandas DataFrame with columns 'Region' and 'Cluster'.\n",
    "    \"\"\"\n",
    "    # Convert the xarray Dataset to a pandas DataFrame\n",
    "    df = ds.to_dataframe().reset_index()\n",
    "    df = df.set_index('region')\n",
    "\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    df_numeric = df[numeric_cols]\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df_numeric = df_numeric.dropna()\n",
    "\n",
    "    # Duplicate important variables to increase their weight\n",
    "    if important_vars and importance_factor > 1:\n",
    "        for var in important_vars:\n",
    "            if var in df_numeric.columns:\n",
    "                for i in range(importance_factor - 1):  # We already have one set of important vars\n",
    "                    df_numeric[f'{var}_dup_{i}'] = df_numeric[var]\n",
    "\n",
    "    # Find the optimal number of clusters using Gaussian Mixture Model\n",
    "    bic_scores = []\n",
    "    K = range(1, max_clusters + 1)\n",
    "    for k in K:\n",
    "        gmm = GaussianMixture(n_components=k, n_init=10, random_state=42)\n",
    "        gmm = gmm.fit(df_numeric)\n",
    "        bic_scores.append(gmm.bic(df_numeric))\n",
    "\n",
    "    # Plot the BIC scores\n",
    "    plt.plot(K, bic_scores, 'bx-')\n",
    "    plt.xlabel('k (number of components)')\n",
    "    plt.ylabel('BIC score')\n",
    "    plt.title('BIC Scoring for Gaussian Mixture Model')\n",
    "    plt.show()\n",
    "\n",
    "    # Ask user for the optimal number of clusters\n",
    "    n_clusters = int(input(\"Enter the optimal number of clusters: \"))\n",
    "\n",
    "    # Use the Gaussian Mixture Model to find clusters with the optimal number of clusters\n",
    "    gmm = GaussianMixture(n_components=n_clusters, n_init=10, random_state=42)\n",
    "    gmm = gmm.fit(df_numeric)\n",
    "\n",
    "    # Predict the cluster for each region\n",
    "    cluster_labels = gmm.predict(df_numeric)\n",
    "\n",
    "    # Create a DataFrame with region identifiers and their assigned clusters\n",
    "    clustered_df = pd.DataFrame({\n",
    "        'Region': df.index,  # Adjust this if your region identifier column is named differently\n",
    "        'Cluster': cluster_labels\n",
    "    }).sort_values('Cluster')\n",
    "\n",
    "    # Merge the original DataFrame (with variable values) with the cluster assignments\n",
    "    merged_df = clustered_df.merge(df[numeric_cols + ['abbrevs', 'names']], left_on='Region', right_index=True)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33927c-6910-4565-95ad-1c6217088230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "important_variables = ['bgws']\n",
    "clustered_negative_bgws_ds = cluster_regions_gmm(negative_bgws_ds, important_vars=None, importance_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e7562-8be5-453d-af1c-ab8ba773fe0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set pandas options to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set pandas options to display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# If the table is still too wide, you can also adjust the max column width\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# If your table has many columns and you want to see them all at once without truncation, you can set a high threshold for column width\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baad080-d208-49a3-a421-0a999cecc70f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_negative_bgws_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d948dd-69d5-436f-bfee-13c63a76cd63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_positive_bgws_ds = cluster_regions_gmm(positive_bgws_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0c521-19cb-476c-9294-947c54cd72c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_positive_bgws_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9a570-e7f4-4ec1-b914-8466bb613359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_positive_bgws_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbabecc0-4852-4f3a-b6c2-e0b43432dcb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot regional var change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacecae-67af-418b-acbf-dd95c1cec334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_ticks(min_val, max_val, base=20):\n",
    "    \"\"\"Calculate tick positions for the y-axis.\"\"\"\n",
    "    # Calculate the 10% of the min and max values\n",
    "    offset_min = abs(min_val) * 0.1\n",
    "    offset_max = max_val * 0.1\n",
    "\n",
    "    # Find the next \"round\" number beyond min and max values based on the base\n",
    "    lower_tick = base * np.floor(min_val / base)\n",
    "    upper_tick = base * np.ceil(max_val / base)\n",
    "\n",
    "    # Ensure lower_tick is not more than 10% lower than min_val\n",
    "    if min_val - lower_tick > offset_min:\n",
    "        lower_tick += base\n",
    "\n",
    "    # Ensure upper_tick is not more than 10% higher than max_val\n",
    "    if upper_tick - max_val > offset_max:\n",
    "        upper_tick -= base\n",
    "\n",
    "    # If the lower_tick or upper_tick is equal to the base, divide the base by 2\n",
    "    if lower_tick >= -base or upper_tick <= base:\n",
    "        base = base / 2\n",
    "        # Recalculate the ticks with the new base\n",
    "        lower_tick = base * np.floor(min_val / base)\n",
    "        upper_tick = base * np.ceil(max_val / base)\n",
    "\n",
    "    # Include zero and extend to the next round number beyond the data's min and max\n",
    "    ticks = [lower_tick] if lower_tick < 0 else []\n",
    "    ticks += [0]  # Always include zero\n",
    "    ticks += [upper_tick] if upper_tick > 0 else []\n",
    "\n",
    "    # Generate intermediate ticks between the round numbers\n",
    "    intermediate_ticks = np.arange(lower_tick + base, upper_tick, base)\n",
    "    ticks.extend(intermediate_ticks)\n",
    "\n",
    "    return sorted(ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab421365-c368-4fae-8744-643cefae1fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "\n",
    "# Define start and end colors for both gradients\n",
    "deep_blue = (20/255, 110/255, 180/255)  \n",
    "light_blue = (180/255, 215/255, 255/255)\n",
    "deep_green = (14/255, 119/255, 14/255) \n",
    "light_green = (160/255, 220/255, 140/255)\n",
    "\n",
    "# Create custom colormaps\n",
    "blue_cmap = LinearSegmentedColormap.from_list(\"blue_cmap\", [light_blue, deep_blue], N=4)\n",
    "green_cmap = LinearSegmentedColormap.from_list(\"green_cmap\", [deep_green, light_green], N=4)\n",
    "\n",
    "# Sample colors from colormaps\n",
    "blue_colors = [blue_cmap(i) for i in np.linspace(0, 1, 4)]\n",
    "green_colors = [green_cmap(i) for i in np.linspace(0, 1, 4)]\n",
    "\n",
    "# Combine both gradients\n",
    "combined_grad = green_colors + blue_colors \n",
    "\n",
    "# Define boundaries\n",
    "boundaries = [-0.1, -0.75, -0.05, -0.25, 0, 0.25, 0.05, 0.75, 0.1]\n",
    "norm = BoundaryNorm(boundaries, len(combined_grad), clip=True)\n",
    "\n",
    "cmap_name = 'BGWS colormap'\n",
    "bgws_cm = LinearSegmentedColormap.from_list(cmap_name, combined_grad, N=len(combined_grad))\n",
    "\n",
    "# To test and display the colormap\n",
    "fig, ax = plt.subplots(figsize=(6, 1))\n",
    "ax.set_title(cmap_name)\n",
    "plt.imshow(np.linspace(0, 1, 256).reshape(1, -1), aspect='auto', cmap=bgws_cm)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94199d-decc-4c7e-ba6a-2788d5628ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_region_change(ds_dict_change, selected_indices=\"ALL\", save_fig=True):\n",
    "    \n",
    "    # Compute ensemble data\n",
    "    ds_dict_change = compute_ensemble(ds_dict_change)\n",
    "\n",
    "    # Determine the type of change: relative or absolute\n",
    "    change = 'rel_change' if ds_dict_change[list(ds_dict_change.keys())[0]]['pr'].units == '%' else 'abs_change'\n",
    "\n",
    "    # Extract models and variables information\n",
    "    models = list(ds_dict_change.keys())\n",
    "    ensemble_median = ds_dict_change['Ensemble median']\n",
    "    variables = [var for var in ensemble_median.data_vars.keys() if var != 'bgws']\n",
    "    experiment_id = ds_dict_change[list(ds_dict_change.keys())[0]].experiment_id\n",
    "    description = ds_dict_change[list(ds_dict_change.keys())[0]].description\n",
    "    # Check for 'period' and 'yearly_sum' attributes, set defaults if not found\n",
    "    months = ds_dict_change[list(ds_dict_change.keys())[0]].months\n",
    "    yearly_sum = ds_dict_change[list(ds_dict_change.keys())[0]].yearly_sum\n",
    "\n",
    "            \n",
    "\n",
    "    # Create a map for variable display names\n",
    "    var_map = {\n",
    "        'tas': 'T', 'vpd': 'VPD', 'gpp': 'GPP', 'pr': 'P', 'mrro': 'R',\n",
    "        'evspsbl': 'ET', 'tran': 'Tran', 'lai': 'Lai', 'mrso': 'SM'\n",
    "    }\n",
    "    display_vars = [\n",
    "        f\"$\\Delta\\, \\mathrm{{\\it{{{var_map[var]}}}}}$\" if var in var_map else var for var in variables\n",
    "    ]\n",
    "\n",
    "    # Handle selection of indices\n",
    "    selected_indices = (ensemble_median.region.values.tolist()\n",
    "                        if selected_indices == \"ALL\" else selected_indices)\n",
    "\n",
    "    # Setup plot grid\n",
    "    nregions = len(selected_indices)\n",
    "    ncols = 9\n",
    "    nrows = (nregions + ncols - 1) // ncols \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8 * ncols, 7 * nrows))\n",
    "\n",
    "    # Setup plot grid\n",
    "    #nrows = len(selected_indices) // 9 + (len(selected_indices) % 9 > 0)\n",
    "    #fig, axes = plt.subplots(nrows=nrows, ncols=9, figsize=(8 * 9, 7 * nrows), squeeze=False)\n",
    "    \n",
    "    # bgws_colormap\n",
    "    bgws_vals = np.array([ds_dict_change[model]['bgws'].values for model in models])\n",
    "    bgws_min, bgws_max = np.nanmin(bgws_vals), np.nanmax(bgws_vals)\n",
    "    norm = plt.Normalize(vmin=-0.1, vmax=0.1)\n",
    "    \n",
    "    # Preliminary settings\n",
    "    threshold = 100  # Set the threshold for y-axis limits\n",
    "    capped_value = 105  # The value to assign to capped data points\n",
    "\n",
    "    if selected_indices == \"ALL\":\n",
    "        selected_indices = ds_dict_change['Ensemble median'].region.values.tolist()\n",
    "        \n",
    "    # Iterate over each region to plot data\n",
    "    for ridx, region_idx in enumerate(selected_indices):\n",
    "        ax = axes.flatten()[ridx]\n",
    "        max_change = 0  # Track the maximum change for y-axis limits   \n",
    "        min_change = 0 \n",
    "        \n",
    "        # Collect all y-values to determine if any exceed the threshold\n",
    "        all_y_vals = []\n",
    "        \n",
    "        for idx, model in enumerate(models):\n",
    "            model_data = ds_dict_change[model]\n",
    "            y = [\n",
    "                model_data[var].sel(region=region_idx).values\n",
    "                if var in ds_dict_change[model].data_vars else float('nan')\n",
    "                for var in variables\n",
    "            ]\n",
    "            all_y_vals.extend(y)\n",
    "            \n",
    "            # Ensure y is a NumPy array for element-wise comparison\n",
    "            y_array = np.array(y)\n",
    "            \n",
    "            # Cap the values at the threshold\n",
    "            y_capped = np.clip(y_array, -threshold, threshold)\n",
    "            # Set values that were above or below the threshold\n",
    "            y_capped = np.where(y_array > threshold, capped_value, y_capped)  # For values above the threshold\n",
    "            y_capped = np.where(y_array < -threshold, -capped_value, y_capped)  # For values below the threshold\n",
    "            \n",
    "            # Update max_change and min_change\n",
    "            max_change = max(max_change, np.nanmax(y))  \n",
    "            min_change = min(min_change, np.nanmin(y)) \n",
    "            \n",
    "            # Set plotting of bgws\n",
    "            x = range(len(display_vars))\n",
    "            bgws_val = model_data['bgws'].sel(region=region_idx).values if 'bgws' in model_data.data_vars else None\n",
    "            linestyle = '-' if bgws_val is not None and bgws_val >= 0 else '--'\n",
    "            color = bgws_cm(norm(bgws_val))\n",
    "            \n",
    "            # Plot all models\n",
    "            if model != \"Ensemble median\":\n",
    "                ax.plot(x, y_capped, linestyle=linestyle, color=color, linewidth=1.0, alpha=0.6, zorder=1)\n",
    "                for xi, yi in zip(x, y_capped):\n",
    "                    if not np.isnan(yi):\n",
    "                        ax.text(xi, yi, str(idx + 1), ha='center', va='center', fontsize=16, color=color, fontweight='bold', zorder=2)\n",
    "            else: # Plot Ensemble median\n",
    "                ax.scatter(x, y_capped, marker='D', edgecolors='red', s=150, label=model, zorder=3, facecolors='none', lw=2)\n",
    "        \n",
    "        \n",
    "        # Set plot titles and labels\n",
    "        ax.set_title(ensemble_median.names.sel(region=region_idx).values, fontsize=20)\n",
    "        ax.set_xticks(range(len(display_vars)))\n",
    "        ax.set_xticklabels(display_vars, rotation=0, fontsize=18)  \n",
    "        y_label = 'End of century response [%]' if change == 'rel_change' else 'End of century response'\n",
    "        ax.set_ylabel(y_label, fontsize=18)\n",
    "        \n",
    "        # Set y-axis limits dynamically based on data spread\n",
    "        lower_limit = max(-threshold, min_change) \n",
    "        upper_limit = min(threshold, max_change) \n",
    "\n",
    "        # Add 10 on y-axis if change exceeds threshold\n",
    "        extra_space = 10  # 10 percent extra space\n",
    "        \n",
    "        # Plot dashed line, extent and add arrows to y-axis if change > threshold\n",
    "        # Max Change\n",
    "        if max_change > threshold:\n",
    "            ax.axhline(threshold, color='grey', linestyle='--', linewidth=0.5, zorder=1)\n",
    "            ax.plot(0, threshold+10, \"^k\", transform=ax.get_yaxis_transform(), clip_on=False)\n",
    "\n",
    "        # Min Change    \n",
    "        if min_change < -threshold:\n",
    "            ax.axhline(-threshold, color='grey', linestyle='--', linewidth=0.5, zorder=1)\n",
    "            ax.plot(0, -threshold-10, \"vk\", transform=ax.get_yaxis_transform(), clip_on=False)\n",
    "            \n",
    "        # Set y-limits \n",
    "        #ax.set_ylim(lower_limit - extra_space, upper_limit + extra_space)\n",
    "        ax.set_ylim(lower_limit * 1.1, upper_limit * 1.1)\n",
    "        # Define the tick positions, ensuring they do not go beyond the max/min values or thresholds\n",
    "        # Calculate and set y-axis ticks\n",
    "        y_ticks = calculate_ticks(lower_limit, upper_limit)\n",
    "        ax.set_yticks(y_ticks)\n",
    "        \n",
    "        ax.axhline(0, color='grey', linewidth=0.5)\n",
    "        ax.tick_params(axis='x', length=0)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(True)\n",
    "        ax.yaxis.set_tick_params(labelsize=14)\n",
    "    \n",
    "    \n",
    "    # Caption and figure saving\n",
    "    caption = f\"{description} (2071-2100) - Historical (1985-2014)\"\n",
    "    fig.text(0.52, 1.04, caption, ha='center', va='top', fontsize=45, wrap=True, weight='bold')\n",
    "    \n",
    "    # Layout adjustments and legend\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.7)\n",
    "    \n",
    "    # Remove the empty plot in the last row and third column\n",
    "    axes.flatten()[-1].remove()\n",
    "    \n",
    "     # Define positions for the legend and colorbar\n",
    "    legend_position = [0.8, 0.04, 0.3, 0.15]  # [x0, y0, width, height]\n",
    "    colorbar_position = [0.905, 0.055, 0.09, 0.015]\n",
    "\n",
    "    # Add legend directly to the figure\n",
    "    legend_ax = fig.add_axes(legend_position, frame_on=False)\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='D', markeredgecolor='red', markerfacecolor='none', label='Ensemble median', markersize=10, linestyle='None', lw=2)]\n",
    "    for idx, model in enumerate(models):\n",
    "        if model != \"Ensemble median\":\n",
    "            legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='w', label=f\"{idx + 1}: {model}\", markersize=10))\n",
    "    \n",
    "    legend = legend_ax.legend(handles=legend_elements, fontsize=14, ncol=2, loc='center')\n",
    "    legend_ax.axis('off')\n",
    "\n",
    "    # Add the colorbar below the legend\n",
    "    cbar_ax = fig.add_axes(colorbar_position)\n",
    "    cbar = fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=bgws_cm), cax=cbar_ax, orientation='horizontal', extend='both')\n",
    "    cbar.set_label(\"$\\Delta$ Blue-Green Water Share\", fontsize=16, weight='bold')\n",
    "\n",
    "    # Define tick locations\n",
    "    tick_locs = [-0.1, -0.075, -0.05, -0.025, 0, 0.025, 0.05, 0.075, 0.1]\n",
    "\n",
    "    # Define tick labels\n",
    "    tick_labels = [\"-0.1\", \"-0.075\", \"-0.05\", \"-0.025\", \"0\", \"0.025\", \"0.05\", \"0.075\", \"0.1\"]\n",
    "\n",
    "    # Set the ticks and tick labels\n",
    "    cbar.set_ticks(tick_locs)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        savepath = os.path.join('../..', 'results', 'CMIP6', 'comparison', 'regional_var_change')\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        filename = f'regional_var_{change}_{experiment_id}_{months}_{yearly_sum}.pdf'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        fig.savefig(filepath, dpi=600, bbox_inches='tight', format='pdf')\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45feda64-9daa-4585-bcf8-f6277fec2d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_region_change(ds_dict_region_change_ssp126, selected_indices=\"ALL\", save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3bbca-1676-4c37-b3b8-8a19e9edb401",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_region_change(ds_dict_region_change_ssp370, selected_indices=\"ALL\", save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795c4e4-544d-49b1-a603-5232d4e29827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_region_change(ds_dict_region_change_ssp585, selected_indices=\"ALL\", save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da520c0a-945b-46ec-9340-b01b16325f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustern und bgws nicht den change plotten, da ich den plot sonst sehr schwer zu \n",
    "# verstehen finde. Grn kann nmich bedeuten, dass sehr positive (runoff dominiert) bgws regionen etwas\n",
    "# negativer werden. Eventuell doch absolute vernderung plotten. Gleich gre relative Abnahme von runoff \n",
    "# und Transpiration knnen bedeuten, dass runoff viel mehr abnimmt in absoluten Zahlen und deswegen\n",
    "# BGWS abnimmt (grn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f5d03-f367-4e6f-8edf-c10f88ec754a",
   "metadata": {},
   "source": [
    "### Compute global median change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab319a-3613-4b15-a98f-6b6d0b5055dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Change\n",
    "ds_dict_change = compute_change(ds_dict_hist_period_metric, ds_dict_ssp126_period_metric, var_rel_change=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2016863-0724-4f44-a814-d1a97a3e6b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute ensemble median for the change\n",
    "ds_dict_change_ensemble = compute_ensemble(ds_dict_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8488c-115a-4344-a0fa-e855fc5ef005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only use ensemble median for further analysis\n",
    "ds_ensmed_glob = ds_dict_change_ensemble['Ensemble median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78296b-272c-499c-9ad3-972f9d3cc4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_ensmed_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ac0d7-b920-4aab-9987-c1d3d330caf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_region_change_ssp126_ensemble = compute_ensemble(ds_dict_region_change_ssp126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15879a-0013-4b06-aac9-c6ffd8e8a02b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds_dict_region_change_ssp126_ensemble['Ensemble median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf65bc1-3c7e-410a-98f1-dac85e9ca91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded64b21-332d-464c-ba2a-68327bfc1c94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot clusters on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c115f4e-131f-4bb3-865f-9773c0e6f58e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import ShapelyFeature\n",
    "import regionmask\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6258ed-b2ba-465b-8543-87c5b8255f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "def split_polygon(polygon, meridian=180):\n",
    "    \"\"\"\n",
    "    Splits a Shapely polygon into two polygons at a specified meridian\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    if maxx > meridian and minx < -meridian:\n",
    "        # Polygon crosses the antimeridian\n",
    "        left_poly = []\n",
    "        right_poly = []\n",
    "        for x, y in polygon.exterior.coords:\n",
    "            if x >= meridian:\n",
    "                right_poly.append((x - 360, y))  # Wraparound for the right side\n",
    "            else:\n",
    "                left_poly.append((x, y))\n",
    "        return [Polygon(left_poly), Polygon(right_poly)]\n",
    "    else:\n",
    "        return [polygon]  # Wrap the single polygon in a list for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbe31d-a20f-4a58-b15e-148334bb5dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_clusters_and_regions(ds_ensmed_glob, ds, clustered_df):\n",
    "    # Initialize the plot with a cartopy projection\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "    ax_main = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "    \n",
    "    # Plot the 'bgws' variable from the dataset\n",
    "    ds_ensmed_glob['bgws'].plot(ax=ax_main, vmin=-0.2, vmax=0.2, cmap=bgws_cm, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "    \n",
    "    # Add coastlines and gridlines\n",
    "    ax_main.coastlines()\n",
    "    ax_main.tick_params(axis='both', which='major', labelsize=20)\n",
    "    gridlines = ax_main.gridlines(draw_labels=True, color='black', alpha=0.2, linestyle='--')\n",
    "    gridlines.top_labels = gridlines.right_labels = False\n",
    "    gridlines.xlabel_style = {'size': 18}\n",
    "    gridlines.ylabel_style = {'size': 18}\n",
    "    \n",
    "    # Get region bounds using regionmask\n",
    "    land_regions = regionmask.defined_regions.ar6.land\n",
    "    \n",
    "    # Create a mapping from region names to region numbers\n",
    "    region_name_to_number = dict(zip(ds.names.values, ds.region.values))\n",
    "    \n",
    "    # Map the region names to the same order as ds.names.values\n",
    "    region_to_cluster_map = dict(zip(clustered_df['Region'], clustered_df['Cluster']))\n",
    "\n",
    "    # Now create a new column in ds that maps the region names to cluster numbers\n",
    "    # This assumes ds.names.values has the same region names as in clustered_df['Region']\n",
    "    ds['Cluster'] = [region_to_cluster_map[name] for name in ds.names.values]\n",
    "\n",
    "    # Convert the 'Cluster' DataArray to a NumPy array and get unique values\n",
    "    unique_clusters = np.unique(ds['Cluster'].values)\n",
    "\n",
    "    # Prepare colors for clusters - this assumes a finite number of clusters\n",
    "    cluster_colors = plt.cm.tab20b(np.linspace(0, 1, len(unique_clusters)))\n",
    "\n",
    "\n",
    "    # Loop over the regions and plot the cluster numbers with abbreviations\n",
    "    for region_name, cluster_number in zip(ds.names.values, ds['Cluster']):\n",
    "        reg_num = region_name_to_number[region_name]\n",
    "        region_polygons = land_regions[reg_num].polygon\n",
    "        \n",
    "        region_abbr = ds.abbrevs.values[ds.region.values == reg_num][0]  # Assuming this gives us the correct abbreviation\n",
    "        cluster_color = cluster_colors[cluster_number]  # Get the color for the cluster\n",
    "        \n",
    "        # Fetch the polygon or polygons for this region\n",
    "        region_obj = land_regions[reg_num]\n",
    "        if hasattr(region_obj, 'polygons'):\n",
    "            # If the attribute is 'polygons', we assume it's iterable (e.g., a list of Polygon objects)\n",
    "            region_polygons = region_obj.polygons\n",
    "        elif hasattr(region_obj, 'polygon'):\n",
    "            # If there's only one Polygon, we wrap it in a list to make it iterable\n",
    "            region_polygons = [region_obj.polygon]\n",
    "        else:\n",
    "            raise AttributeError(f\"The region object does not have 'polygons' or 'polygon' attribute.\")\n",
    "\n",
    "        for region_polygon in region_polygons:\n",
    "            # If the polygon crosses the antimeridian, split it\n",
    "            split_polys = split_polygon(region_polygon)\n",
    "\n",
    "            # Handle both Polygon and MultiPolygon types after splitting\n",
    "            for poly in split_polys:\n",
    "                if isinstance(poly, Polygon):\n",
    "                    features_to_plot = [poly]\n",
    "                elif isinstance(poly, MultiPolygon):\n",
    "                    features_to_plot = list(poly.geoms)\n",
    "                else:\n",
    "                    raise TypeError(f\"Unhandled geometry type: {type(poly)}\")\n",
    "\n",
    "                for feature_poly in features_to_plot:\n",
    "                    feature = ShapelyFeature([feature_poly], ccrs.PlateCarree(), edgecolor=cluster_color, facecolor='none', linewidth=2)\n",
    "                    ax_main.add_feature(feature)\n",
    "\n",
    "            # Calculate the centroid for text placement using features_to_plot\n",
    "            centroids = [feature_poly.centroid for feature_poly in features_to_plot]\n",
    "    \n",
    "            text_lon, text_lat = max(centroids, key=lambda c: c.x).coords[0]  # Use the easternmost centroid\n",
    "        \n",
    "            # Ensure cluster_number is a plain integer if it's a single-item array or DataArray\n",
    "            if isinstance(cluster_number, np.ndarray) and cluster_number.size == 1:\n",
    "                cluster_number = cluster_number.item()  # Converts a one-element array to a scalar\n",
    "            elif isinstance(cluster_number, xr.DataArray) and cluster_number.ndim == 0:\n",
    "                cluster_number = cluster_number.values.item()  # Gets the scalar value from a 0-dim DataArray\n",
    "\n",
    "            # Annotate the cluster number for each region\n",
    "            ax_main.text(text_lon, text_lat, f\"{region_abbr}\\n{cluster_number}\",\n",
    "                         horizontalalignment='center', verticalalignment='center', transform=ccrs.PlateCarree(),\n",
    "                         fontsize=20, bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9eeaf4-9075-4d1f-bf71-cbf978c00ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_clusters_and_regions(ds_ensmed_glob, ds, clustered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769075e-bdb5-436c-b452-e123f4c51c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_ensemble_average_per_cluster(clustered_df, ds_dict):\n",
    "    \"\"\"\n",
    "    Calculate the ensemble average for each variable across all models, grouped by cluster,\n",
    "    and append the region names corresponding to each cluster.\n",
    "\n",
    "    :param clustered_df: DataFrame with regions and their corresponding cluster assignments.\n",
    "    :param ds_dict: Dictionary of xarray.Dataset objects with percentage changes for each model.\n",
    "    :return: A DataFrame with the ensemble mean for each variable for each cluster, along with region names.\n",
    "    \"\"\"\n",
    "    # Aggregate data across all models\n",
    "    ensemble_data = []\n",
    "\n",
    "    # Extract all variables from the first dataset as a reference\n",
    "    reference_ds = list(ds_dict.values())[0]\n",
    "    variables = list(reference_ds.data_vars)\n",
    "\n",
    "    for var in variables:\n",
    "        # Check if the variable exists in all models\n",
    "        if all(var in ds.data_vars for ds in ds_dict.values()):\n",
    "            var_all_models = [ds[var].to_dataframe().reset_index()[['region', var]] for ds in ds_dict.values()]\n",
    "            var_combined = pd.concat(var_all_models, axis=0)\n",
    "            ensemble_mean = var_combined.groupby('region').mean()\n",
    "            ensemble_data.append(ensemble_mean)\n",
    "\n",
    "    # Combine the ensemble means for all variables\n",
    "    combined_ensemble = pd.concat(ensemble_data, axis=1)\n",
    "\n",
    "    # Merge with cluster assignments using the region index\n",
    "    combined_ensemble.reset_index(inplace=True)\n",
    "    combined_ensemble = combined_ensemble.merge(clustered_df, left_on='region', right_index=True)\n",
    "\n",
    "    # Group by cluster and calculate the mean for each variable, excluding 'region'\n",
    "    final_stats_df = combined_ensemble.drop(columns=['region']).groupby('Cluster').mean(numeric_only=True)\n",
    "\n",
    "    # Append region names to each cluster\n",
    "    region_names = reference_ds.abbrevs.values\n",
    "    clustered_regions = clustered_df.reset_index().groupby('Cluster')['index'].apply(\n",
    "        lambda x: [region_names[i] for i in x])\n",
    "    final_stats_df['Regions'] = clustered_regions\n",
    "    \n",
    "    return final_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e894c-9520-4497-a6f5-bd632a614585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_changes_per_cluster = calculate_ensemble_average_per_cluster(clustered_df, ds_dict_region_change_ssp126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5316a3-dcb2-4e4b-a120-d6fcc6d0aefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_changes_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ffd62-d910-4b4d-a452-cfb720f2fb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def plot_cluster_distributions(clustered_df, ensemble_ds):\n",
    "    # Convert the xarray Dataset to a pandas DataFrame\n",
    "    ensemble_df = ensemble_ds.to_dataframe().reset_index()\n",
    "\n",
    "    # Multiply 'bgws' by 100\n",
    "    ensemble_df['bgws'] *= 100\n",
    "\n",
    "    # Melt the DataFrame to long-form for plotting\n",
    "    long_df = ensemble_df.melt(id_vars=['region', 'abbrevs', 'names'],\n",
    "                               var_name='Variable', value_name='Value')\n",
    "\n",
    "    # Merge with cluster assignments\n",
    "    long_df = long_df.merge(clustered_df, left_on='names', right_on='Region')\n",
    "\n",
    "    # Number of unique clusters\n",
    "    num_clusters = clustered_df['Cluster'].nunique()\n",
    "\n",
    "    # Calculate the layout for subplots (3 columns)\n",
    "    num_cols = 3\n",
    "    num_rows = np.ceil(num_clusters / num_cols).astype(int)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Define markers for regions\n",
    "    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'H', 'X']  # All filled markers\n",
    "    unique_abbrevs = long_df['abbrevs'].unique()\n",
    "    marker_dict = {abbrev: markers[i % len(markers)] for i, abbrev in enumerate(unique_abbrevs)}\n",
    "\n",
    "    # Iterate over each cluster to create a scatter plot\n",
    "    for cluster_num in range(num_clusters):\n",
    "        ax = axes[cluster_num]\n",
    "        cluster_data = long_df[long_df['Cluster'] == cluster_num]\n",
    "        sns.scatterplot(data=cluster_data, x='Variable', y='Value', ax=ax, hue='abbrevs', \n",
    "                        style='abbrevs', markers=marker_dict, legend='brief')\n",
    "        # Use region abbreviations\n",
    "        region_abbrevs = cluster_data['abbrevs'].unique()\n",
    "        ax.set_title(f'Cluster {cluster_num}: {\", \".join(region_abbrevs)}')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Setting the legend outside the plot\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_clusters, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b47be-ac47-46a2-8c3f-c5201befb4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_cluster_distributions(clustered_df, ensemble_ds):\n",
    "    # Convert the xarray Dataset to a pandas DataFrame\n",
    "    ensemble_df = ensemble_ds.to_dataframe().reset_index()\n",
    "\n",
    "    # Multiply 'bgws' by 100\n",
    "    ensemble_df['bgws'] *= 100\n",
    "\n",
    "    # Exclude 'Cluster' from the variables if present\n",
    "    ensemble_df = ensemble_df.drop(columns=['Cluster'], errors='ignore')\n",
    "\n",
    "    # Melt the DataFrame to long-form for plotting\n",
    "    long_df = ensemble_df.melt(id_vars=['region', 'abbrevs', 'names'],\n",
    "                               var_name='Variable', value_name='Value')\n",
    "\n",
    "    # Merge with cluster assignments\n",
    "    long_df = long_df.merge(clustered_df, left_on='names', right_on='Region')\n",
    "\n",
    "    # Number of unique clusters\n",
    "    num_clusters = clustered_df['Cluster'].nunique()\n",
    "\n",
    "    # Calculate the layout for subplots (3 columns)\n",
    "    num_cols = 3\n",
    "    num_rows = np.ceil(num_clusters / num_cols).astype(int)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Define markers for regions\n",
    "    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'H', 'X']  # All filled markers\n",
    "    unique_abbrevs = long_df['abbrevs'].unique()\n",
    "    marker_dict = {abbrev: markers[i % len(markers)] for i, abbrev in enumerate(unique_abbrevs)}\n",
    "\n",
    "    # Iterate over each cluster to create a scatter plot\n",
    "    for cluster_num in range(num_clusters):\n",
    "        ax = axes[cluster_num]\n",
    "        cluster_data = long_df[long_df['Cluster'] == cluster_num]\n",
    "        sns.scatterplot(data=cluster_data, x='Variable', y='Value', ax=ax, hue='abbrevs', \n",
    "                        style='abbrevs', markers=marker_dict, legend='brief')\n",
    "        # Use region abbreviations\n",
    "        region_abbrevs = cluster_data['abbrevs'].unique()\n",
    "        ax.set_title(f'Cluster {cluster_num}: {\", \".join(region_abbrevs)}')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Setting the legend outside the plot\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_clusters, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b234e-1455-4b2c-a974-5688a01d088e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_cluster_distributions(clustered_df, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0171b487-9048-4fc5-8676-c32f79f7e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop regions that cluster alone:\n",
    "RAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05543f05-8d9c-49a4-8b5c-0751786caf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "mypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
