{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7716ffb9-a564-44c7-87d4-2d4caeecdb1c",
   "metadata": {},
   "source": [
    "# CMIP6 Statistics and Plots\n",
    "\n",
    "**Following steps are included in this script:**\n",
    "\n",
    "1. Load netCDF files\n",
    "2. Compute statistics\n",
    "3. Plot statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854af429-43a1-489d-8513-dca8c1466663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Packages ==========\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import dask\n",
    "from matplotlib import rcParams\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from cftime import DatetimeNoLeap\n",
    "import glob\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams[\"mathtext.default\"] = 'regular'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5daa16-4811-4773-b98d-6b69626ff6cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766cd9d-393f-4db9-b0a3-744388ad88c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistic_single(ds, statistic, dimension, yearly_mean=True):\n",
    "    if dimension == \"time\":\n",
    "        stat_ds = getattr(ds, statistic)(\"time\", keep_attrs=True, skipna=True)\n",
    "        stat_ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "    if dimension == \"space\":\n",
    "        # Assign the period attribute before grouping by year\n",
    "        ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "        if yearly_mean:\n",
    "            ds = ds.groupby('time.year').mean('time', keep_attrs=True, skipna=True)\n",
    "            ds.attrs['mean'] = 'yearly mean'\n",
    "            \n",
    "        \n",
    "        #get the weights, apply on data, and compute statistic\n",
    "        weights = np.cos(np.deg2rad(ds.lat))\n",
    "        weights.name = \"weights\"\n",
    "        ds_weighted = ds.weighted(weights)\n",
    "        stat_ds = getattr(ds_weighted, statistic)((\"lon\", \"lat\"), keep_attrs=True, skipna=True)\n",
    "    \n",
    "    stat_ds.attrs['statistic'] = statistic\n",
    "    stat_ds.attrs['statistic_dimension'] = dimension\n",
    "\n",
    "    return stat_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadad33e-375e-4f6f-94b1-918dce7e2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistic(ds_dict, statistic, dimension, start_year=None, end_year=None, yearly_mean=True):\n",
    "    \"\"\"\n",
    "    Computes the specified statistic for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        statistic (str): The statistic to compute, which can be one of 'mean', 'std', 'min', 'var', or 'median'.\n",
    "        dimension (str): The dimension to compute over, which can be 'time' or 'space'.\n",
    "        start_year (str, optional): The start year of the period to compute the statistic over.\n",
    "        end_year (str, optional): The end year of the period to compute the statistic over.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with computed statistic for each dataset.\n",
    "    \"\"\"\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if statistic not in [\"mean\", \"std\", \"min\", \"max\", \"var\", \"median\"]:\n",
    "        raise ValueError(f\"Invalid statistic '{statistic}' specified.\")\n",
    "    if dimension not in [\"time\", \"space\"]:\n",
    "        raise ValueError(f\"Invalid dimension '{dimension}' specified.\")\n",
    "\n",
    "    if start_year is not None and end_year is not None:\n",
    "        # Convert integer years to DatetimeNoLeap format\n",
    "        start_year = DatetimeNoLeap(start_year, 1, 16) # 16th of January of start year\n",
    "        end_year = DatetimeNoLeap(end_year, 12, 16) # 16th of December of end year\n",
    "        ds_dict = {k: v.sel(time=slice(start_year, end_year)) for k, v in ds_dict.items()}\n",
    "        \n",
    "    # Use multiprocessing to compute the statistic for each dataset in parallel\n",
    "    with mp.Pool() as pool:\n",
    "        results = pool.starmap(compute_statistic_single, [(ds, statistic, dimension, yearly_mean) for ds in ds_dict.values()])\n",
    "\n",
    "    return dict(zip(ds_dict.keys(), results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173177e7-1327-46be-9952-4ba5d9d45155",
   "metadata": {},
   "source": [
    "### 1. Load netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000af7f-db51-4fd2-a219-e72cbb403fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "variable=['evspsbl', 'gpp', 'huss', 'lai', 'mrro', 'pr', 'tran', 'lmrso_1m', 'lmrso_2m']\n",
    "experiment_id = 'historical'\n",
    "source_id = ['TaiESM1', 'AWI-ESM-1-1-LR', 'BCC-CSM2-MR', 'BCC-ESM1', 'CanESM5', 'CNRM-CM6-1','CNRM-CM6-1-HR', 'CNRM-ESM2-1', 'IPSL-CM6A-LR',  'UKESM1-0-LL', 'MPI-ESM1-2-LR', 'CESM2', 'CESM2-FV2', 'CESM2-WACCM', 'CESM2-WACCM-FV2',  'NorESM2-MM'] #\n",
    "folder='preprocessed'\n",
    "\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# ========= Create a helper function to open the dataset ========\n",
    "def open_dataset(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    return ds\n",
    "\n",
    "# Define a helper function to open and merge datasets\n",
    "def open_and_merge_datasets(folder, model, experiment_id, variables):\n",
    "    filepaths = []\n",
    "    for var in variables:\n",
    "        path = f'../../data/CMIP6/{experiment_id}/{folder}/{var}'\n",
    "        fp = glob.glob(os.path.join(path, f'CMIP.{model}.{experiment_id}.{var}_regridded.nc'))\n",
    "        if fp:\n",
    "            filepaths.append(fp[0])\n",
    "        else:\n",
    "            #print(f\"No file found for variable '{var}' in model '{model}'.\")\n",
    "            print(fp)\n",
    "\n",
    "    datasets = [xr.open_dataset(fp) for fp in filepaths]\n",
    "    ds = xr.merge(datasets)\n",
    "    return ds\n",
    "\n",
    "# Create dictionary using a dictionary comprehension and Dask\n",
    "ds_dict = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, variable) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f749cba-efb8-4532-9e4b-a7b880530fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= Have a look into the data ==============\n",
    "print(ds_dict.keys())\n",
    "ds_dict[list(ds_dict.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22c103-7cc8-487f-9799-b21767d83939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert integer years to DatetimeNoLeap format\n",
    "start_year=1930\n",
    "end_year=2014\n",
    "\n",
    "start_year = DatetimeNoLeap(start_year, 1, 16) # 16th of January of start year\n",
    "end_year = DatetimeNoLeap(end_year, 12, 16) # 16th of December of end year\n",
    "ds_dict = {k: v.sel(time=slice(start_year, end_year)) for k, v in ds_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de9ead-0d36-4f06-9430-9efeae7cc9bb",
   "metadata": {},
   "source": [
    "### 2. Compute statistics and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a9c21-8094-4700-b972-36e3f720d629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_stat = compute_statistic(ds_dict, 'mean', 'time', start_year=1985, end_year=2014) #, yearly_mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df0ee9-d467-459f-9731-06192c1cd068",
   "metadata": {},
   "source": [
    "### 3. Perfrom hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36485811-79b5-4db6-9bee-592299f4fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd66653-932f-443e-a0fe-6529818517b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable=['evspsbl', 'gpp', 'huss', 'lai', 'pr', 'tran', 'lmrso_1m', 'lmrso_2m']\n",
    "#variable=['evspsbl', 'pr', 'huss']\n",
    "#variable=['lai', 'mrro', 'gpp', 'tran', 'lmrso_1m', 'lmrso_2m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cbc934-cf1a-44f9-9ba0-a8ed633a2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold the flattened data arrays\n",
    "data = {}\n",
    "\n",
    "# Loop over all variables\n",
    "for var in variable: \n",
    "    # Initialize an empty list to hold the flattened data arrays for each model\n",
    "    data_arrays = []\n",
    "    \n",
    "    # Loop over all models\n",
    "    for model in ds_dict.keys():\n",
    "        # Extract the data for this variable and model\n",
    "        var_data = ds_dict[model][var].values.flatten()\n",
    "        # Exclude NaN values\n",
    "        var_data = var_data[~np.isnan(var_data)]\n",
    "        \n",
    "        # Check if the array is empty\n",
    "        if len(var_data) == 0:\n",
    "            # Assign NaN as the mean value\n",
    "            mean_data = np.nan\n",
    "        else:\n",
    "            # Compute the mean of the data\n",
    "            mean_data = var_data.mean()\n",
    "        \n",
    "        # Add it to the list\n",
    "        data_arrays.append(mean_data)\n",
    "    \n",
    "    # Add the variable and its corresponding data arrays to the dictionary\n",
    "    data[var] = data_arrays\n",
    "\n",
    "# Create a DataFrame from the data dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set the row labels using the model names\n",
    "df.index = list(ds_dict.keys())\n",
    "\n",
    "# Convert the DataFrame to a numpy array\n",
    "data_matrix = df.to_numpy()\n",
    "\n",
    "# Standardize the DataFrame\n",
    "scaler = StandardScaler()\n",
    "df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "# Perform hierarchical/agglomerative clustering\n",
    "# The 'ward' method has generally good performance\n",
    "linked = linkage(df_standardized, method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689e331-d9d2-4db4-9606-97bc712fd3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the cluster assignments using a desired number of clusters\n",
    "num_clusters = 3\n",
    "cluster_assignments = fcluster(linked, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Calculate the cluster weights\n",
    "cluster_weights = np.bincount(cluster_assignments) / len(cluster_assignments)\n",
    "\n",
    "print(\"Cluster Weights:\", cluster_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a9fd9-1822-4cc0-bc1c-54958d42354c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "\n",
    "# Suppress the warning messages\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "\n",
    "# Initialize an empty list to store the within-cluster sum of squares (WCSS) values\n",
    "wcss = []\n",
    "\n",
    "# Define the range of numbers of clusters to try\n",
    "num_clusters_range = range(1, 10)\n",
    "\n",
    "# Calculate the WCSS for each number of clusters\n",
    "for num_clusters in num_clusters_range:\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(linked)\n",
    "    \n",
    "    # Append the WCSS to the list\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the WCSS values\n",
    "plt.plot(num_clusters_range, wcss)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b336b-8f50-4254-9a15-7741aa0d2d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize an empty list to store the silhouette scores\n",
    "silhouette_scores = []\n",
    "\n",
    "# Define the range of numbers of clusters to try\n",
    "num_clusters_range = range(2, 10)\n",
    "\n",
    "# Calculate the silhouette score for each number of clusters\n",
    "for num_clusters in num_clusters_range:\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    labels = kmeans.fit_predict(df_standardized)\n",
    "    \n",
    "    # Calculate the silhouette score\n",
    "    silhouette_scores.append(silhouette_score(df_standardized, labels))\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.plot(num_clusters_range, silhouette_scores)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e6d75-824a-4126-b65b-8a0e9b528680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef1dd5-a48b-47f9-a916-8142fecc4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "# Perform hierarchical/agglomerative clustering\n",
    "# The 'ward' method has generally good performance\n",
    "linked = linkage(df_standardized, method='ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e892f6a-5246-4cf4-8544-9744ab7f15bd",
   "metadata": {},
   "source": [
    "4. Plot the clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee05ad2-6b96-4fcd-8d0b-355eeee1543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels for the plots\n",
    "\n",
    "labels= ['TaiESM1 - (TaiAM1 // CLM4.0)',\n",
    " 'AWI-ESM-1-1-LR - (ECHAM6.3.04p1 // JSBACH 3.20 + dyn veg)',\n",
    " 'BCC-CSM2-MR - (BCC_AGCM3_MR // BCC_AVIM2)',\n",
    " 'BCC-ESM1 - (BCC_AGCM3_LR // BCC_AVIM2)',\n",
    " 'CanESM5 - (CanAM5 // CLASS3.6/CTEM1.2)',\n",
    " 'CNRM-CM6-1 - (Arpege 6.3 // Surfex 8.0c)',\n",
    " 'CNRM-CM6-1-HR - (Arpege 6.3 // Surfex 8.0c)',\n",
    " 'CNRM-ESM2-1 - (Arpege 6.3 // Surfex 8.0c)',\n",
    " 'IPSL-CM6A-LR - (LMDZ // ORCHIDEE)',\n",
    " 'UKESM1-0-LL - (MetUM-HadGEM3-GA7.1 // JULES-ES-1.0)',\n",
    " 'MPI-ESM1-2-LR - (ECHAM6.3 // JSBACH4.20)',\n",
    " 'CESM2 - (CAM6 // CLM5)',\n",
    " 'CESM2-FV2 - (CAM6 // CLM5)',\n",
    " 'CESM2-WACCM - (WACCM6 // CLM5)',\n",
    " 'CESM2-WACCM-FV2 - (WACCM6 // CLM5)',\n",
    " 'NorESM2-MM - (CAM_OSLO5 // CLM5)']\n",
    "\n",
    "labels_ls= ['TaiESM1 - (CLM4.0)',\n",
    " 'AWI-ESM-1-1-LR - (JSBACH 3.20 + dyn veg)',\n",
    " 'BCC-CSM2-MR - (BCC_AVIM2)',\n",
    " 'BCC-ESM1 - (BCC_AVIM2)',\n",
    " 'CanESM5 - (CLASS3.6/CTEM1.2)',\n",
    " 'CNRM-CM6-1 - (Surfex 8.0c)',\n",
    " 'CNRM-CM6-1-HR - (Surfex 8.0c)',\n",
    " 'CNRM-ESM2-1 - (Surfex 8.0c)',\n",
    " 'IPSL-CM6A-LR - (ORCHIDEE)',\n",
    " 'UKESM1-0-LL - (JULES-ES-1.0)',\n",
    " 'MPI-ESM1-2-LR - (JSBACH4.20)',\n",
    " 'CESM2 - (CLM5)',\n",
    " 'CESM2-FV2 - (CLM5)',\n",
    " 'CESM2-WACCM - (CLM5)',\n",
    " 'CESM2-WACCM-FV2 - (CLM5)',\n",
    " 'NorESM2-MM - (CLM5)']\n",
    "\n",
    "labels_atm= ['TaiESM1 - (TaiAM1)',\n",
    " 'AWI-ESM-1-1-LR - (ECHAM6.3.04p1)',\n",
    " 'BCC-CSM2-MR - (BCC_AGCM3_MR)',\n",
    " 'BCC-ESM1 - (BCC_AGCM3_LR)',\n",
    " 'CanESM5 - (CanAM5)',\n",
    " 'CNRM-CM6-1 - (Arpege 6.3)',\n",
    " 'CNRM-CM6-1-HR - (Arpege 6.3)',\n",
    " 'CNRM-ESM2-1 - (Arpege 6.3)',\n",
    " 'IPSL-CM6A-LR - (LMDZ)',\n",
    " 'UKESM1-0-LL - (MetUM-HadGEM3-GA7.1)',\n",
    " 'MPI-ESM1-2-LR - (ECHAM6.3)',\n",
    " 'CESM2 - (CAM6)',\n",
    " 'CESM2-FV2 - (CAM6)',\n",
    " 'CESM2-WACCM - (WACCM6)',\n",
    " 'CESM2-WACCM-FV2 - (WACCM6)',\n",
    " 'NorESM2-MM - (CAM_OSLO5)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60fd113-2302-43ed-8bfb-df19a88bddb1",
   "metadata": {},
   "source": [
    "Radial tree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7013c5d1-be99-4aa3-ae0d-14be856ecd7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import radialtree as rt\n",
    "\n",
    "Z2 = dendrogram(linked,labels=labels,no_plot=True)\n",
    "\n",
    "# plot a circular dendrogram\n",
    "rt.plot(Z2)\n",
    "# Add a title\n",
    "plt.title('Dendrogram of Historical CMIP6 Data (1985-2014)', y=1.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717aa5e-1b62-431f-a760-dafa280a1e91",
   "metadata": {},
   "source": [
    "Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50794bfe-b595-4624-841f-a78f2e424a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the desired colormap\n",
    "sns.set_palette('hsv')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7), frameon=False)\n",
    "dendrogram(linked, labels=labels,\n",
    "           orientation='left')#,\n",
    "           #distance_sort='descending',\n",
    "           #show_leaf_counts=True)\n",
    "\n",
    "# Remove the frame\n",
    "plt.box(False)\n",
    "\n",
    "# Rotate the x-axis tick labels vertically\n",
    "#plt.xticks(rotation='vertical')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Dendrogram of Historical CMIP6 Data (1985-2014)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f537b-ebbf-4b46-a7d6-320cfbd17451",
   "metadata": {},
   "source": [
    "Clustermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbcf936-8412-46ea-8d2f-828e6556aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustermap=sns.clustermap(df_standardized, cmap='viridis')\n",
    "clustermap.ax_heatmap.set_title(\"Cluster Map of Historical CMIP6 Data (1985-2014)\", y=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e52992-4ee8-49ce-a2c1-b4f2cf90abed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "mypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
