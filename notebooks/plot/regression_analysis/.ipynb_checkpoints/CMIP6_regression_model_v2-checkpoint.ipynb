{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7716ffb9-a564-44c7-87d4-2d4caeecdb1c",
   "metadata": {},
   "source": [
    "# CMIP6 Regression and Driver Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5a4f3-40bb-459a-88f0-f180c8b3ee8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2abf7d-6400-4813-af39-ae142ef75fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and preprocess\n",
    "import load_and_preprocess as lap\n",
    "\n",
    "# Statistics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "\n",
    "# Regression models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import matplotlib.cm\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# For color map\n",
    "from matplotlib.colors import LinearSegmentedColormap, BoundaryNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f28f1-e87d-4475-850f-1df51ea220d3",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ca7b2-8a4c-476e-b1b6-76d3d2cdff4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict = lap.load_and_preprocess(vars='all', scenarios=['historical', 'ssp370'], models='all', period=None, yearly_sum=False, period_statistic='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d1995-0cba-4005-af68-9f1f61b4293f",
   "metadata": {},
   "source": [
    "## 2. Subdivide Regions, Compute Mean and Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e75d8b1-6909-4cac-b557-83cd569aa420",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_regions = lap.subdivide_region_and_compute_mean(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f475a7-4705-4d0a-b99a-243381713d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_regions_change = lap.compute_change_dict(ds_dict_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f560d7c-c3b9-47c7-8d9c-04941b086af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd0b08-ccf5-4647-8c01-c64e5f0994e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2937ed6-e84d-4870-a3c1-edf6597aeefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854af429-43a1-489d-8513-dca8c1466663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== Packages ==========\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams[\"mathtext.default\"] = 'regular'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5daa16-4811-4773-b98d-6b69626ff6cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb57b43-bbbe-414b-8075-8cda5d1ea4ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b6f18-2d39-4b69-a33a-954e0e83fd13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Create a helper function to open the dataset ========\n",
    "def open_dataset(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    return ds\n",
    "\n",
    "# Define a helper function to open and merge datasets\n",
    "def open_and_merge_datasets(folder, model, experiment_id, temp_res, variables):\n",
    "    filepaths = []\n",
    "    for var in variables:\n",
    "        path = f'../../data/CMIP6/{experiment_id}/{folder}/{temp_res}/{var}'\n",
    "        fp = glob.glob(os.path.join(path, f'CMIP.{model}.{experiment_id}.{var}.regridded.nc'))\n",
    "        if fp:\n",
    "            filepaths.append(fp[0])\n",
    "        else:\n",
    "            print(f\"No file found for variable '{var}' in model '{model}'.\")\n",
    "            print(fp)\n",
    "\n",
    "    datasets = [xr.open_dataset(fp) for fp in filepaths]\n",
    "    ds = xr.merge(datasets)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9148cef-f033-40ac-aeef-dec5ccdb277d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0755d44-865c-439e-b702-f2ccb30e1010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_var(ds_dict, var):\n",
    "    for name, ds in ds_dict.items():\n",
    "        ds_dict[name] = ds.drop(var)\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402da3c7-b918-4a5c-a00e-d8b6004d9893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_period(ds_dict, start_year=None, end_year=None, period=None, yearly_sum=False):\n",
    "    '''\n",
    "    Helper function to select periods and optionally compute yearly sums.\n",
    "    \n",
    "    Parameters:\n",
    "    ds_dict (dict): Dictionary with xarray datasets.\n",
    "    start_year (int): The start year of the period.\n",
    "    end_year (int): The end year of the period.\n",
    "    period (int, list, str, None): Single month (int), list of months (list), multiple seasons (str) to select,\n",
    "                                   or None to not select any specific period.\n",
    "    yearly_sum (bool): If True, compute the yearly sum over the selected period.\n",
    "    '''\n",
    "    # Define season to month mapping for northern hemisphere\n",
    "    seasons_to_months = {\n",
    "        'nh_winter': [12, 1, 2],\n",
    "        'nh_spring': [3, 4, 5],\n",
    "        'nh_summer': [6, 7, 8],\n",
    "        'nh_fall': [9, 10, 11]\n",
    "    }\n",
    "\n",
    "    months = []\n",
    "\n",
    "    # If no specific period is selected, all data will be used.\n",
    "    if period is None:\n",
    "        months = None\n",
    "    elif isinstance(period, int):\n",
    "        months = [period]\n",
    "    elif isinstance(period, str):\n",
    "        # Check if the input is a single season or multiple seasons\n",
    "        if 'and' in period:\n",
    "            seasons = period.lower().split('and')\n",
    "            for season in seasons:\n",
    "                season = season.strip()\n",
    "                months.extend(seasons_to_months.get(season, []))\n",
    "        else:\n",
    "            months = seasons_to_months.get(period.lower(), [])\n",
    "    elif isinstance(period, list):\n",
    "        months = period\n",
    "    else:\n",
    "        raise ValueError(\"Period must be None, an integer, a string representing a single season, \"\n",
    "                         \"a string with multiple seasons separated by 'and', or a list of integers.\")\n",
    "\n",
    "    for k, ds in ds_dict.items():\n",
    "        if start_year and end_year:\n",
    "            start_date = f'{start_year}-01-01'\n",
    "            end_date = f'{end_year}-12-31'\n",
    "            ds = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # If months are specified, select those months\n",
    "        if months is not None:\n",
    "            month_mask = xr.DataArray(ds['time.month'].isin(months), coords=ds['time'].coords)\n",
    "            ds = ds.where(month_mask, drop=True)\n",
    "\n",
    "        # If yearly_sum is True, sum over 'time' dimension to get yearly sum\n",
    "        if yearly_sum:\n",
    "            ds = ds.resample(time='AS').sum(dim='time')\n",
    "\n",
    "        ds_dict[k] = ds\n",
    "\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a690d-ac2f-4875-96dc-66e8424a4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Standardize ========\n",
    "def standardize(ds_dict):\n",
    "    '''\n",
    "    Helper function to standardize datasets of a dictionary\n",
    "    '''\n",
    "    ds_dict_stand = {}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        ds_stand = (ds - ds.mean()) / ds.std()\n",
    "\n",
    "        # Preserve variable attributes from the original dataset\n",
    "        for var in ds.variables:\n",
    "            if var in ds_stand.variables:\n",
    "                ds_stand[var].attrs = ds[var].attrs\n",
    "\n",
    "        ds_stand.attrs = attrs\n",
    "        ds_dict_stand[name] = ds_stand\n",
    "        \n",
    "    return ds_dict_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090c127-204b-4d69-9e33-5769b28e4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_args_and_get_info(ds_dict, variable):\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if not isinstance(variable, str):\n",
    "        raise TypeError('variable must be a string.')\n",
    "        \n",
    "    # Dictionary to store plot titles for each statistic\n",
    "    titles = {\"mean\": \"Mean\", \"std\": \"Standard deviation of yearly means\", \"min\": \"Minimum\", \"max\": \"Maximum\", \"median\": \"Median\", \"time\": \"Time\", \"space\": \"Space\"}\n",
    "    freq = {\"mon\": \"Monthly\"}\n",
    "    \n",
    "    long_name = {\n",
    "        'Precipitation': 'Precipitation',\n",
    "        'Total Runoff': 'Total Runoff',\n",
    "        'Vapor Pressure Deficit': 'Vapor Pressure Deficit',\n",
    "        'Evaporation Including Sublimation and Transpiration': 'Evapotranspiration',\n",
    "        'Transpiration': 'Transpiration',\n",
    "        'Leaf Area Index': 'Leaf Area Index',\n",
    "        'Carbon Mass Flux out of Atmosphere Due to Gross Primary Production on Land [kgC m-2 s-1]': 'Gross Primary Production',\n",
    "        'Total Liquid Soil Moisture Content of 1 m Column': '1 m Soil Moisture',\n",
    "        'Total Liquid Soil Moisture Content of 2 m Column': '2 m Soil Moisture',\n",
    "        'Runoff - Precipitation': 'Runoff - Precipitation',\n",
    "        'Transpiration - Precipitation': 'Transpiration - Precipitation',\n",
    "        '(Runoff + Transpiration) - Precipitation':  '(Runoff + Transpiration) - Precipitation',\n",
    "        'ET - Precipitation':  'ET - Precipitation', \n",
    "        'Negative Runoff': 'Negative Runoff',\n",
    "    }\n",
    "   \n",
    "    # Data information\n",
    "    var_long_name = ds_dict[list(ds_dict.keys())[0]][variable].long_name\n",
    "    period = f\"{ds_dict[list(ds_dict.keys())[0]].attrs['period'][0]}-{ds_dict[list(ds_dict.keys())[0]].attrs['period'][1]}\"\n",
    "    experiment_id =  ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    unit = ds_dict[list(ds_dict.keys())[0]][variable].units\n",
    "    statistic_dim = ds_dict[list(ds_dict.keys())[0]].statistic_dimension\n",
    "    statistic = ds_dict[list(ds_dict.keys())[0]].attrs['statistic']\n",
    "    frequency = freq[ds_dict[list(ds_dict.keys())[0]].frequency]\n",
    "\n",
    "    return var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279066b-3218-4895-929d-b2e1dd5bab19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regionmask\n",
    "\n",
    "def apply_region_mask(ds_dict, with_global=False):\n",
    "    \"\"\"\n",
    "    Applies the AR6 land region mask to datasets in the provided dictionary, adds a region dimension,\n",
    "    and optionally includes a 'Global' aggregation.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets.\n",
    "        with_global (bool): If True, includes a 'Global' region with aggregated data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary where keys are the same as in the input dictionary,\n",
    "              and each value is an xarray Dataset with a region dimension added to each variable,\n",
    "              and optionally includes a 'Global' region.\n",
    "    \"\"\"\n",
    "\n",
    "    land_regions = regionmask.defined_regions.ar6.land\n",
    "    \n",
    "    if with_global:\n",
    "        global_mask = regionmask.defined_regions.natural_earth_v5_0_0.land_110\n",
    "    \n",
    "    ds_masked_dict = {}\n",
    "\n",
    "    for ds_name, ds in ds_dict.items():\n",
    "        ds_masked = xr.Dataset()  # Initiate an empty Dataset for the masked data\n",
    "        \n",
    "        # Get attributes\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        for var in ds:\n",
    "            # Get the binary mask\n",
    "            mask = land_regions.mask_3D(ds[var])\n",
    "            \n",
    "            var_attrs = ds[var].attrs\n",
    "\n",
    "            # Multiply the original data with the mask to get the masked data\n",
    "            masked_var = ds[var] * mask\n",
    "\n",
    "            # Replace 0s with NaNs, if desired\n",
    "            masked_var = masked_var.where(masked_var != 0)\n",
    "\n",
    "            if with_global:\n",
    "                # Convert the global mask to 3D to match the regional mask dimensions\n",
    "                glob_mask = global_mask.mask_3D(ds[var])\n",
    "                \n",
    "                global_masked_var = ds[var] * glob_mask\n",
    "                \n",
    "                # Replace 0s with NaNs, if desired\n",
    "                global_masked_var = global_masked_var.where(global_masked_var != 0)\n",
    "\n",
    "                # Combine masked data\n",
    "                masked_var = xr.concat([masked_var, global_masked_var], dim='region')\n",
    "                \n",
    "            # Add the masked variable to the output Dataset\n",
    "            ds_masked[var] = masked_var\n",
    "\n",
    "            ds_masked[var].attrs = var_attrs\n",
    "\n",
    "        # Copy dataset attributes\n",
    "        ds_masked.attrs.update(ds.attrs)\n",
    "        \n",
    "        correct_region_numbers = np.arange(0, ds_masked.dims['region'])\n",
    "\n",
    "        ds_masked = ds_masked.assign_coords(region=correct_region_numbers)\n",
    "\n",
    "        # Add the modified dataset to the dictionary\n",
    "        ds_masked_dict[ds_name] = ds_masked\n",
    "\n",
    "    return ds_masked_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e4851-532d-4dd6-af51-15e12b7761f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regionmask\n",
    "\n",
    "def apply_region_mask(ds_dict):\n",
    "    \"\"\"\n",
    "    Applies the AR6 land region mask to datasets in the provided dictionary and adds a region dimension.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary where keys are the same as in the input dictionary,\n",
    "              and each value is an xarray Dataset with a region dimension added to each variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    land_regions = regionmask.defined_regions.ar6.land\n",
    "    ds_masked_dict = {}\n",
    "    \n",
    "    for ds_name, ds in ds_dict.items():\n",
    "        ds_out = xr.Dataset()  # Initiate an empty Dataset for the masked data\n",
    "        \n",
    "        # Get attributes\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        for var in ds:\n",
    "            # Get the binary mask\n",
    "            mask = land_regions.mask_3D(ds[var])\n",
    "            \n",
    "            var_attrs = ds[var].attrs\n",
    "\n",
    "            # Multiply the original data with the mask to get the masked data\n",
    "            masked_var = ds[var] * mask\n",
    "\n",
    "            # Replace 0s with NaNs, if desired\n",
    "            masked_var = masked_var.where(masked_var != 0)\n",
    "\n",
    "            # Add the masked variable to the output Dataset\n",
    "            ds_out[var] = masked_var\n",
    "            \n",
    "            ds_out[var].attrs = var_attrs\n",
    "            \n",
    "        # Add the attributes\n",
    "        ds_out.attrs = attrs\n",
    "\n",
    "        # Add the Dataset to the output dictionary\n",
    "        ds_masked_dict[ds_name] = ds_out\n",
    "\n",
    "    return ds_masked_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfd1d3-d787-4484-aa0e-fbedcd496cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_ensemble(ds_dict_change, metric='mean'):\n",
    "    for key in ['Ensemble mean', 'Ensemble median']:\n",
    "        if key in ds_dict_change:\n",
    "            ds_dict_change.pop(key)\n",
    "            \n",
    "    var_attrs = {}\n",
    "\n",
    "    # Drop 'member_id' coordinate if it exists in any of the datasets\n",
    "    for ds_key in ds_dict_change:\n",
    "        if 'member_id' in ds_dict_change[ds_key].coords:\n",
    "            ds_dict_change[ds_key] = ds_dict_change[ds_key].drop('member_id')\n",
    "        for var in list(ds_dict_change[ds_key].data_vars.keys()):\n",
    "            var_attrs[var] = ds_dict_change[ds_key][var].attrs\n",
    "\n",
    "    combined = xr.concat(ds_dict_change.values(), dim='ensemble')\n",
    "    ds_dict_change[f'Ensemble {metric}'] = getattr(combined, metric)(dim='ensemble')\n",
    "    \n",
    "    for ds_key in ds_dict_change:\n",
    "        for var in list(ds_dict_change[f'Ensemble {metric}'].data_vars.keys()):\n",
    "            ds_dict_change[f'Ensemble {metric}'][var].attrs = var_attrs[var]\n",
    "    \n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a2736-eb0f-466e-b547-4d293e8a5af5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2f45a-7c54-4e4b-a62d-c43186adb78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_spatial_mean(ds_dict):\n",
    "    ds_dict_mean = {}\n",
    "    for key, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        for var in list(ds.data_vars.keys()):\n",
    "            var_attrs = ds[var].attrs\n",
    "            \n",
    "            ds_dict_mean[key][var] = ds.mean(['lon', 'lat'])\n",
    "            ds_dict_mean[key][var].attrs = var_attrs\n",
    "        \n",
    "        ds_dict_mean[key].attrs = attrs\n",
    "        \n",
    "    return ds_dict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766cd9d-393f-4db9-b0a3-744388ad88c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic_single(ds, statistic, dimension, yearly_mean=True):\n",
    "    if dimension == \"time\":\n",
    "        stat_ds = getattr(ds, statistic)(\"time\", keep_attrs=True, skipna=True)\n",
    "        stat_ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "    if dimension == \"space\":\n",
    "        # Assign the period attribute before grouping by year\n",
    "        ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "        if yearly_mean:\n",
    "            ds = ds.groupby('time.year').mean('time', keep_attrs=True, skipna=True)\n",
    "            ds.attrs['mean'] = 'yearly mean'\n",
    "            \n",
    "        \n",
    "        #get the weights, apply on data, and compute statistic\n",
    "        weights = np.cos(np.deg2rad(ds.lat))\n",
    "        weights.name = \"weights\"\n",
    "        ds_weighted = ds.weighted(weights)\n",
    "        stat_ds = getattr(ds_weighted, statistic)((\"lon\", \"lat\"), keep_attrs=True, skipna=True)\n",
    "    \n",
    "    stat_ds.attrs['statistic'] = statistic\n",
    "    stat_ds.attrs['statistic_dimension'] = dimension\n",
    "\n",
    "    return stat_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadad33e-375e-4f6f-94b1-918dce7e2f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic(ds_dict, statistic, dimension, start_year=None, end_year=None, yearly_mean=True):\n",
    "    \"\"\"\n",
    "    Computes the specified statistic for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        statistic (str): The statistic to compute, which can be one of 'mean', 'std', 'min', 'var', or 'median'.\n",
    "        dimension (str): The dimension to compute over, which can be 'time' or 'space'.\n",
    "        start_year (str, optional): The start year of the period to compute the statistic over.\n",
    "        end_year (str, optional): The end year of the period to compute the statistic over.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with computed statistic for each dataset.\n",
    "    \"\"\"\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if statistic not in [\"mean\", \"std\", \"min\", \"max\", \"var\", \"median\"]:\n",
    "        raise ValueError(f\"Invalid statistic '{statistic}' specified.\")\n",
    "    if dimension not in [\"time\", \"space\"]:\n",
    "        raise ValueError(f\"Invalid dimension '{dimension}' specified.\")\n",
    "\n",
    "    # Select period\n",
    "    if start_year is not None and end_year is not None:\n",
    "        select_period(ds_dict, start_year=start_year, end_year=end_year)\n",
    "        \n",
    "        \n",
    "    # Use multiprocessing to compute the statistic for each dataset in parallel\n",
    "    with mp.Pool() as pool:\n",
    "        results = pool.starmap(compute_statistic_single, [(ds, statistic, dimension, yearly_mean) for ds in ds_dict.values()])\n",
    "\n",
    "    return dict(zip(ds_dict.keys(), results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d176a36-95a5-4a18-b46f-a311e49243f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def precompute_metrics(ds_dict, variables, metrics=['pearson']):\n",
    "    # Initialize the results dictionary\n",
    "    results_dict = {metric: {} for metric in metrics}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        # Create a DataFrame with all the variables\n",
    "        df = pd.DataFrame({var: ds[var].values.flatten() for var in variables})\n",
    "        \n",
    "        # Define all pairs of variables\n",
    "        pairs = list(permutations(variables, 2))  # <-- Change here\n",
    "        args = [(df, var1, var2, metrics) for var1, var2 in pairs]\n",
    "\n",
    "        # Use a multiprocessing pool to compute the metrics for all pairs\n",
    "        with Pool() as p:\n",
    "            results = p.map(compute_metrics_for_pair, args)\n",
    "        \n",
    "        # Store the results in the results_dict\n",
    "        for var1, var2, metric_dict in results:\n",
    "            for metric, value in metric_dict.items():\n",
    "                # Ensure the keys exist in the dictionary\n",
    "                results_dict[metric].setdefault(name, {}).setdefault(f'{var1}_{var2}', value)\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9381cb-6944-4552-b6c7-aac1b6a3c6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_stats(ds_dict):\n",
    "    \"\"\"\n",
    "    Compute yearly mean of each variable in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds_dict (dict): The input dictionary of xarray.Dataset.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are the dataset names and the values are another dictionary.\n",
    "          This inner dictionary has keys as variable names and values as DataArray of yearly means.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    for model, ds in ds_dict.items():\n",
    "        # Compute the yearly mean\n",
    "        yearly_ds = ds.resample(time='1Y').mean()\n",
    "\n",
    "        stats[model] = {}\n",
    "        for var in yearly_ds.data_vars:\n",
    "            # Compute the spatial mean\n",
    "            spatial_mean = yearly_ds[var].mean(dim=['lat', 'lon'])\n",
    "            \n",
    "            # Store the yearly mean values\n",
    "            stats[model][var] = spatial_mean\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43015f97-e54e-4f20-aa0c-3e10f0c8d748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_yearly_means(ds_dict):\n",
    "    yearly_means_dict = {}\n",
    "\n",
    "    # For each dataset, compute the yearly mean over the 'time', 'lat', and 'lon' dimensions\n",
    "    for name, ds in ds_dict.items():  \n",
    "        ds_yearly = ds.groupby('time.year').mean('time')    \n",
    "        \n",
    "        yearly_means_dict[name] = ds_yearly\n",
    "\n",
    "    return yearly_means_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbbc45-b47c-461b-99db-d20d9fa0625d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_yearly_regional_means(ds_dict_region):\n",
    "    yearly_means_dict = {}\n",
    "\n",
    "    # For each dataset, compute the yearly mean over the 'time', 'lat', and 'lon' dimensions\n",
    "    for region, ds_dict in ds_dict_region.items():\n",
    "        yearly_means_dict[region] = {}\n",
    "        for ds_name, ds in ds_dict.items():\n",
    "            # Compute the yearly mean\n",
    "            ds_yearly = ds.groupby('time.year').mean('time')\n",
    "            \n",
    "            # Create weights\n",
    "            weights = np.cos(np.deg2rad(ds.lat))\n",
    "            # Apply the weights and calculate the spatial mean\n",
    "            ds_weighted = ds_yearly.weighted(weights)\n",
    "            yearly_means_dict[region][ds_name] = ds_weighted.mean(('lat', 'lon'))\n",
    "\n",
    "    return yearly_means_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36692801-5a4b-4e4d-bd3e-268e3de553d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_change(ds_dict_hist, ds_dict_fut, var_rel_change=None):\n",
    "    ds_dict_change = {}\n",
    "    \n",
    "    fut_scenario = ds_dict_fut[list(ds_dict_fut.keys())[0]].experiment_id\n",
    "    \n",
    "    if var_rel_change == \"ALL\":\n",
    "        var_rel_change = list(ds_dict_hist[next(iter(ds_dict_hist))].data_vars.keys())\n",
    "    elif var_rel_change is None:\n",
    "        var_rel_change = []\n",
    "\n",
    "    for name, ds in ds_dict_hist.items():\n",
    "        if name in ds_dict_fut:\n",
    "            ds_f = ds_dict_fut[name]\n",
    "            \n",
    "            change_ds = xr.Dataset()\n",
    "            \n",
    "            for variable in ds.data_vars:\n",
    "                if variable in var_rel_change:\n",
    "                    # Compute relative change only where ds is not 0\n",
    "                    rel_change = ds[variable].where(ds[variable] != 0)\n",
    "                    rel_change = ((ds_f[variable] - rel_change) / abs(rel_change)) * 100\n",
    "                    \n",
    "                    # Where ds was 0, set the corresponding relative change to np.nan\n",
    "                    rel_change = rel_change.where(ds[variable] != 0)\n",
    "                    \n",
    "                    change_ds[variable] = rel_change\n",
    "                    \n",
    "                else:\n",
    "                    change_ds[variable] = ds_f[variable] - ds[variable]\n",
    "                    \n",
    "                change_ds[variable].attrs = ds_f[variable].attrs\n",
    "\n",
    "            ds_dict_change[name] = change_ds\n",
    "\n",
    "            ds_dict_change[name].attrs = {\n",
    "                'period': f'Change {fut_scenario} - Historical',\n",
    "                'statistic': ds_dict_fut[list(ds_dict_fut.keys())[0]].statistic,\n",
    "                'statistic_dimension': ds_dict_fut[list(ds_dict_fut.keys())[0]].statistic_dimension,\n",
    "                'experiment_id': fut_scenario.lower() + '-historical',\n",
    "                'source_id': ds_dict_fut[list(ds_dict_fut.keys())[0]].source_id,\n",
    "                'frequency': ds_dict_fut[list(ds_dict_fut.keys())[0]].frequency,\n",
    "                'change': 'Mixed Change',\n",
    "            }\n",
    "\n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c1277-aa7c-451c-8ad4-6efac3e2a78b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_numeric(data):\n",
    "    try:\n",
    "        _ = data.astype(float)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def compute_change(ds_dict_hist, ds_dict_fut, var_rel_change=None):\n",
    "    ds_dict_change = {}\n",
    "\n",
    "    for name, ds_hist in ds_dict_hist.items():\n",
    "        if name in ds_dict_fut:\n",
    "            ds_future = ds_dict_fut[name]\n",
    "            common_vars = set(ds_hist.data_vars).intersection(ds_future.data_vars)\n",
    "\n",
    "            ds_change = ds_hist.copy(deep=True)\n",
    "            \n",
    "            if var_rel_change == 'all':\n",
    "                var_rel_change = common_vars\n",
    "                \n",
    "            for var in common_vars:\n",
    "                if is_numeric(ds_hist[var].data) and is_numeric(ds_future[var].data):\n",
    "                    # Always compute percentage change for 'mrso' as models have different depths\n",
    "                    if var == 'mrso':\n",
    "                        rel_change = (ds_future[var] - ds_hist[var]) / ds_hist[var].where(ds_hist[var] != 0) * 100\n",
    "                        ds_change[var].data = rel_change.data\n",
    "                        ds_change[var].attrs['units'] = '%'\n",
    "                    elif var_rel_change is not None and var in var_rel_change:\n",
    "                        # Compute relative change where ds_hist is not zero for specified variables\n",
    "                        rel_change = (ds_future[var] - ds_hist[var]) / ds_hist[var].where(ds_hist[var] != 0) * 100\n",
    "                        ds_change[var].data = rel_change.data\n",
    "                        ds_change[var].attrs['units'] = '%'\n",
    "                    else:\n",
    "                        # Compute absolute change for other variables\n",
    "                        abs_change = ds_future[var] - ds_hist[var]\n",
    "                        ds_change[var].data = abs_change.data\n",
    "\n",
    "            ds_change.attrs = ds_future.attrs\n",
    "            ds_dict_change[name] = ds_change\n",
    "\n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b71e4e-b89b-4169-bac4-97f010c2f2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_spatial_mean(ds_dict):\n",
    "    ds_dict_mean = {}\n",
    "    \n",
    "    for key, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        # Initialize a new Dataset for this key\n",
    "        ds_dict_mean[key] = xr.Dataset()\n",
    "        \n",
    "        for var in list(ds.data_vars.keys()):\n",
    "            var_attrs = ds[var].attrs\n",
    "            \n",
    "            ds_dict_mean[key][var] = ds[var].mean(['lon', 'lat'])\n",
    "            ds_dict_mean[key][var].attrs = var_attrs\n",
    "        \n",
    "        ds_dict_mean[key].attrs = attrs\n",
    "        \n",
    "    return ds_dict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354754c-b13c-4650-b31b-7ae9e9e424ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_bgws(ds_dict):\n",
    "\n",
    "    for model, ds in ds_dict.items():\n",
    "        bgws = (ds['mrro']-ds['tran'])/ds['pr']\n",
    "\n",
    "        # Replace infinite values with NaN\n",
    "        bgws = xr.where(np.isinf(bgws), float('nan'), bgws)\n",
    "\n",
    "        # Set all values above 2 and below -2 to NaN\n",
    "        bgws = xr.where(bgws > 2, float('nan'), bgws)\n",
    "        bgws = xr.where(bgws < -2, float('nan'), bgws)\n",
    "\n",
    "        ds_dict[model]['bgws'] = bgws\n",
    "        ds_dict[model]['bgws'].attrs = {'long_name': 'Blue Green Water Share',\n",
    "                             'units': ''}\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c460ccd-131b-4c40-a8c1-2181a9346593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_wue(ds_dict):\n",
    "\n",
    "    for model, ds in ds_dict.items():\n",
    "        if 'gpp' in ds.variables:\n",
    "            wue = ds['gpp']/ds['tran']\n",
    "\n",
    "            # Replace infinite values with NaN\n",
    "            wue = xr.where(np.isinf(wue), float('nan'), wue)\n",
    "\n",
    "            # Set all values above 4 and below -4 to ±5\n",
    "            wue = xr.where(wue > 4, 5, wue)\n",
    "            wue = xr.where(wue < -4, -5, wue)\n",
    "\n",
    "            ds_dict[model]['wue'] = wue\n",
    "            ds_dict[model]['wue'].attrs = {'long_name': 'Water Use Efficiency',\n",
    "                                 'units': ''}\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fbc11d-adbf-4362-bc0b-8d03f553e503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_rgtr(ds_dict):\n",
    "\n",
    "    for model, ds in ds_dict.items():\n",
    "        if 'gpp' in ds.variables:\n",
    "            \n",
    "            gpp_standardized = ds['gpp'] / ds['gpp'].max()\n",
    "            tas_standardized = (ds['tas'] + 273.15) / (ds['tas'] + 273.15).max()\n",
    "\n",
    "            # Compute Relative GPP-Temperature Response\n",
    "            rgtr = gpp_standardized / tas_standardized\n",
    "\n",
    "            # Replace infinite values with NaN\n",
    "            rgtr = xr.where(np.isinf(rgtr), float('nan'), rgtr)\n",
    "            \n",
    "            # Set all values below 0 to 0\n",
    "            rgtr = xr.where(rgtr < 0, 0, rgtr)\n",
    "\n",
    "            ds_dict[model]['rgtr'] = rgtr\n",
    "            ds_dict[model]['rgtr'].attrs = {'long_name': 'Relative GPP-Temperature Response',\n",
    "                                 'units': ''}\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a4589-6801-4d80-bb7f-281c8fbd6a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_etp(ds_dict):\n",
    "    \"\"\"\n",
    "    Computes the partitioning of evapotranspiration into evaporation and transpiration components,\n",
    "    scaled between -100 and 100 to represent the share of evaporation to transpiration in the total\n",
    "    evapotranspiration (ET), where -100 indicates total evaporation and 100 indicates total transpiration.\n",
    "\n",
    "    Parameters:\n",
    "    - ds_dict (dict): A dictionary of datasets, where each dataset contains variables\n",
    "                      for evaporation (E), transpiration (T), and evapotranspiration (ET).\n",
    "\n",
    "    Returns:\n",
    "    - dict: The same dictionary with an added variable for each model representing the\n",
    "            Evapotranspiration Partitioning metric.\n",
    "    \"\"\"\n",
    "    \n",
    "    for model, ds in ds_dict.items():\n",
    "        if {'evapo', 'tran', 'evspsbl'}.issubset(ds.variables):\n",
    "            # Ensure ET is consistent with E + T if necessary or use directly as provided\n",
    "            total_ET = ds['evspsbl']\n",
    "            \n",
    "            # Calculate the partitioning of ET into E and T, scaled between -100 and 100\n",
    "            et_partitioning = ((ds['evapo'] - ds['tran']) / total_ET) * 100\n",
    "            \n",
    "            # Handle division by zero or invalid operations\n",
    "            #et_partitioning = xr.where(np.isnan(et_partitioning) | np.isinf(et_partitioning), float('nan'), et_partitioning)\n",
    "            \n",
    "            # Set all values above 100 and below -100 to ±100\n",
    "            #et_partitioning = xr.where(et_partitioning > 100, 100, et_partitioning)\n",
    "            #et_partitioning = xr.where(et_partitioning < -100, -100, et_partitioning)\n",
    "            \n",
    "            # Add the new metric to the dataset\n",
    "            ds_dict[model]['et_partitioning'] = et_partitioning\n",
    "            ds_dict[model]['et_partitioning'].attrs = {\n",
    "                'long_name': 'Evapotranspiration Partitioning',\n",
    "                'units': '%',\n",
    "                'description': 'Indicates the share of Evaporation to Transpiration in ET, scaling between -100 (total evaporation) and 100 (total transpiration)'\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Model {model} lacks necessary variables ('evapo', 'tran', 'evspsbl'). Skipping.\")\n",
    "            \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c6728-aa37-48ed-88ee-2bca84fa1521",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39238c09-3874-41ae-a859-a782bb16917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_correlations(yearly_correlations, variable_pairs, target_variable):\n",
    "    \"\"\"\n",
    "    Plots the time series of correlations for each variable pair that includes the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    yearly_correlations (dict): The output from calculate_yearly_correlations.\n",
    "    variable_pairs (list of tuples): The pairs of variables that the correlations were calculated for.\n",
    "    target_variable (str): The variable that must be included in a pair for it to be plotted.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the number of plots\n",
    "    n_plots = sum([var1 == target_variable or var2 == target_variable for var1, var2 in variable_pairs])\n",
    "\n",
    "    # Calculate the dimensions of the grid of subplots\n",
    "    grid_size = math.ceil(math.sqrt(n_plots))\n",
    "    \n",
    "    # Create the figure\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15), sharex=True, sharey=True)\n",
    "\n",
    "    # Flatten the axes\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Create an index for the current plot\n",
    "    i_plot = 0\n",
    "\n",
    "    # Prepare a list to store handles and labels for the legend\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for var1, var2 in variable_pairs:\n",
    "        if var1 == target_variable or var2 == target_variable:\n",
    "            # Select the current axes\n",
    "            ax = axs[i_plot]\n",
    "            \n",
    "            # Construct the correlation variable name\n",
    "            corr_var = f'{var1}-{var2}'\n",
    "\n",
    "            # Prepare a list to store correlations of all models\n",
    "            all_corrs = []\n",
    "\n",
    "            for name, ds in yearly_correlations.items():\n",
    "                # Check if this variable exists in the Dataset\n",
    "                if corr_var in ds:\n",
    "                    # Plot the time series of the correlation\n",
    "                    line, = ax.plot(ds['time'], ds[corr_var], label=name)\n",
    "\n",
    "                    # Append to all_corrs\n",
    "                    all_corrs.append(ds[corr_var])\n",
    "\n",
    "                    # Append to handles and labels if not already present\n",
    "                    if name not in labels:\n",
    "                        handles.append(line)\n",
    "                        labels.append(name)\n",
    "\n",
    "            # Compute the mean correlation across all models\n",
    "            mean_corr = xr.concat(all_corrs, dim='model').mean(dim='model')\n",
    "            mean_line, = ax.plot(mean_corr['time'], mean_corr, color='black', linestyle='--')\n",
    "\n",
    "            ax.set_xlabel('Year')\n",
    "            ax.set_ylabel('Correlation')\n",
    "            ax.set_title(f'{var1} vs {var2}')\n",
    "            ax.grid(True)\n",
    "\n",
    "            # Increment the plot index\n",
    "            i_plot += 1\n",
    "\n",
    "    # Add 'Mean' to the legend\n",
    "    handles.append(mean_line)\n",
    "    labels.append('Mean')\n",
    "\n",
    "    # Show the figure with a legend\n",
    "    fig.legend(handles, labels, loc='center right', bbox_to_anchor=(1.1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10db210-8757-4858-ae1e-4f89e7949440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_mean_correlations(correlations_hist, correlations_ssp370, variable_pairs, target_variable, scale_axis=False, variable_captions=None):\n",
    "    \"\"\"\n",
    "    Plots the mean correlations for each variable pair that includes the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    correlations_hist (dict): The output from calculate_correlations for the historical period.\n",
    "    correlations_ssp370 (dict): The output from calculate_correlations for the SSP370 scenario.\n",
    "    variable_pairs (list of tuples): The pairs of variables that the correlations were calculated for.\n",
    "    target_variable (str): The variable that must be included in a pair for it to be plotted.\n",
    "    scale_axis (bool): Whether to scale the y-axis according to metric value ranges. Default is False.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get info\n",
    "    metric = correlations_hist[list(correlations_hist.keys())[0]].Metric\n",
    "    metric_sign = correlations_hist[list(correlations_hist.keys())[0]].Metric_sign\n",
    "    \n",
    "    # Filter variable pairs\n",
    "    variable_pairs = [(var1, var2) for var1, var2 in variable_pairs if var1 == target_variable or var2 == target_variable]\n",
    "\n",
    "    # Calculate the number of plots and dimensions of the grid of subplots\n",
    "    n_plots = len(variable_pairs)\n",
    "    n_cols = min(n_plots, 3)  # Maximum 3 plots in a row\n",
    "    n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "    # Create the figure\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows), squeeze=False)\n",
    "    axs = axs.flatten()  # Flatten the axes array\n",
    "    \n",
    "    for i, (var1, var2) in enumerate(variable_pairs):\n",
    "        # Prepare lists to store yearly correlation values\n",
    "        yearly_corr_hist, yearly_corr_ssp370 = [], []\n",
    "\n",
    "        for name in correlations_hist.keys():\n",
    "            # Extract the yearly mean values for the historical period\n",
    "            yearly_corr_hist.append(correlations_hist[name][f'{var1}-{var2}'].values)\n",
    "\n",
    "            # Extract the yearly mean values for the SSP370 scenario\n",
    "            yearly_corr_ssp370.append(correlations_ssp370[name][f'{var1}-{var2}'].values)\n",
    "\n",
    "        # Compute the box plot positions\n",
    "        positions = np.arange(len(correlations_hist.keys()))\n",
    "\n",
    "        # Select the current axes\n",
    "        ax = axs[i]\n",
    "\n",
    "        # Define an offset for x-values to place box plots for different periods side by side\n",
    "        offset = 0.15\n",
    "\n",
    "        # Plot the box plots for the historical period\n",
    "        ax.boxplot(yearly_corr_hist, positions=positions-offset, widths=0.3, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='cornflowerblue'), medianprops=dict(color='black'), \n",
    "                   showfliers=True)\n",
    "\n",
    "        # Plot the box plots for the SSP370 scenario\n",
    "        ax.boxplot(yearly_corr_ssp370, positions=positions+offset, widths=0.3, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='sandybrown'), medianprops=dict(color='black'), \n",
    "                   showfliers=True)\n",
    "\n",
    "        # Set the x-ticks labels and the title\n",
    "        ax.set_ylabel(f'{metric_sign}')\n",
    "        ax.set_title(f'{variable_captions.get(var1, var1)} x {variable_captions.get(var2, var2)}', fontsize=9)\n",
    "        ax.set_xticks(positions)\n",
    "        ax.set_xticklabels(list(correlations_hist.keys()), rotation=90)\n",
    "        \n",
    "    fig.suptitle(f'Yearly {metric} for Historical (1985-2014) and SSP370 (2071-2100) Period', fontsize=12, y=1.0)\n",
    "    \n",
    "    # Set a legend\n",
    "    axs[0].legend([Patch(facecolor='cornflowerblue'), Patch(facecolor='sandybrown')], ['Historical', 'SSP370'])\n",
    "\n",
    "    # Handle empty subplots in case n_plots is less than n_rows * n_cols\n",
    "    for i in range(n_plots, n_rows*n_cols):\n",
    "        fig.delaxes(axs[i])\n",
    "    \n",
    "    # Handle y-axis scaling\n",
    "    if scale_axis:\n",
    "        for ax in axs:\n",
    "            ax.set_ylim([-1, 1] if metric != 'r2_lr' else [0, 1])\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    suffix = \"_scaled_axis\" if scale_axis else \"\"\n",
    "    filename = f\"{metric}_changes_{target_variable}{suffix}.png\"\n",
    "    \n",
    "    savepath = f'../../results/CMIP6/yearly_metrics_comparison'\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    filepath = os.path.join(savepath, filename)\n",
    "\n",
    "    fig.savefig(filepath, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555de63-1237-4ed3-b496-48f9f835eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_corr_change_plot(ds_dict, target_variable, full_var_names_and_unit, cmap='viridis', save_fig=False, file_format='png'):\n",
    "    \"\"\"\n",
    "    Plots a map of the specified statistic of the given variable for the Ensemble_mean dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        target_variable (str): The target variable to plot correlations with.\n",
    "        cmap (str): The name of the colormap to use for the plot. Default is 'viridis'.\n",
    "        save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "        file_format (str): The format of the saved figure. Default is 'png'.\n",
    "    \"\"\"\n",
    "    # Info\n",
    "    experiment_id = ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    metric = ds_dict[list(ds_dict.keys())[0]].Metric\n",
    "    metric_sign = ds_dict[list(ds_dict.keys())[0]].Metric_sign\n",
    "    means = ds_dict[list(ds_dict.keys())[0]].attrs['means']\n",
    "    target_var_long_name = full_var_names_and_unit[target_variable][0]\n",
    "    \n",
    "    # Create a figure\n",
    "    n_cols = 3  # Set number of columns to 3\n",
    "    n_rows = 3  # Set number of rows to 3\n",
    "    \n",
    "    fig = plt.figure(figsize=[12 * n_cols, 6.5 * n_rows])  # Start with a blank figure, without subplots\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Loop over datasets and plot the requested statistic\n",
    "    subplot_counter = 0\n",
    "\n",
    "    # Get the Ensemble_mean dataset\n",
    "    ensemble_ds = ds_dict.get(\"Ensemble_mean\", None)\n",
    "\n",
    "    if ensemble_ds is None:\n",
    "        print(\"Ensemble_mean dataset not found.\")\n",
    "        return None\n",
    "\n",
    "    for variable in ensemble_ds.variables:\n",
    "        if not (f'{target_variable} x ' in variable or f' x {target_variable}' in variable):\n",
    "            continue\n",
    "\n",
    "        # Add a new subplot with a cartopy projection\n",
    "        ax = fig.add_subplot(n_rows, n_cols, subplot_counter+1, projection=ccrs.Robinson())\n",
    "\n",
    "        data_to_plot = ensemble_ds[variable]\n",
    "        im = data_to_plot.plot(ax=ax, cmap=cmap, vmin = -1, vmax = 1, transform=ccrs.PlateCarree(), add_colorbar=False)  # Added a cartopy transform to the plot and cmap parameter\n",
    "        \n",
    "        if f'{target_variable} x ' in variable:\n",
    "            corr_var = variable.replace(f'{target_variable} x ', '')\n",
    "        elif f' x {target_variable}' in variable:\n",
    "            corr_var = variable.replace(f' x {target_variable}', '')\n",
    "        else:\n",
    "            continue\n",
    "        corr_var_long_name = full_var_names_and_unit[corr_var][0]\n",
    "        ax.set_title(f'{target_var_long_name} x {corr_var_long_name}', fontsize=18)  # Use the long names in the title\n",
    "        ax.coastlines()  # Adds lines around the continents\n",
    "\n",
    "        subplot_counter += 1\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # Add a common colorbar at the bottom of the plots\n",
    "    cbar = fig.colorbar(im, ax=fig.axes, orientation='horizontal', fraction=0.02, pad=0.04, aspect=75, shrink=0.4)\n",
    "    \n",
    "\n",
    "    # Set colorbar ticks\n",
    "    cbar.set_ticks([-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75])\n",
    "    \n",
    "    \n",
    "    # Set tick size\n",
    "    cbar.ax.tick_params(labelsize=20)  # Adjust size as needed\n",
    "    \n",
    "    # Set colorbar label\n",
    "    cbar.set_label(f'{metric_sign} change', size=26)  # Adjust size as needed\n",
    "    \n",
    "    # Set figure title with first and last year of dataset \n",
    "    if experiment_id == 'historical' or experiment_id == 'ssp370':\n",
    "        fig.suptitle(f\"{metric} ({experiment_id}) of Ensemble Mean for Variable Combinations with {target_var_long_name} ({means})\", fontsize=20, y=0.9)\n",
    "    elif experiment_id == 'ssp370-historical':\n",
    "        fig.suptitle(f\"{metric} Change ({experiment_id}) of Ensemble Mean for Variable Combinations with {target_var_long_name} ({means})\", fontsize=20, y=0.9)\n",
    "    \n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        if experiment_id == 'historical' or experiment_id == 'ssp370':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename = f'ensmean_{target_variable}_correlations.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "        elif experiment_id == 'ssp370-historical':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename =f'ensmean_{target_variable}_correlations_change.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c30d8-7d61-497f-9fd6-689ad3634fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_maps(ds_dict, target_variable_combination, cmap='viridis', save_fig=False, file_format='png'):\n",
    "    \"\"\"\n",
    "    Plots a map of the specified variable combination for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        target_variable_combination (str): The target variable combination to plot.\n",
    "        cmap (str): The name of the colormap to use for the plot. Default is 'viridis'.\n",
    "        save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "        file_format (str): The format of the saved figure. Default is 'png'.\n",
    "    \"\"\"\n",
    "    # Info\n",
    "    experiment_id = ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    metric = ds_dict[list(ds_dict.keys())[0]].Metric\n",
    "    metric_sign = ds_dict[list(ds_dict.keys())[0]].Metric_sign\n",
    "    means = ds_dict[list(ds_dict.keys())[0]].attrs['means']\n",
    "    \n",
    "    # Create a figure\n",
    "    n_datasets_with_var = sum([1 for ds in ds_dict.values() if target_variable_combination in ds.variables])\n",
    "    n_cols = 3  # Set number of columns to 3\n",
    "    n_rows = math.ceil(n_datasets_with_var / n_cols)  # Calculate rows\n",
    "    \n",
    "    fig = plt.figure(figsize=[12 * n_cols, 6.5 * n_rows])  # Start with a blank figure, without subplots\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Loop over datasets and plot the requested variable combination\n",
    "    subplot_counter = 0\n",
    "    for i, (name, ds) in enumerate(ds_dict.items()):\n",
    "        if target_variable_combination not in ds.variables:\n",
    "            continue\n",
    "\n",
    "        # Add a new subplot with a cartopy projection\n",
    "        ax = fig.add_subplot(n_rows, n_cols, subplot_counter+1, projection=ccrs.Robinson())\n",
    "        \n",
    "        data_to_plot = ds[target_variable_combination]\n",
    "        im = data_to_plot.plot(ax=ax, cmap=cmap, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "        ax.set_title(name, fontsize=18)\n",
    "        ax.coastlines()  # Adds lines around the continents\n",
    "\n",
    "        subplot_counter += 1\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # Add a common colorbar at the bottom of the plots\n",
    "    cbar = fig.colorbar(im, ax=fig.axes, orientation='horizontal', fraction=0.02, pad=0.04, aspect=75, shrink=0.4)\n",
    "    \n",
    "    # Set tick size\n",
    "    cbar.ax.tick_params(labelsize=14)  # Adjust size as needed\n",
    "    \n",
    "    # Set colorbar label\n",
    "    cbar.set_label(metric_sign, size=18)  # Adjust size as needed\n",
    "    \n",
    "    # Set figure title with first and last year of dataset \n",
    "    if experiment_id == 'historical' or 'ssp370':\n",
    "        fig.suptitle(f\"{metric} ({experiment_id}) for Variable Combination {target_variable_combination} ({means})\", fontsize=20, y=0.9)\n",
    "    elif experiment_id == 'ssp370-historical':\n",
    "        fig.suptitle(f\"{metric} Change ({experiment_id}) for Variable Combination {target_variable_combination} ({means})\", fontsize=20, y=0.9)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        if experiment_id == 'historical' or 'ssp370':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename = f'{target_variable_combination}_correlation.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "        elif experiment_id == 'ssp370-historical':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', 'comparison', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename =f'{target_variable_combination}_correlation_change.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173177e7-1327-46be-9952-4ba5d9d45155",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343c372-5fab-4276-9c97-171ecae7340b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "#variables=['tas', 'pr', 'vpd', 'evspsbl', 'mrro', 'lmrso_1m', 'lmrso_2m', 'tran', 'lai', 'gpp', 'EI', 'wue']\n",
    "variables=['tas', 'pr', 'vpd', 'mrro', 'mrso', 'tran', 'lai', 'gpp', 'evapo', 'evspsbl']\n",
    "folder='preprocessed'\n",
    "temp_res = 'month'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# Create dictionary using a dictionary comprehension and Dask\n",
    "experiment_id = 'historical'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','GISS-E2-1-G','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','TaiESM1','UKESM1-0-LL']\n",
    "ds_dict_hist = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]\n",
    "\n",
    "experiment_id = 'ssp370'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','GISS-E2-1-G','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','TaiESM1','UKESM1-0-LL']\n",
    "ds_dict_ssp370 = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25d68c-26d5-4270-b0cb-26ca96b148bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist['GFDL-ESM4'].mrso.isel(time=1100).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae4915-1c1e-4b8f-8cbd-a98e513a5583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "variables=['RX5day']\n",
    "folder='preprocessed'\n",
    "temp_res = 'year'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# Create dictionary using a dictionary comprehension and Dask\n",
    "experiment_id = 'historical'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0','CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','UKESM1-0-LL']\n",
    "ds_dict_hist_idx = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]\n",
    "\n",
    "experiment_id = 'ssp370'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','UKESM1-0-LL']\n",
    "ds_dict_ssp370_idx = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84352a-1f0c-445c-9cdd-7b0094ed755f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "variables=['growing_season_length']\n",
    "folder='preprocessed'\n",
    "temp_res = 'period_mean'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# Create dictionary using a dictionary comprehension and Dask\n",
    "experiment_id = 'historical'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0','CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','UKESM1-0-LL']\n",
    "ds_dict_hist_period_mean = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]\n",
    "\n",
    "experiment_id = 'ssp370'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','UKESM1-0-LL']\n",
    "ds_dict_ssp370_period_mean = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f417b3-2b33-4081-b1d8-6d3a1c21d269",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29262fe-a12e-44ff-894e-6d851abac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_period = select_period(ds_dict_hist, start_year=1985, end_year=2014, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a12260-1df3-4232-bf91-196fb8144269",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period = select_period(ds_dict_ssp370, start_year=2071, end_year=2100, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de9ead-0d36-4f06-9430-9efeae7cc9bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b6390-f4b2-41d0-951d-c6bae78ce192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_hist_period_metric = compute_statistic(ds_dict_hist_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a9c21-8094-4700-b972-36e3f720d629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_ssp370_period_metric = compute_statistic(ds_dict_ssp370_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ccb3c7-78a6-4bbf-b45d-eb4dab386928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_idx_period_metric = compute_statistic(ds_dict_hist_idx, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def02d4-ea82-4a56-84d6-83982bfec8bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, ds in ds_dict_hist_idx_period_metric.items():\n",
    "    ds_dict_hist_period_metric[name]['RX5day'] = ds.RX5day\n",
    "    ds_dict_hist_period_metric[name]['growing_season_length'] = ds_dict_hist_period_mean[name]['growing_season_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39b485-3ec7-4aa1-9370-9f8a48fe7cec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_idx_period_metric = compute_statistic(ds_dict_ssp370_idx, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3a73e-65af-4a4a-8851-c32d881a23ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, ds in ds_dict_ssp370_idx_period_metric.items():\n",
    "    ds_dict_ssp370_period_metric[name]['RX5day'] = ds.RX5day\n",
    "    ds_dict_ssp370_period_metric[name]['growing_season_length'] = ds_dict_ssp370_period_mean[name]['growing_season_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6bc0a-0837-4359-bd61-a6ad7408853e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute BGWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e99e6-709a-410c-abaa-c9a087e0346f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric = compute_bgws(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b485b-57a9-4d84-9581-52e28e732963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric_ensemble = compute_ensemble(ds_dict_hist_period_metric, metric='mean')\n",
    "ds_ensmed_glob_hist = ds_dict_hist_period_metric_ensemble['Ensemble mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d56759-5317-4815-936e-9b352c10589a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric = compute_bgws(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1123cf2-5611-412b-ac06-d9343976247b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric_ensemble = compute_ensemble(ds_dict_ssp370_period_metric, metric='mean')\n",
    "ds_ensmed_glob_ssp370 = ds_dict_ssp370_period_metric_ensemble['Ensemble mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975c11c-0e20-49c4-aae4-c89b1b475f0b",
   "metadata": {},
   "source": [
    "### Compute WUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f251b61-bdb5-40b3-9ad7-24bec6ffa735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric = compute_wue(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa9c022-de07-405e-a822-4e891cb93da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric = compute_wue(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fde11d-dd24-4c5a-9868-b11705db5255",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Copmute RGTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551965c-1972-4c46-ad85-fb8f0c2ea318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric = compute_rgtr(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7fac7-44f8-465f-9a84-d8ad4a70b7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric = compute_rgtr(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42727c92-13b4-4c0c-84a3-e74e6c58b755",
   "metadata": {},
   "source": [
    "### Compute ETP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98bd90c-cb9c-4132-a22e-1e2573c9c32d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric = compute_etp(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e58f83-4c41-47d1-934e-cd033afc746f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric = compute_etp(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059612f5-0cdc-49bb-8bab-a8908037a89c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute global change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d33b29-a3a8-4be4-bb46-c410d4dfb0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Change\n",
    "ds_dict_change = compute_change(ds_dict_hist_period_metric, ds_dict_ssp370_period_metric, var_rel_change=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cf701-4d57-434c-ada9-d6fd836a8009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute ensemble median for the change\n",
    "ds_dict_change_ensemble = compute_ensemble(ds_dict_change, metric='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81befddc-a8da-482f-970c-d1a16431da05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only use ensemble median for further analysis\n",
    "ds_ens_glob_change = ds_dict_change_ensemble['Ensemble mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9bcbc0-ed42-4ee6-90ae-fa687f038290",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Compute Growing Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363b461-3c5d-45f7-8e71-2bd3bb7f5153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_monthly_mean(ds, variable):\n",
    "    \"\"\"\n",
    "    Calculate the mean for each month across all years for a given variable in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The input dataset.\n",
    "    variable (str): The name of the variable to calculate the monthly mean for.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: A dataset containing the monthly mean for the specified variable.\n",
    "    \"\"\"\n",
    "    # Calculate the monthly mean\n",
    "    monthly_means = ds[variable].groupby('time.month').mean('time')\n",
    "\n",
    "    # Create a new dataset for the monthly means\n",
    "    monthly_mean_ds = xr.Dataset({variable: monthly_means})\n",
    "\n",
    "    # Copy attributes from the original dataset and variable\n",
    "    monthly_mean_ds.attrs = ds.attrs\n",
    "    monthly_mean_ds[variable].attrs = ds[variable].attrs\n",
    "    monthly_mean_ds[variable].attrs['description'] = f'Monthly mean of {variable}'\n",
    "\n",
    "    return monthly_mean_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d437ef-3406-441a-ae52-408141a13a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_growing_season(ds, min_value, max_value):\n",
    "    # Determine the number of bins\n",
    "    n_bins = max_value - min_value + 1\n",
    "\n",
    "    # Use the \"BrBG\" colormap\n",
    "    cmap = plt.cm.BrBG\n",
    "\n",
    "    # Normalize the colormap to center it at 0\n",
    "    norm = plt.Normalize(vmin=min_value, vmax=max_value)\n",
    "\n",
    "    # Plot the data with the defined colormap and levels\n",
    "    plot = ds['growing_season_length'].plot(\n",
    "        cmap=cmap, \n",
    "        norm=norm,\n",
    "        levels=np.linspace(min_value - 0.5, max_value + 0.5, n_bins + 1),\n",
    "        add_colorbar=False  # Turn off automatic colorbar to adjust it manually later\n",
    "    )\n",
    "\n",
    "    # Calculate tick positions to be in the middle of each color segment\n",
    "    tick_positions = np.linspace(min_value + 0.5, max_value - 0.5, n_bins)\n",
    "\n",
    "    # Create and adjust the colorbar manually\n",
    "    cbar = plt.colorbar(plot, ticks=tick_positions)\n",
    "    cbar.set_label('Growing Season Length (Months)')\n",
    "\n",
    "    # Adjust the number of labels to match the number of tick positions\n",
    "    cbar.set_ticklabels([str(int(tick)) for tick in np.linspace(min_value, max_value, n_bins)])\n",
    "\n",
    "    plt.title('Growing Season Length Change')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567319f-58b0-492a-a37b-59176b4a0ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_percentages(ds):\n",
    "    increase = np.sum(ds.growing_season_length > 0)\n",
    "    decrease = np.sum(ds.growing_season_length < 0)\n",
    "    constant = np.sum(ds.growing_season_length == 0)\n",
    "    total = increase + decrease + constant\n",
    "\n",
    "    percent_increase = (increase / total) * 100\n",
    "    percent_decrease = (decrease / total) * 100\n",
    "    percent_constant = (constant / total) * 100\n",
    "\n",
    "    return percent_increase.item(), percent_decrease.item(), percent_constant.item()\n",
    "\n",
    "# Step 4: Compute global average change in growing season length\n",
    "def global_average_change(ds_change, ds_hist):\n",
    "    avg_change = ds_change.growing_season_length.mean().item()\n",
    "    avg_past = ds_hist.growing_season_length.mean().item()\n",
    "    avg_change_percent = avg_change / avg_past * 100\n",
    "    \n",
    "    return avg_change_percent\n",
    "\n",
    "# Step 5: Compute average change in growing season length for different climates\n",
    "def average_change_by_climate(ds_change, ds_hist):\n",
    "    arctic_hist = ds_hist.where(abs(ds_hist.lat) > 66.5, drop=True)\n",
    "    temperate_hist = ds_hist.where((abs(ds_hist.lat) > 23.5) & (abs(ds_hist.lat) <= 66.5), drop=True)\n",
    "    tropical_hist = ds_hist.where(abs(ds_hist.lat) <= 23.5, drop=True)\n",
    "    arctic_change = ds_change.where(abs(ds_hist.lat) > 66.5, drop=True)\n",
    "    temperate_change = ds_change.where((abs(ds_hist.lat) > 23.5) & (abs(ds_hist.lat) <= 66.5), drop=True)\n",
    "    tropical_change = ds_change.where(abs(ds_hist.lat) <= 23.5, drop=True)\n",
    "\n",
    "    avg_arctic = arctic_change.growing_season_length.mean().item() / arctic_hist.growing_season_length.mean().item() * 100\n",
    "    avg_temperate = temperate_change.growing_season_length.mean().item() / temperate_hist.growing_season_length.mean().item() * 100\n",
    "    avg_tropical = tropical_change.growing_season_length.mean().item() / tropical_hist.growing_season_length.mean().item() * 100\n",
    "\n",
    "    return avg_arctic, avg_temperate, avg_tropical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09774d15-e663-4ccc-be15-73dd2c6077be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_growing_season_length(ds_monthly_mean):\n",
    "    lai = ds_monthly_mean['lai']\n",
    "\n",
    "    # Create a DataArray for storing the growing season length\n",
    "    growing_season_length = xr.DataArray(\n",
    "        np.nan, \n",
    "        dims=('lat', 'lon'), \n",
    "        coords={'lat': ds_monthly_mean.lat, 'lon': ds_monthly_mean.lon}\n",
    "    )\n",
    "\n",
    "    for lat in ds_monthly_mean.lat.values:\n",
    "        for lon in ds_monthly_mean.lon.values:\n",
    "            lai_ts = lai.sel(lat=lat, lon=lon)\n",
    "\n",
    "            # Check if the LAI time series is entirely NaN (ocean cell)\n",
    "            if lai_ts.isnull().all():\n",
    "                continue  # Skip this cell, it remains NaN in growing_season_length\n",
    "\n",
    "            monthly_pct_change = calculate_monthly_pct_change(lai_ts)\n",
    "            starts, ends = detect_season_starts_and_ends(monthly_pct_change)\n",
    "            length = calculate_season_length(starts, ends)\n",
    "            growing_season_length.loc[dict(lat=lat, lon=lon)] = length\n",
    "\n",
    "    # Create a new dataset for the growing season length\n",
    "    ds_growing_season_length = xr.Dataset()\n",
    "    ds_growing_season_length['growing_season_length'] = growing_season_length\n",
    "    ds_growing_season_length['growing_season_length'].attrs = {\n",
    "        'description': 'Length of the growing season in months',\n",
    "        'calculated_from': 'LAI'\n",
    "    }\n",
    "\n",
    "    return ds_growing_season_length\n",
    "\n",
    "\n",
    "def calculate_monthly_pct_change(lai_ts):\n",
    "    monthly_pct_change = ((lai_ts - lai_ts.roll(month=1)) / lai_ts.roll(month=1)) * 100\n",
    "    monthly_pct_change[0] = ((lai_ts[0] - lai_ts[-1]) / lai_ts[-1]) * 100\n",
    "    return monthly_pct_change\n",
    "\n",
    "def detect_season_starts_and_ends(monthly_pct_change):\n",
    "    starts, ends = [], []\n",
    "    \n",
    "    # Check if December starts or ends a growing season \n",
    "    # STARTS\n",
    "    if monthly_pct_change[-2] < 0 and monthly_pct_change[-1] > 0:\n",
    "        starts.append(12) \n",
    "    # ENDS\n",
    "    if monthly_pct_change[-2] > 0 and monthly_pct_change[-1] < 0:\n",
    "        ends.append(12)\n",
    "    \n",
    "    #Check if January stats or ends a growing season\n",
    "    # STARTS\n",
    "    if monthly_pct_change[-1] < 0 and monthly_pct_change[0] > 0:\n",
    "        starts.append(1) \n",
    "    # ENDS\n",
    "    if monthly_pct_change[0] < 0 and monthly_pct_change[-1] > 0:\n",
    "        ends.append(1)\n",
    "        \n",
    "    # Now handle February to November for starts and ends\n",
    "    for i in range(1, len(monthly_pct_change) - 1):\n",
    "        if monthly_pct_change[i-1] <= 0 and monthly_pct_change[i] > 0:\n",
    "            starts.append(i + 1)\n",
    "        elif monthly_pct_change[i-1] > 0 and monthly_pct_change[i] <= 0:\n",
    "            ends.append(i + 1)\n",
    "            \n",
    "    return starts, ends\n",
    "\n",
    "def calculate_season_length(starts, ends):\n",
    "    growing_season_length = 0\n",
    "    \n",
    "    # Calculate the growing season length(s)\n",
    "    if len(starts) > 1:\n",
    "        if any(end > starts[0] for end in ends):\n",
    "            closest_end = min(filter(lambda end: end > starts[0], ends), key=lambda end: end - starts[0], default=None)\n",
    "            growing_season_length = closest_end - starts[0]\n",
    "        else:\n",
    "            growing_season_length = min(ends) + (12 - starts[0])\n",
    "\n",
    "        if any(end > starts[1] for end in ends):\n",
    "            growing_season_length = growing_season_length + max(ends) - starts[1]\n",
    "        else:\n",
    "            growing_season_length = growing_season_length + min(ends) + (12 - starts[1])\n",
    "    elif len(starts) == 1:\n",
    "        if starts[0] < ends[0]:\n",
    "            growing_season_length = ends[0] - starts[0]\n",
    "        elif starts[0] > ends[0]:\n",
    "            growing_season_length = (12 - starts[0]) + ends[0]\n",
    "    \n",
    "    return growing_season_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d51101-503b-4586-a60f-e6332bad2e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locations = {\n",
    "    \"Manaus, Brazil (Tropical Rainforest)\": {\"lat\": -3, \"lon\": -60},\n",
    "    \"Sahara Desert (Desert)\": {\"lat\": 23, \"lon\": 13},\n",
    "    \"Madrid, Spain (Mediterranean)\": {\"lat\": 40, \"lon\": -4},\n",
    "    \"Moscow, Russia (Continental)\": {\"lat\": 56, \"lon\": 37},\n",
    "    \"Northern Siberia, Russia (Tundra)\": {\"lat\": 70, \"lon\": 78},\n",
    "    \"Hanoi, Vietnam (Multiple Growing Seasons)\": {\"lat\": 21, \"lon\": 105},\n",
    "    \"New Delhi, India (Monsoon)\": {\"lat\": 28, \"lon\": 77},\n",
    "    \"Central Greenland (Greenland)\": {\"lat\": 72, \"lon\": -40},\n",
    "    \"Melbourne, Australia (Temperate)\": {\"lat\": -38, \"lon\": 145},\n",
    "    \"Cape Town, South Africa (Mediterranean)\": {\"lat\": -33, \"lon\": 19},\n",
    "    \"Buenos Aires, Argentina (Pampas)\": {\"lat\": -34, \"lon\": -58},\n",
    "    \"Central Sweden (Evergreen Forests)\": {\"lat\": 60, \"lon\": 15},\n",
    "    \"San José, Costa Rica (Tropical)\": {\"lat\": 9.93, \"lon\": -84.08},\n",
    "    \"Marabá, Brazil (Amazon Rainforest)\": {\"lat\": -5, \"lon\": -50},\n",
    "    \"Newman, Australia (Outback Desert)\": {\"lat\": -20, \"lon\": 120},\n",
    "    \"Warsaw, Poland (Temperate)\": {\"lat\": 52, \"lon\": 21},\n",
    "    \"Frankfurt, Germany (Temperate)\": {\"lat\": 50, \"lon\": 8},\n",
    "    \"North Dakota, USA (Continental)\": {\"lat\": 47.5, \"lon\": -100.5},\n",
    "    \"Mali (Sahel)\": {\"lat\": 17, \"lon\": -4},\n",
    "    \"Beijing, China (Temperate)\": {\"lat\": 39.9, \"lon\": 116.4},\n",
    "    \"Oregon\": {\"lat\": 44, \"lon\": -121},\n",
    "    \"Lokangu, Sankuru, DR Congo\": {\"lat\": -2, \"lon\": 22},\n",
    "    \"San Fernando, Venezuela\": {\"lat\": 8, \"lon\": -67},\n",
    "    \"Borneo\": {\"lat\": 2, \"lon\": 112},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e2cbd-8a61-46b0-90ef-42a50406d2f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_growing_seasons(ds_past, ds_future, locations):\n",
    "    n_locations = len(locations)\n",
    "    fig, axs = plt.subplots(n_locations, 2, figsize=(15, n_locations * 5))\n",
    "\n",
    "    for i, (name, coords) in enumerate(locations.items()):\n",
    "        max_lai = -np.inf\n",
    "        min_lai = np.inf\n",
    "        max_pct_change = -np.inf\n",
    "        min_pct_change = np.inf\n",
    "        \n",
    "        # First, find the max and min values for LAI and percentage change\n",
    "        for ds in [ds_past, ds_future]:\n",
    "            lai = ds['lai']\n",
    "            lai_ts = lai.sel(lat=coords['lat'], lon=coords['lon'], method='nearest')\n",
    "            monthly_mean = lai_ts.groupby('time.month').mean('time')\n",
    "            monthly_pct_change = calculate_monthly_pct_change(monthly_mean)\n",
    "\n",
    "            max_lai = max(max_lai, monthly_mean.max())\n",
    "            min_lai = min(min_lai, monthly_mean.min())\n",
    "            max_pct_change = max(max_pct_change, monthly_pct_change.max())\n",
    "            min_pct_change = min(min_pct_change, monthly_pct_change.min())\n",
    "\n",
    "        # Then, plot and set the same y-axis limits for both subplots\n",
    "        for j, ds in enumerate([ds_past, ds_future]):\n",
    "            ax = axs[i, j]\n",
    "            lai = ds['lai']\n",
    "            lai_ts = lai.sel(lat=coords['lat'], lon=coords['lon'], method='nearest')\n",
    "            monthly_mean = lai_ts.groupby('time.month').mean('time')\n",
    "            monthly_pct_change = calculate_monthly_pct_change(monthly_mean)\n",
    "            starts, ends = detect_season_starts_and_ends(monthly_pct_change)\n",
    "            growing_season_length = calculate_season_length(starts, ends)\n",
    "            \n",
    "            # Compute the midpoint value for the December to January transition\n",
    "            dec_to_jan_midpoint_lai = (monthly_mean[-1] + monthly_mean[0]) / 2\n",
    "            dec_to_jan_midpoint_pct_change = (monthly_pct_change[-1] + monthly_pct_change[0]) / 2\n",
    "\n",
    "            # Plotting logic\n",
    "            color = 'tab:blue'\n",
    "            ax.set_xlabel('Month')\n",
    "            ax.set_ylabel('Mean LAI', color=color)\n",
    "            ax.plot(range(1, 13), monthly_mean, color=color)\n",
    "            # Plot Monthly Mean LAI\n",
    "            ax.plot(range(1, 13), monthly_mean, color='tab:blue', label='Mean LAI')\n",
    "            ax.plot([12, 12.5], [monthly_mean[-1], dec_to_jan_midpoint_lai], color=color)\n",
    "            ax.plot([0.5, 1], [dec_to_jan_midpoint_lai, monthly_mean[0]], color=color)\n",
    "\n",
    "            # Plot the starts and ends for growing seasons\n",
    "            for start in starts:\n",
    "                ax.axvline(x=start-0.5, color='green', linestyle='--', label='Start of Growing Season')\n",
    "            for end in ends:\n",
    "                ax.axvline(x=end-0.5, color='red', linestyle='--', label='End of Growing Season')\n",
    "\n",
    "            # Plot Monthly Percentage Change\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(range(1, 13), monthly_pct_change, color='tab:orange', label='% Change')\n",
    "            ax2.axhline(y=0, color='gray', linestyle='--', linewidth=1)\n",
    "            ax2.set_ylabel('% Change', color='tab:orange')\n",
    "            ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "            \n",
    "            # Plot the December to January transition for percentage change\n",
    "            ax2.plot([12, 12.5], [monthly_pct_change[-1], dec_to_jan_midpoint_pct_change], color='tab:orange')\n",
    "            ax2.plot([0.5, 1], [dec_to_jan_midpoint_pct_change, monthly_pct_change[0]], color='tab:orange')\n",
    "\n",
    "            # Plot vertical lines for each month\n",
    "            for month in range(1, 13): \n",
    "                ax.axvline(x=month, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "            # Set plot title and labels\n",
    "            ax.set_title(f\"{name} - {['Past', 'Future'][j]} Growing Season: {growing_season_length} Months\")\n",
    "            ax.set_xlabel('Month')\n",
    "            ax.set_ylabel('Mean LAI', color='tab:blue')\n",
    "            \n",
    "            ax.set_xticks(range(1, 13)) \n",
    "            ax.set_xticklabels([str(month) for month in range(1, 13)])\n",
    "            \n",
    "            # Set x-axis and y-axis limits\n",
    "            ax.set_xlim(0.5, 12.5)\n",
    "            ax.set_ylim(min_lai - abs(min_lai)*0.1, max_lai + max_lai*0.1)\n",
    "            ax2.set_ylim(min_pct_change - abs(min_pct_change)*0.1, max_pct_change+max_pct_change*0.1)\n",
    "\n",
    "            # Add legend\n",
    "            #lines, labels = ax.get_legend_handles_labels()\n",
    "            #lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "            #ax.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913e646-e441-4989-b975-a12023b0baeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef8139-5a2c-43ce-902a-c19a7112355b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric_regions = {}\n",
    "ds_dict_hist_period_metric_regions = apply_region_mask(ds_dict_hist_period_metric, with_global=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbb35e-e9ae-4879-b94d-98d345b56487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric_regions = {}\n",
    "ds_dict_ssp370_period_metric_regions = apply_region_mask(ds_dict_ssp370_period_metric, with_global=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dff0a4-5120-4ab6-b5d7-30d5bc890203",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f3039-69a2-42fb-9ba9-b1413889fd37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Change\n",
    "ds_dict_region_change = compute_change(ds_dict_hist_period_metric_regions, ds_dict_ssp370_period_metric_regions, var_rel_change=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864d4d8-3d32-4427-9335-1fc653986222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute ensemble median for the change\n",
    "ds_dict_region_change_ensemble = compute_ensemble(ds_dict_region_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d953358-8a27-4833-b632-459a7d90f168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only use ensemble median for further analysis\n",
    "ds_dict_region_change_ensemble_metric = {}\n",
    "ds_dict_region_change_ensemble_metric['Ensemble mean'] = ds_dict_region_change_ensemble['Ensemble mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdb344-a81a-454d-9929-56f6273f688f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds_dict_region_change_ensemble_metric['Ensemble mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627f8da-5cea-456d-8da6-7ed4eb5a1834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_region_ensemble = compute_ensemble(ds_dict_hist_period_metric_regions)\n",
    "ds_hist_ensemble_metric = ds_dict_hist_region_ensemble['Ensemble mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b99e4e-a726-4ce8-b26a-26023f23222f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Extreme Gradient Boosting Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7f259-5e26-4f37-83f7-b33502c671bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build extreme gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c9b97-dbfb-4f02-a88d-ad37526d7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00d2ee-b4eb-4801-8500-d2226b5cd21d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_scale(data):\n",
    "    # Find the absolute maximum value in the data\n",
    "    max_val = np.max(np.abs(data), axis=0)\n",
    "\n",
    "    # Scale data by dividing by the max value\n",
    "    scaled_data = data / max_val\n",
    "\n",
    "    return scaled_data, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b578b-2636-4e9e-927d-e87f04a9f1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_data(X, method):\n",
    "    scaler_data = {}\n",
    "\n",
    "    if method == 'std':\n",
    "        scaler = StandardScaler()\n",
    "        X_standardized = scaler.fit_transform(X)\n",
    "        scaler_data = {'mean': scaler.mean_, 'std': scaler.scale_}\n",
    "\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        X_standardized = scaler.fit_transform(X)\n",
    "        scaler_data = {'min': scaler.data_min_, 'max': scaler.data_max_}\n",
    "        \n",
    "    elif method == 'max':\n",
    "        X_standardized, max_val = custom_scale(X)\n",
    "        scaler_data = {'max': max_val}\n",
    "\n",
    "    elif method == 'no_scaling':\n",
    "        X_standardized = X\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Scaling method not known')\n",
    "\n",
    "    return X_standardized, scaler_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea737c-ae11-40df-8617-53aa42b37fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import variation, gaussian_kde, skew, kurtosis\n",
    "\n",
    "\n",
    "def custom_scale(data):\n",
    "    max_val = np.max(np.abs(data), axis=0)\n",
    "    scaled_data = data / max_val\n",
    "    return scaled_data, max_val\n",
    "\n",
    "def plot_data_with_dual_axis(data, scaled_data, ax, skew_unscaled, skew_scaled, kurt_unscaled, kurt_scaled):\n",
    "    # Plotting unscaled data\n",
    "    density = gaussian_kde(data)\n",
    "    xs = np.linspace(min(data), max(data), 200)\n",
    "    ax.hist(data, bins=30, alpha=0.5, color='blue', density=True)\n",
    "    ax.plot(xs, density(xs), color='darkblue')\n",
    "\n",
    "    # Creating secondary axis for scaled data\n",
    "    ax2 = ax.twinx()\n",
    "    density_scaled = gaussian_kde(scaled_data)\n",
    "    xs_scaled = np.linspace(min(scaled_data), max(scaled_data), 200)\n",
    "    ax2.hist(scaled_data, bins=30, alpha=0.5, color='orange', density=True)\n",
    "    ax2.plot(xs_scaled, density_scaled(xs_scaled), color='darkorange')\n",
    "\n",
    "    # Adding scores as text\n",
    "    textstr = f'skew_unscaled: {skew_unscaled:.2f}\\nskew_scaled: {skew_scaled:.2f}\\nkurt_unscaled: {kurt_unscaled:.2f}\\nkurt_scaled: {kurt_scaled:.2f}'\n",
    "    ax.text(0.95, 0.95, textstr, transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='top', horizontalalignment='right', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "def scale_and_plot(ds):\n",
    "    regions = ds.region.values\n",
    "    variables = [var for var in ds.data_vars]\n",
    "\n",
    "    for region in [regions[0]]:\n",
    "        fig, axs = plt.subplots(1, len(variables), figsize=(5 * len(variables), 4))\n",
    "        fig.suptitle(f'Region: {ds.names.sel(region=region).values}')\n",
    "\n",
    "        for i, var in enumerate(variables):\n",
    "            data = ds[var].sel(region=region).values.flatten()\n",
    "            data_nonan = data[~np.isnan(data)]\n",
    "            \n",
    "            skew_unscaled = skew(data_nonan)\n",
    "            kurt_unscaled = kurtosis(data_nonan)\n",
    "\n",
    "            # Scale data\n",
    "            scaled_data, _ = custom_scale(data_nonan)\n",
    "\n",
    "            skew_scaled = skew(scaled_data)\n",
    "            kurt_scaled = kurtosis(scaled_data)\n",
    "\n",
    "            # Plot histograms with dual-axis\n",
    "            plot_data_with_dual_axis(data_nonan, scaled_data, axs[i], skew_unscaled, skew_scaled, kurt_unscaled, kurt_scaled)\n",
    "            axs[i].set_title(var)\n",
    "\n",
    "        # Adjust legend\n",
    "        fig.legend(['Unscaled', 'Density Unscaled', 'Scaled', 'Density Scaled'], loc='upper right')\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405eb2e-8970-43e4-bb18-3479771d6b61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "cv_df = scale_and_plot(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d5b6f-8c51-4978-9059-2c5311bf56f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, ParameterGrid, RandomizedSearchCV\n",
    "\n",
    "def train_xgb_models(ds, predictor_vars, predictant, scaling_method, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Train XGBoost models for each region with Grid Search, Cross-Validation, and Train/Test Split.\n",
    "\n",
    "    Parameters:\n",
    "    - ds: xarray dataset\n",
    "    - predictor_vars: List of predictor variable names\n",
    "    - predictant: Name of the predictant variable\n",
    "    - scaling_method: Method for scaling features\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing trained models, best parameters, best scores, CV scores (R2 and MSE), feature importances, and performance metrics for each region.\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries\n",
    "    xgb_models = {}\n",
    "    best_params = {}\n",
    "    best_scores = {}\n",
    "    feature_importances = {}\n",
    "    permutation_importances_test = {}\n",
    "    permutation_importances_train = {}\n",
    "    performance_metrics_test = {}\n",
    "    performance_metrics_train = {}\n",
    "    cv_scores_r2 = {}\n",
    "    cv_scores_mse = {}\n",
    "    cv_scores_r2_test = {}\n",
    "    cv_scores_mse_test = {}\n",
    "    cv_scores_r2_train = {}\n",
    "    cv_scores_mse_train = {}\n",
    "    scaled_data = {}\n",
    "\n",
    "    # Parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "    'n_estimators': [100, 200, 400, 800, 1000, 1100, 1200, 1300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [5, 7, 10, 14],\n",
    "    'min_child_weight': [0.01, 0.1, 1, 10, 12],\n",
    "    'lambda': [1, 3],\n",
    "    'alpha': [0.001, 0.01],\n",
    "    #'gamma': [0.05, 0.1]\n",
    "    }\n",
    "    \n",
    "\n",
    "    for region in ds.region.values:\n",
    "        # Data preparation\n",
    "        df = ds.sel(region=region).to_dataframe().reset_index()\n",
    "        region_name = ds.names.sel(region=region).values\n",
    "        df.dropna(inplace=True)\n",
    "        X = df[predictor_vars]\n",
    "        y = df[predictant]\n",
    "        \n",
    "        X_scaled, scaled_data[f'{region_name}'] = scale_data(X, method=scaling_method)\n",
    "\n",
    "        # Train/Test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Further split the training set into training and validation sets for hyperparameter tuning\n",
    "        X_train_tuning, X_val, y_train_tuning, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Initialize XGBoost model\n",
    "        xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "        # GridSearchCV for hyperparameter tuning on training data\n",
    "        grid_search = GridSearchCV(xgb, param_grid, cv=cv_folds, n_jobs=-1, scoring='r2')\n",
    "        grid_search.fit(X_train_tuning, y_train_tuning)\n",
    "\n",
    "        # Store the best model, parameters, and score for the region\n",
    "        xgb_models[f'{region_name}'] = grid_search.best_estimator_\n",
    "        best_params[f'{region_name}'] = grid_search.best_params_\n",
    "        best_scores[f'{region_name}'] = -grid_search.best_score_ \n",
    "\n",
    "        # Update feature_importances assignment\n",
    "        feature_importances[f'{region_name}'] = xgb_models[f'{region_name}'].feature_importances_\n",
    "        \n",
    "        #Compute permutation importance\n",
    "        perm_importance = permutation_importance(\n",
    "            xgb_models[f'{region_name}'], X_test, y_test, n_repeats=20, random_state=42\n",
    "        )\n",
    "        permutation_importances_test[f'{region_name}'] = perm_importance['importances']\n",
    "        \n",
    "        perm_importance_train = permutation_importance(\n",
    "            xgb_models[f'{region_name}'], X_train, y_train, n_repeats=20, random_state=42\n",
    "        )\n",
    "        permutation_importances_train[f'{region_name}'] = perm_importance_train['importances']\n",
    "\n",
    "        # Performance metrics\n",
    "        y_pred = xgb_models[f'{region_name}'].predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        performance_metrics_test[f'{region_name}'] = {'MSE': mse, 'R2': r2}\n",
    "        \n",
    "        y_pred_train = xgb_models[f'{region_name}'].predict(X_val)\n",
    "        mse_train = mean_squared_error(y_val, y_pred_train)\n",
    "        r2_train = r2_score(y_val, y_pred_train)\n",
    "        performance_metrics_train[f'{region_name}'] = {'MSE': mse_train, 'R2': r2_train}\n",
    "\n",
    "        # Perform cross-validation and store results\n",
    "        cv_scores_r2[f'{region_name}'] = cross_val_score(xgb_models[f'{region_name}'], X_scaled, y, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse[f'{region_name}'] = -cross_val_score(xgb_models[f'{region_name}'], X_scaled, y, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        cv_scores_r2_train[f'{region_name}'] = cross_val_score(xgb_models[f'{region_name}'], X_train, y_train, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse_train[f'{region_name}'] = -cross_val_score(xgb_models[f'{region_name}'], X_train, y_train, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        cv_scores_r2_test[f'{region_name}'] = cross_val_score(xgb_models[f'{region_name}'], X_test, y_test, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse_test[f'{region_name}'] = -cross_val_score(xgb_models[f'{region_name}'], X_test, y_test, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        \n",
    "    feature_importances_df = pd.DataFrame.from_dict(feature_importances, orient='index', columns=predictor_vars)\n",
    "\n",
    "    return feature_importances_df, permutation_importances_test, permutation_importances_train, best_params, xgb_models, best_scores, performance_metrics_test, performance_metrics_train, cv_scores_r2, cv_scores_mse, cv_scores_r2_train, cv_scores_mse_train, cv_scores_r2_test, cv_scores_mse_test, scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6474a-14c0-4f34-b712-361fb6235235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data and predictor variables\n",
    "predictor_vars = ['tas', 'pr', 'vpd','evapo', 'mrso','lai', 'gpp'] #'pr',  'gpp' 'evspsbl'  'mrro', 'tran', \n",
    "predictant = 'bgws'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a58ad-232f-429e-bb6b-3e50659f6869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e45445-0acc-43e0-ad77-8d7125c6a753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_reduced_regions = ds.sel(region=slice(18, 22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064eaf2-7ed4-4d00-ba57-937af7c9d6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances_df_xgb, permutation_importances_test_xgb, permutation_importances_train_xgb, best_params_xgb, x_gradient_boosting_models, best_scores_xgb, performance_metrics_test_xgb, performance_metrics_train_xgb, cv_scores_r2_xgb, cv_scores_mse_xgb, cv_scores_r2_train_xgb, cv_scores_mse_train_xgb, cv_scores_r2_test_xgb, cv_scores_mse_test_xgb, scaler_data_xgb = train_xgb_models(ds_reduced_regions, predictor_vars, predictant, scaling_method='max', cv_folds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0de453a-a961-4be1-b7cd-5eb3e9a1d48e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6698a1-4096-4f44-834b-eade3cb478af",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91b0a3-3daa-4327-813f-489d558cb9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curves_for_all_regions(models, ds, predictor_vars, predictant, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Plots learning curves for each region's model using the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - models: Dictionary of trained models, one for each region.\n",
    "    - ds: xarray dataset used in training models.\n",
    "    - predictor_vars: List of predictor variable names.\n",
    "    - predictant: Name of the predictant variable.\n",
    "    - region_names: List of region names.\n",
    "    - cv_folds: Number of folds for cross-validation.\n",
    "    \"\"\"\n",
    "    for region in ds.region.values:\n",
    "        # Extract data for the region\n",
    "        df = ds.sel(region=region).to_dataframe().reset_index()\n",
    "        df.dropna(inplace=True)\n",
    "        X = df[predictor_vars]\n",
    "        y = df[predictant]\n",
    "        \n",
    "        region_name = ds.names.sel(region=region).values\n",
    "\n",
    "        # Learning curve computation\n",
    "        train_sizes, train_scores, validation_scores = learning_curve(\n",
    "            estimator=models[f'{region_name}'],\n",
    "            X=X, \n",
    "            y=y, \n",
    "            train_sizes=np.linspace(0.1, 1.0, 10, 550),\n",
    "            cv=cv_folds,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Calculate mean and standard deviation for training and validation set scores\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        validation_mean = np.mean(validation_scores, axis=1)\n",
    "        validation_std = np.std(validation_scores, axis=1)\n",
    "\n",
    "        # Plot learning curves\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_sizes, train_mean, label='Training error', color='blue', marker='o')\n",
    "        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.15)\n",
    "        plt.plot(train_sizes, validation_mean, label='Validation error', color='green', marker='o')\n",
    "        plt.fill_between(train_sizes, validation_mean - validation_std, validation_mean + validation_std, color='green', alpha=0.15)\n",
    "\n",
    "        plt.title(f'Learning Curves for {region_name}')\n",
    "        plt.xlabel('Training Data Size')\n",
    "        plt.ylabel('R^2')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f6234-4251-44fa-8ee2-f3f0cab9d62e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_learning_curves_for_all_regions(x_gradient_boosting_models, ds_reduced_regions, predictor_vars, predictant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b135a45-e0ca-4dcd-b4d3-7e0532605225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_performance_metrics(performance_metrics_train, performance_metrics_test, cv_scores_r2, cv_scores_mse, cv_scores_r2_train, cv_scores_mse_train, cv_scores_r2_test, cv_scores_mse_test):\n",
    "    regions = list(performance_metrics_train.keys())\n",
    "    mse_train = [performance_metrics_train[region]['MSE'] for region in regions]\n",
    "    mse_test = [performance_metrics_test[region]['MSE'] for region in regions]\n",
    "    r2_train = [performance_metrics_train[region]['R2'] for region in regions]\n",
    "    r2_test = [performance_metrics_test[region]['R2'] for region in regions]\n",
    "   \n",
    "    fig, ax = plt.subplots(6, 1, figsize=(15, 30))\n",
    "\n",
    "    # Plot 1: R2 Comparison\n",
    "    ax[0].plot(regions, r2_train, label='Train R2', marker='o')\n",
    "    ax[0].plot(regions, r2_test, label='Test R2', marker='o')\n",
    "    ax[0].set_title('R2 Comparison')\n",
    "    ax[0].set_xlabel('Regions')\n",
    "    ax[0].set_ylabel('R2')\n",
    "    ax[0].set_ylim([0,1])\n",
    "    ax[0].legend()\n",
    "    ax[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # Plot 2: MSE Comparison\n",
    "    ax[1].plot(regions, mse_train, label='Train MSE', marker='o')\n",
    "    ax[1].plot(regions, mse_test, label='Test MSE', marker='o')\n",
    "    ax[1].set_title('MSE Comparison')\n",
    "    ax[1].set_xlabel('Regions')\n",
    "    ax[1].set_ylabel('MSE')\n",
    "    ax[1].set_ylim([0,0.006])\n",
    "    ax[1].legend()\n",
    "    ax[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # Function to plot mean or standard deviation\n",
    "    def plot_cv_scores(ax, cv_scores_whole, cv_scores_train, cv_scores_test, title, ylabel):\n",
    "        means_whole = [np.mean(scores) for scores in cv_scores_whole]\n",
    "        means_train = [np.mean(scores) for scores in cv_scores_train]\n",
    "        means_test = [np.mean(scores) for scores in cv_scores_test]\n",
    "        if ylabel == 'Mean Scores':\n",
    "            ax.plot(regions, means_whole, label='Whole Data', marker='o')\n",
    "            ax.plot(regions, means_train, label='Train Data', marker='o')\n",
    "            ax.plot(regions, means_test, label='Test Data', marker='o')\n",
    "        else:\n",
    "            stds_whole = [np.std(scores) for scores in cv_scores_whole]\n",
    "            stds_train = [np.std(scores) for scores in cv_scores_train]\n",
    "            stds_test = [np.std(scores) for scores in cv_scores_test]\n",
    "            ax.plot(regions, stds_whole, label='Whole Data', marker='o')\n",
    "            ax.plot(regions, stds_train, label='Train Data', marker='o')\n",
    "            ax.plot(regions, stds_test, label='Test Data', marker='o')\n",
    "        \n",
    "        # Set y-axis limits for R2 plots\n",
    "        if title == 'Mean CV R2 Scores' or title == 'Std CV R2 Scores':\n",
    "            ax.set_ylim([0, 1]) \n",
    "        else:\n",
    "            ax.set_ylim([0, 0.006]) \n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Regions')\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.legend()\n",
    "        ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # Plot 3: Mean CV R2 Scores\n",
    "    plot_cv_scores(ax[2], [cv_scores_r2[region] for region in regions], [cv_scores_r2_train[region] for region in regions], [cv_scores_r2_test[region] for region in regions], 'Mean CV R2 Scores', 'Mean Scores')\n",
    "\n",
    "    # Plot 4: Std CV R2 Scores\n",
    "    plot_cv_scores(ax[3], [cv_scores_r2[region] for region in regions], [cv_scores_r2_train[region] for region in regions], [cv_scores_r2_test[region] for region in regions], 'Std CV R2 Scores', 'Standard Deviation')\n",
    "\n",
    "    # Plot 5: Mean CV MSE Scores\n",
    "    plot_cv_scores(ax[4], [cv_scores_mse[region] for region in regions], [cv_scores_mse_train[region] for region in regions], [cv_scores_mse_test[region] for region in regions], 'Mean CV MSE Scores', 'Mean Scores')\n",
    "\n",
    "    # Plot 6: Std CV MSE Scores\n",
    "    plot_cv_scores(ax[5], [cv_scores_mse[region] for region in regions], [cv_scores_mse_train[region] for region in regions], [cv_scores_mse_test[region] for region in regions], 'Std CV MSE Scores', 'Standard Deviation')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70025ea4-c82f-471c-aba9-4c452809bf33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_performance_metrics(performance_metrics_train_xgb, performance_metrics_test_xgb, cv_scores_r2_xgb, cv_scores_mse_xgb, cv_scores_r2_train_xgb, cv_scores_mse_train_xgb, cv_scores_r2_test_xgb, cv_scores_mse_test_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ea344-736b-4e8b-a4b9-a7281b2297a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_permutation_importances(permutation_importances_test, permutation_importances_train, predictor_vars):\n",
    "    # Number of regions\n",
    "    num_regions = len(permutation_importances_test)\n",
    "\n",
    "    # Create subplots - one row per region\n",
    "    fig, axs = plt.subplots(num_regions, 2, figsize=(15, num_regions * 4))\n",
    "\n",
    "    for idx, region in enumerate(permutation_importances_test):\n",
    "        # Convert arrays to DataFrames for easy plotting\n",
    "        test_df = pd.DataFrame(permutation_importances_test[region].T, columns=predictor_vars)\n",
    "        train_df = pd.DataFrame(permutation_importances_train[region].T, columns=predictor_vars)\n",
    "        \n",
    "        # Plot for test data\n",
    "        sns.boxplot(data=test_df, orient='h', ax=axs[idx, 0])\n",
    "        axs[idx, 0].axvline(0, color='grey', linestyle='--')\n",
    "        axs[idx, 0].set_title(f'{region} - Test Data')\n",
    "        axs[idx, 0].set_xlabel('Decrease in Accuracy Score')\n",
    "        axs[idx, 0].set_ylabel('Variables')\n",
    "\n",
    "        # Plot for training data\n",
    "        sns.boxplot(data=train_df, orient='h', ax=axs[idx, 1])\n",
    "        axs[idx, 1].axvline(0, color='grey', linestyle='--')\n",
    "        axs[idx, 1].set_title(f'{region} - Training Data')\n",
    "        axs[idx, 1].set_xlabel('Decrease in Accuracy Score')\n",
    "        axs[idx, 1].set_ylabel('Variables')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e857c84-f455-4997-8252-c6b78f028fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_permutation_importances(permutation_importances_test_xgb, permutation_importances_train_xgb, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c6bc9-4aad-4664-88d2-32b7d1a2b829",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Assess variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59ae94-ef1b-421d-85e6-0cc435bf410a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "permutation_importance_df_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acad1df-45fd-4e9d-870d-5cad9c61ada0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances_df * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32681dd8-0598-49bf-95d0-de6345fab979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances_df * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c64ad7a-fec1-411e-b326-4ff435971e0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb58ac-0320-40f6-b533-60e3796b1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "def test_regression_assumptions_scikit(regression_models, test_data, predictor_vars):\n",
    "    results = []\n",
    "\n",
    "    for region_name in regression_models:\n",
    "        model = regression_models[region_name]\n",
    "        X_test, y_test = test_data[region_name]\n",
    "\n",
    "        # Predict and calculate residuals\n",
    "        predictions = model.predict(X_test)\n",
    "        residuals = y_test - predictions\n",
    "\n",
    "        # Add a constant term for the Breusch-Pagan test\n",
    "        X_test_with_constant = np.column_stack((np.ones(X_test.shape[0]), X_test))\n",
    "\n",
    "        # Test for normality of residuals\n",
    "        shapiro_stat, shapiro_p = shapiro(residuals)\n",
    "\n",
    "        # Test for homoscedasticity\n",
    "        _, _, _, bp_pvalue = het_breuschpagan(residuals, X_test_with_constant)\n",
    "\n",
    "        # VIF for multicollinearity\n",
    "        vif = [variance_inflation_factor(X_test, i) for i in range(X_test.shape[1])]\n",
    "\n",
    "        # Prepare plot data\n",
    "        plot_data = {\n",
    "            'Region': region_name,\n",
    "            'Predictions': predictions,\n",
    "            'Residuals': residuals,\n",
    "            'Shapiro-Wilk': shapiro_stat,\n",
    "            'Shapiro-Wilk p-value': shapiro_p,\n",
    "            'Breusch-Pagan p-value': bp_pvalue,\n",
    "            'VIF': vif\n",
    "        }\n",
    "        results.append(plot_data)\n",
    "    \n",
    "    # Plotting\n",
    "    num_regions = len(results)\n",
    "    fig, axs = plt.subplots(num_regions, 3, figsize=(22, 5 * num_regions)) # Changed to 3 subplots for VIF\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        sns.residplot(x=result['Predictions'], y=result['Residuals'], lowess=True, ax=axs[i, 0])\n",
    "        axs[i, 0].set_title(f'Residuals vs Predictions for {result[\"Region\"]}')\n",
    "        axs[i, 0].set_xlabel('Predicted values')\n",
    "        axs[i, 0].set_ylabel('Residuals')\n",
    "\n",
    "        # Adding text for statistical tests\n",
    "        axs[i, 0].text(0.05, 0.95, f\"Shapiro-Wilk: {result['Shapiro-Wilk']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "        axs[i, 0].text(0.05, 0.90, f\"Shapiro-Wilk p-value: {result['Shapiro-Wilk p-value']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "        axs[i, 0].text(0.05, 0.85, f\"Breusch-Pagan p-value: {result['Breusch-Pagan p-value']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "\n",
    "        sns.histplot(result['Residuals'], kde=True, ax=axs[i, 1])\n",
    "        axs[i, 1].set_title(f'Residual Distribution for {result[\"Region\"]}')\n",
    "        axs[i, 1].set_xlabel('Residuals')\n",
    "        axs[i, 1].set_ylabel('Frequency')\n",
    "\n",
    "        # VIF bar plot\n",
    "        sns.barplot(x=predictor_vars, y=result['VIF'], ax=axs[i, 2])\n",
    "        axs[i, 2].hlines(5, xmin=-0.5, xmax=len(result['VIF'])-0.5, colors='orange', linestyles='dashed')\n",
    "        axs[i, 2].hlines(10, xmin=-0.5, xmax=len(result['VIF'])-0.5, colors='r', linestyles='dashed')\n",
    "        axs[i, 2].set_title(f'VIF for {result[\"Region\"]}')\n",
    "        axs[i, 2].set_xlabel('Predictor Variables')\n",
    "        axs[i, 2].set_ylabel('VIF Value')\n",
    "        axs[i, 2].set_ylim(0, max(result['VIF']) + 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f835f9b-fd63-49b1-80df-e0fcef9413dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assumptions_df = test_regression_assumptions_scikit(regression_models, test_data, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f876b-31e1-4566-b3e5-c43fee81a59f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845e157-982f-4644-ab79-107e9867e0c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_model_performance_all_regions(ds, regression_models, best_params, best_scores, scalers, predictor_vars):\n",
    "    region_names = ds.names.values\n",
    "    region_indices = ds.region.values\n",
    "    \n",
    "    # Determine the number of rows and columns for the subplots based on the number of regions\n",
    "    num_regions = len(region_names)\n",
    "    cols = 3  # We are keeping 3 columns as per your requirement\n",
    "    rows = math.ceil(num_regions / cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 5 * rows), squeeze=False)\n",
    "    \n",
    "    # Initialize an empty dictionary to store performance metrics for each region\n",
    "    performance_metrics = {}\n",
    "\n",
    "    for idx, (ax, region) in enumerate(zip(axs.flatten(), region_indices)):\n",
    "        region_name = ds.names.sel(region=region).values.item()\n",
    "        # Prepare the data for the region\n",
    "        df = ds.sel(region=region).to_dataframe().dropna()\n",
    "        X = df[predictor_vars]\n",
    "        y_true = df['bgws'].values\n",
    "\n",
    "        # Retrieve the scaler for the region\n",
    "        scaler = scalers[region_name]\n",
    "        X_standardized = scaler.transform(X)  # Use transform here, not fit_transform\n",
    "        \n",
    "        # Predict using the trained model\n",
    "        model = regression_models[region_name]\n",
    "        y_pred = model.predict(X_standardized)\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = y_true - y_pred\n",
    "        \n",
    "        # Calculate and store performance metrics\n",
    "        mse_value = mean_squared_error(y_true, y_pred)\n",
    "        r2_value = r2_score(y_true, y_pred)\n",
    "        best_param = best_params[region_name]\n",
    "        best_cv_score = best_scores[region_name]\n",
    "        \n",
    "        performance_metrics[region_name] = {\n",
    "            'MSE': mse_value,\n",
    "            'R^2': r2_value,\n",
    "            'Best Parameters': best_param,\n",
    "            'Best CV Score': best_cv_score\n",
    "        }\n",
    "        \n",
    "        # Plot the residuals\n",
    "        ax.scatter(y_true, residuals, c='blue', alpha=0.5, s=10)\n",
    "        ax.axhline(0, color='red', lw=2)\n",
    "        ax.text(0.05, 0.95, f'MSE: {mse_value:.6f}\\nR^2: {r2_value:.2f}\\nBest CV: {best_cv_score:.4f}', \n",
    "                transform=ax.transAxes, verticalalignment='top')\n",
    "        ax.set_title(f'{region_name}\\n{best_param}')\n",
    "        ax.set_xlabel('True Values')\n",
    "        ax.set_ylabel('Residuals')\n",
    "        \n",
    "        # Hide axes for subplots that are not used (if num_regions < rows*cols)\n",
    "        if idx >= num_regions:\n",
    "            ax.set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224c1a8-8798-47e2-948f-a03fb4c51ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_performance_metrics = assess_model_performance_all_regions(ds, gradient_boosting_models, best_params, best_scores, scaler_data, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abe33ab-3bdd-4e4f-be6d-a301a313c54c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multiple Linear Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20110909-55f2-4799-bb61-38482b75388a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187b7bd-c237-4732-b9d5-999e869dc3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_standardized_data(X_standardized, y, predictor_vars):\n",
    "    # Summarize predictors\n",
    "    X_summary = pd.DataFrame(X_standardized, columns=predictor_vars).describe().transpose()\n",
    "    # Summarize response variable\n",
    "    y_summary = pd.DataFrame(y, columns=['bgws']).describe().transpose()\n",
    "    # Combine summaries\n",
    "    summary = pd.concat([X_summary, y_summary])\n",
    "    return summary[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c2d47-980b-45c8-ae3b-188f5091ddde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarize_standardized_data(X, y, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de39e1e-8e43-4cf3-bd55-fe76aff4f95d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot data correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db9302-7ae6-4afc-b4bb-de5778256460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_spearman_correlation(ds, predictor_vars, predictant):\n",
    "    # Number of regions\n",
    "    n_regions = len(ds.region)\n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    n_cols = 3\n",
    "    n_rows = np.ceil(n_regions / n_cols).astype(int)\n",
    "\n",
    "    # Initialize the subplot figure\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4))\n",
    "    axes = axes.flatten()  # Flatten the array for easy iteration\n",
    "\n",
    "    for index, region in enumerate(ds.region.values):\n",
    "        # Select the current region data\n",
    "        df = ds.sel(region=region).to_dataframe().dropna()\n",
    "\n",
    "        # Concatenate predictor variables and predictant for correlation\n",
    "        data = df[predictor_vars + [predictant]]\n",
    "\n",
    "        # Compute the Spearman correlation matrix\n",
    "        corr_matrix = data.corr(method='spearman')\n",
    "\n",
    "        # Plot the heatmap\n",
    "        sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1, cbar=index == 0, ax=axes[index])\n",
    "\n",
    "        # Set the title with the region name\n",
    "        region_name = ds.names.sel(region=region).values\n",
    "        axes[index].set_title(f\"Region: {region_name}\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for i in range(n_regions, n_rows * n_cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fed71-f1d1-4301-8bc9-203a737ed591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_spearman_correlation(ds, predictor_vars, predictant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b309650-e20e-4ac2-870a-f1627239b557",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203ddc2-e72d-4c07-856f-2c9a57ae4913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_all_distributions(ds, predictor_vars):\n",
    "    num_regions = len(ds.region)\n",
    "    # Set up the matplotlib figure with a certain number of columns\n",
    "    cols = 3\n",
    "    rows = (num_regions // cols) + (num_regions % cols > 0)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5), constrained_layout=True)\n",
    "    axes = axes.flatten()  # Flatten to 1D array for easy iteration\n",
    "    \n",
    "    for idx, region in enumerate(ds.region.values):\n",
    "        # Select data for the region\n",
    "        df = ds.sel(region=region).to_dataframe().reset_index()\n",
    "        df = df.dropna(subset=predictor_vars + ['bgws'])  # Drop NaN values for relevant columns only\n",
    "        \n",
    "        # Extract predictor variables and the target variable\n",
    "        X = df[predictor_vars]\n",
    "        y = df['bgws']\n",
    "        \n",
    "        # Standardize the predictors\n",
    "        scaler = StandardScaler()\n",
    "        X_standardized = scaler.fit_transform(X)\n",
    "        \n",
    "        # Create DataFrame from the standardized predictors\n",
    "        df_standardized = pd.DataFrame(X_standardized, columns=predictor_vars)\n",
    "        df_standardized['bgws'] = y.values  # Add non-standardized 'bgws'\n",
    "        \n",
    "        # Plotting on the respective subplot\n",
    "        ax = axes[idx]\n",
    "        # Create a list to store handles for the legend\n",
    "        handles = []\n",
    "        for col in df_standardized.columns:\n",
    "            # Plot each variable and get the handle\n",
    "            handle = sns.histplot(df_standardized[col], kde=True, ax=ax, label=col)\n",
    "            handles.append(handle)\n",
    "\n",
    "        region_name = ds.names.sel(region=region).values\n",
    "        ax.set_title(f'Region {region_name}')\n",
    "\n",
    "        # Only add legend to the first subplot\n",
    "        if idx == 0:\n",
    "            ax.legend(title='Variable')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for ax in axes[num_regions:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6c7d2-f969-41d3-9c82-7490093f3993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_all_distributions(ds, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec07de-31f8-4912-b821-4bcd580364c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1ff62-2afb-4285-ab20-fbb6e016a41a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "def lr_models(ds_change, predictor_vars, predictant, scaling_method, regression_type='ridge', scoring='r2', cv_folds=5, scaling_back=False):\n",
    "    \"\"\"\n",
    "    Train regularized linear regression models (Ridge, Lasso, or ElasticNet) for each region using Grid Search for hyperparameter tuning,\n",
    "    perform cross-validation, compute permutation importance, and gather performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - ds_change: xarray datasets\n",
    "    - predictor_vars: List of predictor variable names\n",
    "    - predictant: Name of the predictant variable\n",
    "    - regression_type: Type of regression ('ridge', 'lasso', 'elasticnet')\n",
    "    - cv_folds: Number of folds for cross-validation\n",
    "    - scaling_method: Scaling method (std or minmax)\n",
    "    - scaling_back: Boolean flag to scale back coefficients\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing trained models, best hyperparameters, coefficients, cross-validation scores,\n",
    "      permutation importances, and performance metrics for each region.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionaries\n",
    "    regression_models = {}\n",
    "    best_hyperparams = {}\n",
    "    best_scores = {}\n",
    "    regression_coeffs = {}\n",
    "    cv_scores_r2 = {}\n",
    "    cv_scores_mse = {}\n",
    "    cv_scores_r2_test = {}\n",
    "    cv_scores_mse_test = {}\n",
    "    cv_scores_r2_train = {}\n",
    "    cv_scores_mse_train = {}\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    residuals = {}\n",
    "    performance_metrics_test = {}\n",
    "    performance_metrics_train = {}\n",
    "    permutation_importances_test = {}\n",
    "    permutation_importances_train = {}\n",
    "    scaled_data = {}\n",
    "\n",
    "    # Define parameter grid based on regression type\n",
    "    if regression_type == 'ridge':\n",
    "        model = Ridge(random_state=42)\n",
    "        param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    elif regression_type == 'lasso':\n",
    "        model = Lasso(random_state=42)\n",
    "        param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    elif regression_type == 'elasticnet':\n",
    "        model = ElasticNet(random_state=42)\n",
    "        param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100], 'l1_ratio': [0.2, 0.5, 0.8]}\n",
    "    else:\n",
    "        raise ValueError(\"Invalid regression type. Choose 'ridge', 'lasso', or 'elasticnet'.\")\n",
    "\n",
    "    for region in ds_change.region.values:\n",
    "        # Data preparation\n",
    "        df = ds_change.sel(region=region).to_dataframe().reset_index()\n",
    "        df.dropna(inplace=True)\n",
    "        X = df[predictor_vars]\n",
    "        y = df[predictant]\n",
    "        \n",
    "        # Get region name\n",
    "        region_name = ds.names.sel(region=region).values\n",
    "\n",
    "        # Scale the data\n",
    "        X_scaled, scaled_data[f'{region_name}'] = scale_data(X, method=scaling_method)\n",
    "\n",
    "        # Train/Test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Grid search\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=cv_folds, scoring=scoring, n_jobs=-1) #r2 neg_mean_squared_error\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Store best model and hyperparameters\n",
    "        best_model = grid_search.best_estimator_\n",
    "        regression_models[f'{region_name}'] = best_model\n",
    "        best_hyperparams[f'{region_name}'] = grid_search.best_params_\n",
    "        best_scores[f'{region_name}'] = grid_search.best_score_\n",
    "        regression_coeffs[f'{region_name}'] = best_model.coef_\n",
    "        \n",
    "        # Perform cross-validation and store results\n",
    "        cv_scores_r2[f'{region_name}'] = cross_val_score(best_model, X_scaled, y, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse[f'{region_name}'] = -cross_val_score(best_model, X_scaled, y, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        cv_scores_r2_train[f'{region_name}'] = cross_val_score(best_model, X_train, y_train, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse_train[f'{region_name}'] = -cross_val_score(best_model, X_train, y_train, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        cv_scores_r2_test[f'{region_name}'] = cross_val_score(best_model, X_test, y_test, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse_test[f'{region_name}'] = -cross_val_score(best_model, X_test, y_test, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        # Compute and store performance metrics\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "        r2_test = r2_score(y_test, y_pred_test)\n",
    "        performance_metrics_test[f'{region_name}'] = {'MSE': mse_test, 'R2': r2_test}\n",
    "\n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "        r2_train = r2_score(y_train, y_pred_train)\n",
    "        performance_metrics_train[f'{region_name}'] = {'MSE': mse_train, 'R2': r2_train}\n",
    "\n",
    "        # Compute permutation importance\n",
    "        perm_importance_test = permutation_importance(best_model, X_test, y_test, n_repeats=20, random_state=42)\n",
    "        permutation_importances_test[f'{region_name}'] = perm_importance_test['importances']\n",
    "\n",
    "        perm_importance_train = permutation_importance(best_model, X_train, y_train, n_repeats=20, random_state=42)\n",
    "        permutation_importances_train[f'{region_name}'] = perm_importance_train['importances']\n",
    "\n",
    "        # Store train and test data\n",
    "        train_data[f'{region}'] = (X_train, y_train)\n",
    "        test_data[f'{region}'] = (X_test, y_test)\n",
    "\n",
    "    # Convert regression coefficients to DataFrame\n",
    "    coeffs_df = pd.DataFrame.from_dict(regression_coeffs, orient='index', columns=predictor_vars)\n",
    "\n",
    "    return coeffs_df, regression_models, best_hyperparams, best_scores, train_data, test_data, cv_scores_r2, cv_scores_mse, cv_scores_r2_train, cv_scores_mse_train, cv_scores_r2_test, cv_scores_mse_test, performance_metrics_test, performance_metrics_train, permutation_importances_test, permutation_importances_train, scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221ccfa-58e1-4409-99fb-1012bd125ed3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr_models(ds_change, predictor_vars, predictant, scaling_method, cv_folds=5, scaling_back=False):\n",
    "    \"\"\"\n",
    "    Train multivariate regression models for each region, perform cross-validation, \n",
    "    and compute performance metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - ds_change, ds_hist: xarray datasets\n",
    "    - predictor_vars: List of predictor variable names\n",
    "    - predictant: Name of the predictant variable\n",
    "    - cv_folds: Number of folds for cross-validation (default is 5)\n",
    "    - scaling: Scaling method std or minmax\n",
    "    - scaling_back: Boolean flag to scale back coefficients\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary containing trained models, coefficients, cross-validation scores,\n",
    "      and performance metrics for each region.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionaries\n",
    "    regression_models = {}\n",
    "    regression_coeffs = {}\n",
    "    cv_scores = {}\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    residuals = {}\n",
    "    performance_metrics = {}\n",
    "    scaled_data = {}\n",
    "\n",
    "    for region in ds_change.region.values:\n",
    "        # Convert xarray data to pandas DataFrame\n",
    "        df = ds_change.sel(region=region).to_dataframe().reset_index()\n",
    "        df = df.dropna()  # Drop rows with NaN values\n",
    "        \n",
    "        # Get region name\n",
    "        region_name = ds.names.sel(region=region).values\n",
    "\n",
    "        X = df[predictor_vars]\n",
    "        y = df[predictant]\n",
    "        \n",
    "        # Scale the data\n",
    "        X_scaled, scaled_data[f'{region_name}'] = scale_data(X, method=scaling_method)\n",
    "        \n",
    "        # Train/Test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # 30% of the data are used to test the model\n",
    "\n",
    "        # Train the regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Store model and coefficients\n",
    "        regression_models[f'{region_name}'] = model\n",
    "        regression_coeffs[f'{region_name}'] = model.coef_\n",
    "        train_data[f'{region_name}'] = (X_train, y_train)\n",
    "        test_data[f'{region_name}'] = (X_test, y_test)\n",
    "\n",
    "        # Predict using the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate residuals\n",
    "        resid = y_test - y_pred\n",
    "        residuals[f'{region_name}'] = resid\n",
    "\n",
    "        # Compute and store performance metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        performance_metrics[f'{region_name}'] = {'MSE': mse, 'R2': r2}\n",
    "\n",
    "        # Perform cross-validation and store results\n",
    "        cv_score = cross_val_score(model, X, y, cv=cv_folds) \n",
    "        cv_scores[f'{region_name}'] = cv_score\n",
    "\n",
    "    # Convert regression coefficients to DataFrame\n",
    "    coeffs_list = []\n",
    "\n",
    "    if scaling_back:\n",
    "        if scaling_method == 'std':\n",
    "            for region, coefs in regression_coeffs.items():\n",
    "                feature_means, feature_stds = scaled_data[region]['mean'], scaled_data[region]['std']\n",
    "                # Scale back coefficients\n",
    "                original_coefs = coefs / feature_stds\n",
    "                intercept = model.intercept_ - np.sum(original_coefs * feature_means)\n",
    "                coeffs_list.append(np.concatenate(coefs))\n",
    "                \n",
    "        elif scaling_method == 'minmax':\n",
    "            for region, coefs in regression_coeffs.items():\n",
    "                feature_mins, feature_maxs = scaled_data[region]['min'], scaled_data[region]['max']\n",
    "                feature_ranges = feature_maxs - feature_mins\n",
    "                original_coefs = coefs / feature_ranges\n",
    "                intercept = model.intercept_ - np.sum(original_coefs * feature_mins / feature_ranges)\n",
    "                coeffs_list.append(np.concatenate(coefs))\n",
    "        else:\n",
    "            for region, coefs in regression_coeffs.items():\n",
    "                coeffs_list.append(np.concatenate(coefs))\n",
    "    else:\n",
    "        for region, coefs in regression_coeffs.items():\n",
    "            coeffs_list.append(coefs)\n",
    "\n",
    "    coeffs_df = pd.DataFrame(coeffs_list, index=regression_coeffs.keys(), columns=predictor_vars)\n",
    "\n",
    "    return coeffs_df, regression_models, train_data, test_data, cv_scores, performance_metrics, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701e7f7-9d2d-4484-8e00-bf58e94bcf0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define data and predictor variables\n",
    "predictor_vars = ['pr', 'vpd', 'mrso', 'lai', 'wue', 'RX5day']#, 'growing_season_length'] #'pr',  'gpp' 'evspsbl' \n",
    "predictant = 'bgws'\n",
    "\n",
    "#['tas', 'pr', 'vpd', 'mrro', 'mrso', 'tran', 'lai', 'gpp', 'evapo', 'evspsbl', 'RX5day', 'growing_season_length'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78588bf1-830e-4a76-bebd-5cdf1d15e2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coeffs_df_lr, linear_regression_models, best_hyperparams_lr, best_scores_lr, train_data_lr, test_data_lr, cv_scores_r2_lr, cv_scores_mse_lr, cv_scores_r2_train_lr, cv_scores_mse_train_lr, cv_scores_r2_test_lr, cv_scores_mse_test_lr, performance_metrics_test_lr, performance_metrics_train_lr, permutation_importances_test_lr, permutation_importances_train_lr, scaled_data_lr = lr_models(ds, predictor_vars, predictant, scaling_method='max', regression_type='elasticnet', scoring='neg_mean_squared_error', cv_folds=4, scaling_back=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37920169-3c67-4034-b971-8b31dc68ccae",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e11cb-b88b-4904-885c-d34ad0f094d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics_test_lr #all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8496958-5d1c-492d-beec-3bf350e030cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics_test_lr #summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f60dd8-06a1-4bf7-b3fd-b92684f22fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics_test_lr #winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760278b-8ec6-4196-8088-575df59ac709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics_test_xgb['Mediterranean']['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d3d50-15e2-4dde-8131-23d613bf13e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics_test_lr['Mediterranean']['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569c64e-3dcb-47d4-8aac-44efd5886484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics_test_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ceeff8-3dd8-4200-8c35-c23de3f5d8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scaling_method='std', scoring='neg_mean_squared_error', cv_folds=4, \n",
    "plot_performance_metrics(performance_metrics_train_lr, performance_metrics_test_lr, cv_scores_r2_lr, cv_scores_mse_lr, cv_scores_r2_train_lr, cv_scores_mse_train_lr, cv_scores_r2_test_lr, cv_scores_mse_test_lr)\n",
    "#all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2000ff-94a9-4c21-bc89-c8c39d2ebc92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scaling_method='std', scoring='neg_mean_squared_error', cv_folds=4, \n",
    "plot_performance_metrics(performance_metrics_train_lr, performance_metrics_test_lr, cv_scores_r2_lr, cv_scores_mse_lr, cv_scores_r2_train_lr, cv_scores_mse_train_lr, cv_scores_r2_test_lr, cv_scores_mse_test_lr)\n",
    "#summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de88ccc4-8b42-4864-902f-40965405e94f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scaling_method='std', scoring='neg_mean_squared_error', cv_folds=4, \n",
    "plot_performance_metrics(performance_metrics_train_lr, performance_metrics_test_lr, cv_scores_r2_lr, cv_scores_mse_lr, cv_scores_r2_train_lr, cv_scores_mse_train_lr, cv_scores_r2_test_lr, cv_scores_mse_test_lr)\n",
    "#winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48628e3e-f19f-4c96-ba9d-07173eca0355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nrmse und max scaling all\n",
    "plot_permutation_importances(permutation_importances_test_lr, permutation_importances_train_lr, predictor_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e4166-a1d3-4764-b410-54b9234698c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nrmse und max scaling summer\n",
    "plot_permutation_importances(permutation_importances_test_lr, permutation_importances_train_lr, predictor_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f43b2-6530-46ac-9247-128ff175ed70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nrmse und max scaling\n",
    "plot_permutation_importances(permutation_importances_test_lr, permutation_importances_train_lr, predictor_vars)\n",
    "#winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae68df-4e5c-4708-9da2-1d8d297786b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nrmse und max scaling\n",
    "plot_permutation_importances(permutation_importances_test_lr, permutation_importances_train_lr, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd4d1b-00f3-4168-a049-7b29bd47a325",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4d244-ca3c-4b6d-be65-4f7ae8cdc2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coefficients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168da516-836e-4c78-b824-72a961cb80b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coefficients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2a1da-bb4a-4442-b566-71cf5fcfcf7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coefficients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de8020-d7f1-4f82-b9aa-ef9579aa7c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coefficients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230da56-c6ea-4bf5-b826-76c75a79e3e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294bb34-6542-44aa-9d0f-d0aa04792257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "def test_regression_assumptions_scikit(regression_models, test_data, predictor_vars):\n",
    "    results = []\n",
    "\n",
    "    for region_name in regression_models:\n",
    "        model = regression_models[region_name]\n",
    "        X_test, y_test = test_data[region_name]\n",
    "\n",
    "        # Predict and calculate residuals\n",
    "        predictions = model.predict(X_test)\n",
    "        residuals = y_test - predictions\n",
    "\n",
    "        # Add a constant term for the Breusch-Pagan test\n",
    "        X_test_with_constant = np.column_stack((np.ones(X_test.shape[0]), X_test))\n",
    "\n",
    "        # Test for normality of residuals\n",
    "        shapiro_stat, shapiro_p = shapiro(residuals)\n",
    "\n",
    "        # Test for homoscedasticity\n",
    "        _, _, _, bp_pvalue = het_breuschpagan(residuals, X_test_with_constant)\n",
    "\n",
    "        # VIF for multicollinearity\n",
    "        vif = [variance_inflation_factor(X_test, i) for i in range(X_test.shape[1])]\n",
    "\n",
    "        # Prepare plot data\n",
    "        plot_data = {\n",
    "            'Region': region_name,\n",
    "            'Predictions': predictions,\n",
    "            'Residuals': residuals,\n",
    "            'Shapiro-Wilk': shapiro_stat,\n",
    "            'Shapiro-Wilk p-value': shapiro_p,\n",
    "            'Breusch-Pagan p-value': bp_pvalue,\n",
    "            'VIF': vif\n",
    "        }\n",
    "        results.append(plot_data)\n",
    "    \n",
    "    # Plotting\n",
    "    num_regions = len(results)\n",
    "    fig, axs = plt.subplots(num_regions, 3, figsize=(22, 5 * num_regions)) # Changed to 3 subplots for VIF\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        sns.residplot(x=result['Predictions'], y=result['Residuals'], lowess=True, ax=axs[i, 0])\n",
    "        axs[i, 0].set_title(f'Residuals vs Predictions for {result[\"Region\"]}')\n",
    "        axs[i, 0].set_xlabel('Predicted values')\n",
    "        axs[i, 0].set_ylabel('Residuals')\n",
    "\n",
    "        # Adding text for statistical tests\n",
    "        axs[i, 0].text(0.05, 0.95, f\"Shapiro-Wilk: {result['Shapiro-Wilk']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "        axs[i, 0].text(0.05, 0.90, f\"Shapiro-Wilk p-value: {result['Shapiro-Wilk p-value']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "        axs[i, 0].text(0.05, 0.85, f\"Breusch-Pagan p-value: {result['Breusch-Pagan p-value']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "\n",
    "        sns.histplot(result['Residuals'], kde=True, ax=axs[i, 1])\n",
    "        axs[i, 1].set_title(f'Residual Distribution for {result[\"Region\"]}')\n",
    "        axs[i, 1].set_xlabel('Residuals')\n",
    "        axs[i, 1].set_ylabel('Frequency')\n",
    "\n",
    "        # VIF bar plot\n",
    "        sns.barplot(x=predictor_vars, y=result['VIF'], ax=axs[i, 2])\n",
    "        axs[i, 2].hlines(5, xmin=-0.5, xmax=len(result['VIF'])-0.5, colors='orange', linestyles='dashed')\n",
    "        axs[i, 2].hlines(10, xmin=-0.5, xmax=len(result['VIF'])-0.5, colors='r', linestyles='dashed')\n",
    "        axs[i, 2].set_title(f'VIF for {result[\"Region\"]}')\n",
    "        axs[i, 2].set_xlabel('Predictor Variables')\n",
    "        axs[i, 2].set_ylabel('VIF Value')\n",
    "        axs[i, 2].set_ylim(0, max(result['VIF']) + 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b5be9-7f5c-4e04-871c-ac4658adfc37",
   "metadata": {
    "tags": []
   },
   "source": [
    "Linearity: Random pattern without any discernible pattern --> non-linearity\n",
    "\n",
    "Independence of Errors: Durbin-Watson values between 1.5 and 2.5 are relatively normal\n",
    "\n",
    "Homoscedasticity (Equal Variance of Errors):  Variance of the residuals is consistent across all levels of the predicted values +\n",
    "                                              Breusch-Pagan test: A small p-value (typically <= 0.05)  suggests heteroscedasticity, which can invalidate                                                   some of the statistical conclusions of the regression.\n",
    "                                              \n",
    "Normality of Errors: Shapiro-Wilk test: The closer this value is to 1, the more the residuals follow a normal distribution. \n",
    "                     Sapiro-Wilk p-value: The p-value from the Shapiro-Wilk test. A small p-value (typically <= 0.05) indicates that                        the residuals do not follow a normal distribution.\n",
    "                     \n",
    "Variance Inflation Factor (VIF): Measures multicollinearity among the independent variables in the regression model. A VIF value                                        greater than 10 is typically considered an indicator of serious multicollinearity that could affect                                    the model's estimates.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0b0f4-5140-4b39-acfe-f40b8465190c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assumptions_df = test_regression_assumptions_scikit(regression_models, test_data, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739efcc9-d79d-48c1-b8eb-823cc66ad9d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54190025-097b-495b-96e4-a240ea3a7450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_model_performance_all_regions(ds_change, ds_hist, regression_models, test_data, performance_metrics, predictor_vars, residuals, cv_scores):\n",
    "    region_names = ds_change.names.values\n",
    "    region_indices = ds_change.region.values\n",
    "    \n",
    "    num_regions = len(region_names)\n",
    "    plots_per_region = 3  # Number of plots per region\n",
    "    total_plots = num_regions * plots_per_region\n",
    "    cols = 3  # Number of columns (one for each type of plot)\n",
    "    rows = math.ceil(total_plots / cols)  # Calculate the total number of rows needed\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(20, rows * 5), squeeze=False)\n",
    "\n",
    "    plot_idx = 0  # Initialize plot index\n",
    "    for region in region_indices:\n",
    "        region_name = ds_change.names.sel(region=region).values.item()\n",
    "        X_test, y_test = test_data[region_name]\n",
    "        model = regression_models[region_name]\n",
    "        y_pred = model.predict(X_test)\n",
    "        resids = residuals[region_name]\n",
    "\n",
    "        mse_value = performance_metrics[region_name]['MSE']\n",
    "        r2_value = performance_metrics[region_name]['R2']\n",
    "        cv_score = np.mean(cv_scores[region_name])  # Average CV score\n",
    "\n",
    "        # Plot 1: Residuals\n",
    "        ax_resid = axs[plot_idx // cols, plot_idx % cols]\n",
    "        ax_resid.scatter(y_test, resids, c='blue', alpha=0.5, s=10, label='Residuals')\n",
    "        ax_resid.axhline(0, color='red', lw=2, label='Zero Residual')\n",
    "        ax_resid.set_xlabel('True Values')\n",
    "        ax_resid.set_ylabel('Residuals')\n",
    "        ax_resid.set_title(f\"{region_name} - Residuals\")\n",
    "        ax_resid.legend()\n",
    "        plot_idx += 1\n",
    "\n",
    "        # Plot 2: Predictions vs True Values\n",
    "        ax_pred = axs[plot_idx // cols, plot_idx % cols]\n",
    "        ax_pred.scatter(y_test, y_pred, c='green', alpha=0.5, s=10, label='Predictions')\n",
    "        ax_pred.plot(y_test, y_test, color='orange', label='Ideal Prediction')\n",
    "        ax_pred.set_xlabel('True Values')\n",
    "        ax_pred.set_ylabel('Predicted Values')\n",
    "        ax_pred.set_title(f\"{region_name} - Predictions\")\n",
    "        ax_pred.legend()\n",
    "        plot_idx += 1\n",
    "\n",
    "        # Plot 3: Density Plot\n",
    "        ax_density = axs[plot_idx // cols, plot_idx % cols]\n",
    "        sns.kdeplot(y_test, ax=ax_density, label='Actual Values', fill=True)\n",
    "        sns.kdeplot(y_pred, ax=ax_density, label='Predicted Values', fill=True)\n",
    "        ax_density.set_xlabel('Values')\n",
    "        ax_density.set_title(f\"{region_name} - Density Plot\")\n",
    "        ax_density.legend()\n",
    "        plot_idx += 1\n",
    "\n",
    "        # Include performance metrics in the text or title\n",
    "        ax_density.text(0.05, 0.95, f'MSE: {mse_value:.6f}\\nR^2: {r2_value:.2f}\\nCV: {cv_score:.2f}', transform=ax_density.transAxes, verticalalignment='top')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f195a9-76c9-44b0-86d8-a4de4b9cab52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# relative change scaled\n",
    "assess_model_performance_all_regions(ds, ds_hist_ensemble_metric, regression_models, test_data, performance_metrics, predictor_vars, residuals, cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fe453-5ea1-48f6-840f-23c70db2fbd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assess_model_performance_all_regions(ds, ds_hist_ensemble_metric, regression_models, test_data, performance_metrics, predictor_vars, residuals, cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd280fe-f3ee-4897-89d8-efa39f1fc9de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assess_model_performance_all_regions(ds, ds_hist_ensemble_metric, regression_models, test_data, performance_metrics, predictor_vars, residuals, cv_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20765f-92f2-446a-8b9e-364243a61bd0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test optimal variable subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4446c6-b0ce-473f-a625-da48b9ac8f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef977dfd-2fde-4c63-bdb7-70ec7c241c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_combinations = list(all_subsets(predictor_vars)) \n",
    "print(f\"Number of variable combinations with at least 4 variables: {len(all_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a18c3-2755-49f6-8409-be67142163b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def all_subsets(ss, min_length=4):\n",
    "    \"\"\"Generate all combinations of the elements in `ss` with a minimum length of `min_length`.\"\"\"\n",
    "    return chain(*map(lambda x: combinations(ss, x), range(min_length, len(ss)+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df62cd-c63b-4ad1-91d0-ea3a0ad34d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_variable_combinations(ds, all_vars, predictant):\n",
    "    # Get all possible combinations of predictor variables with at least 4 variables\n",
    "    all_combinations = list(all_subsets(all_vars))\n",
    "    print(f\"Number of variable combinations: {len(all_combinations)}\")\n",
    "\n",
    "    # Prepare a list to store the performance metrics\n",
    "    performance_list = []\n",
    "\n",
    "    for combination in all_combinations:\n",
    "        # Convert the tuple to a list of variables for this combination\n",
    "        current_vars = list(combination)\n",
    "\n",
    "        # Train the models using the current combination of variables\n",
    "        _, regression_models, _, _, _, _ = train_multivariate_models(ds, current_vars, predictant)\n",
    "       \n",
    "        # Assess model performance for all regions\n",
    "        for region in ds.region.values:\n",
    "            region_name = ds.names.sel(region=region).values.item()\n",
    "            \n",
    "            # Prepare the data for the region\n",
    "            df = ds.sel(region=region).to_dataframe().dropna()\n",
    "            X = df[current_vars]\n",
    "            y_true = df[predictant].values\n",
    "\n",
    "            # Standardize the predictor variables\n",
    "            scaler = StandardScaler()\n",
    "            X_standardized = scaler.fit_transform(X) # standarized using this method might be wrong cause we need the direction. Standarize \n",
    "            #with mean \n",
    "            \n",
    "            # Predict using the trained model\n",
    "            model = regression_models[region_name]\n",
    "            y_pred = model.predict(X_standardized)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            \n",
    "            # Append the performance metrics for this region and variable combination to the list\n",
    "            performance_list.append({\n",
    "                'Variables': ', '.join(current_vars),\n",
    "                'MSE': mse,\n",
    "                'R^2': r2\n",
    "            })\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    performance_summary = pd.DataFrame(performance_list)\n",
    "    \n",
    "    # Calculate the aggregated performance for each variable combination across all regions\n",
    "    aggregated_performance = performance_summary.groupby('Variables').agg(['mean', 'min', 'max']).reset_index()\n",
    "    aggregated_performance.columns = [' '.join(col).strip() for col in aggregated_performance.columns.values]\n",
    "\n",
    "    # Sort the results by Mean MSE and Mean R^2\n",
    "    aggregated_performance.sort_values(by=['MSE mean', 'R^2 mean'], ascending=[True, False], inplace=True)\n",
    "\n",
    "    # Select and rename the columns to only include the required statistics\n",
    "    final_table = aggregated_performance[['Variables', 'MSE mean', 'MSE min', 'MSE max', 'R^2 mean', 'R^2 min', 'R^2 max']]\n",
    "    final_table.rename(columns={\n",
    "        'MSE mean': 'Mean MSE',\n",
    "        'MSE min': 'Min MSE',\n",
    "        'MSE max': 'Max MSE',\n",
    "        'R^2 mean': 'Mean R^2',\n",
    "        'R^2 min': 'Min R^2',\n",
    "        'R^2 max': 'Max R^2'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a331d62-6de2-41bf-99b5-45dc25417f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "pd.set_option('display.max_rows', None)  \n",
    "test_variable_combinations(ds, predictor_vars, 'bgws')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9cb559-e022-4ce0-9729-864ebc2a3ced",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23b317-affe-4764-a6a9-7109b463ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_df*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccddc8ef-a7e1-4fcb-bf29-1d75cc216562",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Cluster regions based on regression coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8df60-5047-4cbb-9413-f6661a0850d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def cluster_regions(coefficients_df, max_clusters=40):\n",
    "    \"\"\"\n",
    "    This function finds the optimal number of clusters using the elbow method\n",
    "    and clusters regions based on their coefficients.\n",
    "    \n",
    "    :param coefficients_df: A pandas DataFrame containing the coefficients with regions as the index.\n",
    "    :param max_clusters: The maximum number of clusters to test for the elbow method.\n",
    "    :return: A tuple of pandas DataFrames with the first containing the coefficients and an additional\n",
    "             column 'Cluster', and the second containing the centroids of each cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the optimal number of clusters using the elbow method\n",
    "    sum_of_squared_distances = []\n",
    "    K = range(1, max_clusters + 1)\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k,  n_init=10, random_state=42)\n",
    "        km = km.fit(coefficients_df)\n",
    "        sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "    # Plot the elbow curve\n",
    "    plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum of squared distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    # Ask user for the optimal number of clusters\n",
    "    n_clusters = int(input(\"Enter the optimal number of clusters: \"))\n",
    "    \n",
    "    # Use the KMeans algorithm to find clusters with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    kmeans.fit(coefficients_df)\n",
    "\n",
    "    # Assign the clusters to each region\n",
    "    clusters = kmeans.labels_\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Calculate the distance of each region's coefficients to the centroid of its cluster\n",
    "    distances_to_centroid = cdist(coefficients_df, centroids, 'euclidean')\n",
    "    min_distances = distances_to_centroid.min(axis=1)\n",
    "\n",
    "    # Add the cluster labels and distances to the DataFrame\n",
    "    df = coefficients_df.copy()\n",
    "    df['Cluster'] = clusters\n",
    "    df['Distance_to_Centroid'] = (min_distances * 100).round(decimals=2)\n",
    "    df.index.name = 'Region'\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Order the dataframe by cluster label and distance to centroid\n",
    "    clustered_df = df.sort_values(['Cluster', 'Distance_to_Centroid'])\n",
    "\n",
    "    # Get the centroids\n",
    "    centroids_df = pd.DataFrame(centroids, columns=coefficients_df.columns)\n",
    "    centroids_df['Cluster'] = range(n_clusters)\n",
    "\n",
    "    return clustered_df, centroids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9390a-3f10-4f89-b5da-97baaf850f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_regions(coefficients_df, max_clusters=40):\n",
    "    \"\"\"\n",
    "    This function finds the optimal number of clusters using the elbow method\n",
    "    and clusters regions based on their coefficients.\n",
    "\n",
    "    :param coefficients_df: A pandas DataFrame containing the coefficients with regions as the index.\n",
    "    :param max_clusters: The maximum number of clusters to test for the elbow method.\n",
    "    :return: A pandas DataFrame with an additional column 'Cluster' indicating the cluster for each region.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the optimal number of clusters using the elbow method\n",
    "    sum_of_squared_distances = []\n",
    "    K = range(1, max_clusters + 1)\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k,  n_init=10, random_state=42)\n",
    "        km = km.fit(coefficients_df)\n",
    "        sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "    # Plot the elbow curve\n",
    "    plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum of squared distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    # Ask user for the optimal number of clusters\n",
    "    n_clusters = int(input(\"Enter the optimal number of clusters: \"))\n",
    "    \n",
    "    # Use the KMeans algorithm to find clusters with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    kmeans.fit(coefficients_df)\n",
    "\n",
    "    # Assign the clusters to each region\n",
    "    clusters = kmeans.labels_\n",
    "\n",
    "    # Add the cluster labels to the DataFrame\n",
    "    df = coefficients_df.copy()\n",
    "    df['Cluster'] = clusters\n",
    "    df.index.name = 'Region'\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Order the dataframe by cluster label\n",
    "    clustered_df = df.sort_values('Cluster')\n",
    "\n",
    "    return clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbe0b8-664c-48cf-bcab-6b7c784f39e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "def custom_distance(u, v):\n",
    "    # Custom distance metric:\n",
    "    # If any signs differ, the distance is increased\n",
    "    sign_diff = np.sign(u) != np.sign(v)\n",
    "    if sign_diff.any():  # If any signs differ, apply the penalty\n",
    "        return np.linalg.norm(u - v) * 1\n",
    "    else:\n",
    "        return np.linalg.norm(u - v)\n",
    "    \n",
    "def cluster_regions(coefficients_df, n_clusters=12):\n",
    "    # Create a new DataFrame for clustering to avoid modifying the original data\n",
    "    X = coefficients_df.copy()\n",
    "\n",
    "    # Compute the custom distance matrix\n",
    "    dist_matrix = distance_matrix(X.values, X.values, p=2)\n",
    "    for i in range(dist_matrix.shape[0]):\n",
    "        for j in range(dist_matrix.shape[1]):\n",
    "            if i != j:  # No need to penalize the diagonal\n",
    "                dist_matrix[i, j] = custom_distance(X.values[i], X.values[j])\n",
    "\n",
    "    # Perform Agglomerative Clustering with the precomputed distance matrix\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, affinity='precomputed', linkage='complete')\n",
    "    clusters = clustering.fit_predict(dist_matrix)\n",
    "\n",
    "    # Assign the clusters to each region\n",
    "    coefficients_df['Cluster'] = clusters\n",
    "    \n",
    "    # Order the dataframe by cluster label\n",
    "    clustered_df = coefficients_df.sort_values('Cluster')\n",
    "    clustered_df.index.name = 'Region'\n",
    "    clustered_df.reset_index(inplace=True)\n",
    "    \n",
    "    return clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1060349-0757-4abb-9b5f-c8450a86daec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_df, centroids_df = cluster_regions(coefficients_df)\n",
    "clustered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b338d45-d31e-4f18-9a27-5e6bbdcee56b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot clusters on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac1398-83d2-405a-a467-9b8ba965f461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "colors = [(34/255, 139/255, 34/255), (1, 1, 1), (60/255, 145/255, 230/255)]  # Green -> White -> Blue\n",
    "n_bins = [3]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom_div_cmap'\n",
    "\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)\n",
    "\n",
    "# To test and display the colormap\n",
    "fig, ax = plt.subplots(figsize=(6, 1))\n",
    "ax.set_title('Custom Diverging Colormap')\n",
    "plt.imshow(np.linspace(0, 1, 256).reshape(1, -1), aspect='auto', cmap=cm)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6bb7a-ed6a-4c0f-9566-beb8f4af6cad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import ShapelyFeature\n",
    "import regionmask\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956d530-fad6-429d-9af7-17147c713db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "def split_polygon(polygon, meridian=180):\n",
    "    \"\"\"\n",
    "    Splits a Shapely polygon into two polygons at a specified meridian\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    if maxx > meridian and minx < -meridian:\n",
    "        # Polygon crosses the antimeridian\n",
    "        left_poly = []\n",
    "        right_poly = []\n",
    "        for x, y in polygon.exterior.coords:\n",
    "            if x >= meridian:\n",
    "                right_poly.append((x - 360, y))  # Wraparound for the right side\n",
    "            else:\n",
    "                left_poly.append((x, y))\n",
    "        return [Polygon(left_poly), Polygon(right_poly)]\n",
    "    else:\n",
    "        return [polygon]  # Wrap the single polygon in a list for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da892dc0-63f5-4868-8d2f-c222339ab036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_clusters_and_regions(ds_ensmed_glob, ds, clustered_df):\n",
    "    # Initialize the plot with a cartopy projection\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "    ax_main = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "    \n",
    "    # Plot the 'bgws' variable from the dataset\n",
    "    ds_ensmed_glob['bgws'].plot(ax=ax_main, vmin=-0.2, vmax=0.2, cmap=cm, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "    \n",
    "    # Add coastlines and gridlines\n",
    "    ax_main.coastlines()\n",
    "    ax_main.tick_params(axis='both', which='major', labelsize=20)\n",
    "    gridlines = ax_main.gridlines(draw_labels=True, color='black', alpha=0.2, linestyle='--')\n",
    "    gridlines.top_labels = gridlines.right_labels = False\n",
    "    gridlines.xlabel_style = {'size': 18}\n",
    "    gridlines.ylabel_style = {'size': 18}\n",
    "    \n",
    "    # Get region bounds using regionmask\n",
    "    land_regions = regionmask.defined_regions.ar6.land\n",
    "    \n",
    "    # Create a mapping from region names to region numbers\n",
    "    region_name_to_number = dict(zip(ds.names.values, ds.region.values))\n",
    "    \n",
    "    # Map the region names to the same order as ds.names.values\n",
    "    region_to_cluster_map = dict(zip(clustered_df['Region'], clustered_df['Cluster']))\n",
    "\n",
    "    # Now create a new column in ds that maps the region names to cluster numbers\n",
    "    # This assumes ds.names.values has the same region names as in clustered_df['Region']\n",
    "    ds['Cluster'] = [region_to_cluster_map[name] for name in ds.names.values]\n",
    "\n",
    "    # Convert the 'Cluster' DataArray to a NumPy array and get unique values\n",
    "    unique_clusters = np.unique(ds['Cluster'].values)\n",
    "\n",
    "    # Prepare colors for clusters - this assumes a finite number of clusters\n",
    "    cluster_colors = plt.cm.tab20b(np.linspace(0, 1, len(unique_clusters)))\n",
    "\n",
    "\n",
    "    # Loop over the regions and plot the cluster numbers with abbreviations\n",
    "    for region_name, cluster_number in zip(ds.names.values, ds['Cluster']):\n",
    "        reg_num = region_name_to_number[region_name]\n",
    "        region_polygons = land_regions[reg_num].polygon\n",
    "        \n",
    "        region_abbr = ds.abbrevs.values[ds.region.values == reg_num][0]  # Assuming this gives us the correct abbreviation\n",
    "        cluster_color = cluster_colors[cluster_number]  # Get the color for the cluster\n",
    "        \n",
    "        # Fetch the polygon or polygons for this region\n",
    "        region_obj = land_regions[reg_num]\n",
    "        if hasattr(region_obj, 'polygons'):\n",
    "            # If the attribute is 'polygons', we assume it's iterable (e.g., a list of Polygon objects)\n",
    "            region_polygons = region_obj.polygons\n",
    "        elif hasattr(region_obj, 'polygon'):\n",
    "            # If there's only one Polygon, we wrap it in a list to make it iterable\n",
    "            region_polygons = [region_obj.polygon]\n",
    "        else:\n",
    "            raise AttributeError(f\"The region object does not have 'polygons' or 'polygon' attribute.\")\n",
    "\n",
    "        for region_polygon in region_polygons:\n",
    "            # If the polygon crosses the antimeridian, split it\n",
    "            split_polys = split_polygon(region_polygon)\n",
    "\n",
    "            # Handle both Polygon and MultiPolygon types after splitting\n",
    "            for poly in split_polys:\n",
    "                if isinstance(poly, Polygon):\n",
    "                    features_to_plot = [poly]\n",
    "                elif isinstance(poly, MultiPolygon):\n",
    "                    features_to_plot = list(poly.geoms)\n",
    "                else:\n",
    "                    raise TypeError(f\"Unhandled geometry type: {type(poly)}\")\n",
    "\n",
    "                for feature_poly in features_to_plot:\n",
    "                    feature = ShapelyFeature([feature_poly], ccrs.PlateCarree(), edgecolor=cluster_color, facecolor='none', linewidth=2)\n",
    "                    ax_main.add_feature(feature)\n",
    "\n",
    "            # Calculate the centroid for text placement using features_to_plot\n",
    "            centroids = [feature_poly.centroid for feature_poly in features_to_plot]\n",
    "    \n",
    "            text_lon, text_lat = max(centroids, key=lambda c: c.x).coords[0]  # Use the easternmost centroid\n",
    "        \n",
    "            # Ensure cluster_number is a plain integer if it's a single-item array or DataArray\n",
    "            if isinstance(cluster_number, np.ndarray) and cluster_number.size == 1:\n",
    "                cluster_number = cluster_number.item()  # Converts a one-element array to a scalar\n",
    "            elif isinstance(cluster_number, xr.DataArray) and cluster_number.ndim == 0:\n",
    "                cluster_number = cluster_number.values.item()  # Gets the scalar value from a 0-dim DataArray\n",
    "\n",
    "            # Annotate the cluster number for each region\n",
    "            ax_main.text(text_lon, text_lat, f\"{region_abbr}\\n{cluster_number}\",\n",
    "                         horizontalalignment='center', verticalalignment='center', transform=ccrs.PlateCarree(),\n",
    "                         fontsize=20, bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71db8b-8bee-4dd8-bf6c-15673089129a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_clusters_and_regions(ds_ensmed_glob, ds, clustered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03c495-fab5-4094-8cf2-b66258c891eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a242050-5cba-4f75-b7f6-4f9e9fb1a321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_simple_map(ds_ensmed_glob, projection, save_fig=False):\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "    ax_main = fig.add_subplot(1, 1, 1, projection=projection)\n",
    "    \n",
    "    # Plot the 'bgws' variable from the dataset\n",
    "    img = ds_ensmed_glob['bgws'].plot(ax=ax_main, vmin=-0.6, vmax=0.6,  cmap=cm, transform=ccrs.PlateCarree(), add_colorbar=False) \n",
    "    \n",
    "    # Add coastlines and gridlines\n",
    "    ax_main.coastlines()\n",
    "    ax_main.tick_params(axis='both', which='major', labelsize=20)\n",
    "    gridlines = ax_main.gridlines(draw_labels=True, color='black', alpha=0.2, linestyle='--')\n",
    "    gridlines.top_labels = gridlines.right_labels = False\n",
    "    gridlines.xlabel_style = {'size': 18}\n",
    "    gridlines.ylabel_style = {'size': 18}\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar_ax = fig.add_axes([0.314, 0.1, 0.4, 0.025]) #left, bottom, width, height\n",
    "    cbar = fig.colorbar(img, cax=cbar_ax, extend='both', orientation='horizontal')\n",
    "    cbar.set_label(\"Blue-Green Water Share\", fontsize=22, weight='bold', labelpad=15) \n",
    "    cbar.ax.tick_params(labelsize=18)\n",
    "   \n",
    "    plt.show()\n",
    "    \n",
    "    # Safe figure\n",
    "    if save_fig:\n",
    "        savepath = os.path.join('../..', 'results', 'CMIP6', 'historical', 'time', 'median', 'bgws')\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        filename = f'Ensemble_median.1985-2014.bgws.historical.pdf'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        fig.savefig(filepath, dpi=600, bbox_inches='tight', format='pdf')\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830683f-e43c-47f4-8feb-b877525d64e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_simple_map(ds_ensmed_glob, ccrs.Robinson(), save_fig=True) # Robinson PlateCarree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890934ec-5181-4729-b988-5b7ac0321d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_simple_map(ds_ensmed_glob, ccrs.PlateCarree(), save_fig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449211a-b442-4503-a52b-98fc8ee19228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d4e8ecb-11c5-41a6-8818-e99123f72e1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare Permutation Importance of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd8172-1b1f-4bc1-8c7b-b9ff32b52d68",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Compare LR test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5b647-7ce1-429d-b450-7bc710888ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_permutation_importance(permutation_importances_train, permutation_importances_test, performance_metrics_train, performance_metrics_test, predictor_vars):\n",
    "    comparison_results = {}\n",
    "\n",
    "    for region in permutation_importances_train.keys():\n",
    "        mean_importance_train = pd.Series(permutation_importances_train[region].mean(axis=1), index=predictor_vars)\n",
    "        mean_importance_test = pd.Series(permutation_importances_test[region].mean(axis=1), index=predictor_vars)\n",
    "\n",
    "        # Rank the variables, excluding negative importances\n",
    "        rank_train = mean_importance_train.rank(method='dense', ascending=False).where(mean_importance_train >= 0, np.nan)\n",
    "        rank_test = mean_importance_test.rank(method='dense', ascending=False).where(mean_importance_test >= 0, np.nan)\n",
    "\n",
    "        # Special rank for zero importance in both training and test\n",
    "        zero_importance = (mean_importance_train == 0) & (mean_importance_test == 0)\n",
    "        max_rank = max(rank_train.max(), rank_test.max()) + 1\n",
    "        rank_train[zero_importance] = max_rank\n",
    "        rank_test[zero_importance] = max_rank\n",
    "\n",
    "        # Calculate agreement\n",
    "        valid_indices = (mean_importance_train >= 0) | (mean_importance_test >= 0)\n",
    "        agreement = np.mean(rank_train[valid_indices] == rank_test[valid_indices]) * 100\n",
    "\n",
    "        # Prepare data for dataframe\n",
    "        data = {\n",
    "            'Variable': predictor_vars,\n",
    "            'Train_Rank': rank_train,\n",
    "            'Test_Rank': rank_test,\n",
    "            'Train_Mean_Importance': mean_importance_train,\n",
    "            'Test_Mean_Importance': mean_importance_test,\n",
    "            'Train_R2': performance_metrics_train[region]['R2'],\n",
    "            'Test_R2': performance_metrics_test[region]['R2'],\n",
    "            'Agreement (%)': agreement\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(data).sort_values(by='Train_Rank')\n",
    "        comparison_results[region] = df\n",
    "\n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6841e-4031-4f58-be5f-27bafd7f8711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_permutation_importance(permutation_importances_train, permutation_importances_test, performance_metrics_train, performance_metrics_test, predictor_vars, zero_threshold=0.001):\n",
    "    comparison_results = {}\n",
    "\n",
    "    for region in permutation_importances_train.keys():\n",
    "        mean_importance_train = pd.Series(permutation_importances_train[region].mean(axis=1), index=predictor_vars)\n",
    "        mean_importance_test = pd.Series(permutation_importances_test[region].mean(axis=1), index=predictor_vars)\n",
    "\n",
    "        # Separate zero and non-zero importance variables\n",
    "        is_zero_train = mean_importance_train.abs() <= zero_threshold\n",
    "        is_zero_test = mean_importance_test.abs() <= zero_threshold\n",
    "\n",
    "        # Rank the variables, treating near-zero importance variables separately\n",
    "        rank_train = mean_importance_train.rank(method='dense', ascending=False, na_option='bottom').astype('Int64')\n",
    "        rank_test = mean_importance_test.rank(method='dense', ascending=False, na_option='bottom').astype('Int64')\n",
    "\n",
    "        # Adjust ranks for zero importance variables\n",
    "        max_rank = max(rank_train.max(), rank_test.max()) + 1\n",
    "        rank_train[is_zero_train & is_zero_test] = max_rank\n",
    "        rank_test[is_zero_train & is_zero_test] = max_rank\n",
    "\n",
    "        # Calculate agreement\n",
    "        agreement = np.mean(rank_train == rank_test) * 100\n",
    "\n",
    "        # Prepare data for dataframe\n",
    "        data = {\n",
    "            'Variable': predictor_vars,\n",
    "            'Train_Rank': rank_train,\n",
    "            'Test_Rank': rank_test,\n",
    "            'Train_Mean_Importance': mean_importance_train,\n",
    "            'Test_Mean_Importance': mean_importance_test,\n",
    "            'Train_R2': performance_metrics_train[region]['R2'],\n",
    "            'Test_R2': performance_metrics_test[region]['R2'],\n",
    "            'Agreement (%)': agreement\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(data).sort_values(by='Train_Rank')\n",
    "        comparison_results[region] = df\n",
    "\n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27797b-f7a8-4706-9f7d-f8bb4a8ce2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "comparison_results = compare_permutation_importance(permutation_importances_train_lr, permutation_importances_test_lr, performance_metrics_train_lr, performance_metrics_test_lr, predictor_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f09ee-b3eb-455d-9b94-642b27625abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(comparison_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54323b-2015-40c1-815d-a597cbf5ff76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Displaying the results for one region as an example\n",
    "region_number = 8\n",
    "print(list(comparison_results.keys())[region_number])\n",
    "comparison_results[list(comparison_results.keys())[region_number]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bf976-55dc-44c9-959a-e6f650d87d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean overall agreement and r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95257d65-7dc2-4c22-9431-27fd9af6287c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_agreement_metrics(comparison_results):\n",
    "    overall_agreement = []\n",
    "    first_rank_agreement = []\n",
    "\n",
    "    for region, df in comparison_results.items():\n",
    "        # Calculate overall agreement for the region\n",
    "        agreement = df['Agreement (%)'].iloc[0]\n",
    "        overall_agreement.append(agreement)\n",
    "\n",
    "        # Check if the top-ranked variable is the same in training and test data\n",
    "        top_train = df[df['Train_Rank'] == 1.0]['Variable'].iloc[0] if any(df['Train_Rank'] == 1.0) else None\n",
    "        top_test = df[df['Test_Rank'] == 1.0]['Variable'].iloc[0] if any(df['Test_Rank'] == 1.0) else None\n",
    "        first_rank_agreement.append(top_train == top_test)\n",
    "\n",
    "    # Calculate average agreement\n",
    "    avg_overall_agreement = sum(overall_agreement) / len(overall_agreement)\n",
    "    avg_first_rank_agreement = sum(first_rank_agreement) / len(first_rank_agreement) * 100\n",
    "\n",
    "    return avg_overall_agreement, avg_first_rank_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77cea22-a82d-4138-8ab2-8152a86c87db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_overall_agreement, avg_first_rank_agreement = compute_agreement_metrics(comparison_results)\n",
    "print(\"Average Overall Agreement:\", avg_overall_agreement)\n",
    "print(\"Average Agreement on First Rank:\", avg_first_rank_agreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a8b8e-607a-4c89-bf82-abab6ae8990a",
   "metadata": {},
   "source": [
    "##### Compare xgb test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb73b00-5f9f-4e39-b5e0-bc143879a077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "comparison_results_xgb = compare_permutation_importance(permutation_importances_train_xgb, permutation_importances_test_xgb, performance_metrics_train_xgb, performance_metrics_test_xgb, predictor_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7522d9-e451-4858-8f70-44099726bf9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Displaying the results for one region as an example\n",
    "region_number = 15\n",
    "print(list(comparison_results_xgb.keys())[region_number])\n",
    "comparison_results_xgb[list(comparison_results_xgb.keys())[region_number]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e2c6ba-83d6-406c-b456-dd40cc2a9230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_overall_agreement, avg_first_rank_agreement = compute_agreement_metrics(comparison_results_xgb)\n",
    "print(\"Average Overall Agreement:\", avg_overall_agreement)\n",
    "print(\"Average Agreement on First Rank:\", avg_first_rank_agreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1087570-0785-456d-acc5-182c27251f82",
   "metadata": {},
   "source": [
    "##### Compare xgb and lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f6cb3-f735-4d31-b002-ef91db448173",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Build Gaussian Processes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76996ca-c3bf-493f-9910-68861753f0d5",
   "metadata": {},
   "source": [
    "Probabilistic Outputs: GPs not only provide a prediction for each data point but also give a measure of uncertainty (variance) associated with that prediction. This can help in understanding the confidence of the model in different regions of the input space.\n",
    "\n",
    "Non-Linear Relationships: GPs, with the right choice of kernel, can model complex non-linear relationships between inputs and outputs, making them more flexible than traditional linear regression models.\n",
    "\n",
    "Kernel Flexibility: The kernel in a GP defines the relationship between data points. By selecting or designing a kernel that captures the underlying structure of the data, GPs can be adapted to various types of data patterns.\n",
    "\n",
    "Finally, regarding variable importance: In the context of GPs, interpreting variable importance is not as straightforward as in linear regression. However, one common approach is to examine the sensitivity of the GP's predictions to changes in each input variable. The Automatic Relevance Determination (ARD) kernel, for instance, can adapt its length scale for each dimension of the input space, which can give an indication of the relative importance of each input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea48e9e-6c48-442b-8981-68199cac085a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_data_for_gp(ds, region_index):\n",
    "    # Extract data for the given region\n",
    "    region_data = ds.sel(region=region_index)\n",
    "    \n",
    "    # Prepare predictor and target variables\n",
    "    X = region_data[['pr', 'vpd', 'evspsbl', 'mrro', 'mrso', 'tran', 'lai', 'gpp']].to_array().transpose('lat', 'lon', 'variable')\n",
    "    y = region_data['bgws'].stack(z=(\"lat\", \"lon\"))\n",
    "    \n",
    "    # Convert X to a 2D array\n",
    "    X = X.stack(z=(\"lat\", \"lon\")).transpose('z', 'variable').values\n",
    "    \n",
    "    # Create a mask where either X or y has NaN values\n",
    "    mask = ~np.isnan(y) & ~np.any(np.isnan(X), axis=1)\n",
    "    \n",
    "    # Filter out rows using the mask\n",
    "    X_filtered = X[mask, :]\n",
    "    y_filtered = y[mask].values  # Convert to numpy array\n",
    "    \n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "def train_gp_for_region(ds, region_name):\n",
    "    # Prepare data\n",
    "    X, y = prepare_data_for_gp(ds, region_name)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X_standardized = scaler.transform(X)\n",
    "    \n",
    "    # Define the kernel: RBF kernel with ARD + constant term + white noise term for model noise\n",
    "    # Kernel = Covarianzfunction:\n",
    "    # Defines relation between input variables\n",
    "    # \n",
    "    kernel = C(1.0, (1e-3, 1e3)) * RBF(np.ones(8), (1e-2, 1e2)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
    "    \n",
    "    # Initialize and train GP regressor\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, normalize_y=True)\n",
    "    gp.fit(X_standardized, y)\n",
    "    \n",
    "    return gp, scaler\n",
    "\n",
    "def predict_with_gp(gp, scaler, X):\n",
    "    # Standardize the features\n",
    "    X_standardized = scaler.transform(X)\n",
    "    \n",
    "    # Predict using the GP regressor\n",
    "    y_pred, y_std = gp.predict(X_standardized, return_std=True)\n",
    "    \n",
    "    return y_pred, y_std\n",
    "\n",
    "def train_and_predict_for_all_regions(ds):\n",
    "    region_indices = ds.region.values\n",
    "    region_names = ds.names.values\n",
    "    gp_models = {}\n",
    "    scalers = {}\n",
    "    predictions = {}\n",
    "    std_devs = {}\n",
    "    \n",
    "    for i, region_index in enumerate(region_indices):\n",
    "        region_name = region_names[i]\n",
    "        gp, scaler = train_gp_for_region(ds, region_index)\n",
    "        X, y = prepare_data_for_gp(ds, region_index)\n",
    "        \n",
    "        y_pred, y_std = predict_with_gp(gp, scaler, X)\n",
    "        \n",
    "        gp_models[region_name] = gp\n",
    "        scalers[region_name] = scaler\n",
    "        predictions[region_name] = y_pred\n",
    "        std_devs[region_name] = y_std\n",
    "    \n",
    "    return gp_models, scalers, predictions, std_devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28ef89-e109-485e-b4e9-bcc08d62e320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp_models, scalers, predictions, std_devs = train_and_predict_for_all_regions(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0011b-1124-448e-ac82-b97467d1a07f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cc546-460e-45f0-a48d-4586b9ef1428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "def assess_model_performance_all_regions(ds, predictions, std_devs):\n",
    "    region_names = ds.names.values\n",
    "    region_indices = ds.region.values\n",
    "    \n",
    "    # Determine the number of rows and columns for the subplots based on the number of regions\n",
    "    num_regions = len(region_names)\n",
    "    cols = 3  # Assuming you'd like 3 columns\n",
    "    rows = math.ceil(num_regions / cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    \n",
    "    for i, region_index in enumerate(region_indices):\n",
    "        region_name = region_names[i]\n",
    "        \n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        \n",
    "        # Extract true values\n",
    "        _, y_true = prepare_data_for_gp(ds, region_index)\n",
    "        \n",
    "        # Extract predicted values and standard deviations for the region\n",
    "        y_pred = predictions[region_name]\n",
    "        y_std = std_devs[region_name]\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = y_pred - y_true\n",
    "        \n",
    "        # Calculate the RMSE\n",
    "        mse_value = mean_squared_error(y_true, y_pred)\n",
    "        r2_s = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # Plot\n",
    "        ax = axs[row, col]\n",
    "        ax.scatter(y_true, residuals, c='blue', alpha=0.5, s=10)\n",
    "        ax.fill_between(y_true, residuals - y_std, residuals + y_std, color='gray', alpha=0.2)\n",
    "        ax.axhline(0, color='red', lw=2)\n",
    "        ax.text(0.05, 0.95, f'MSE: {mse_value:.6f}', transform=ax.transAxes, verticalalignment='top')\n",
    "        ax.text(0.05, 0.9, f'R2 score: {r2_s:.2f}', transform=ax.transAxes, verticalalignment='top')\n",
    "        ax.set_title(region_name)\n",
    "        ax.set_xlabel('True Values')\n",
    "        ax.set_ylabel('Residuals')\n",
    "        \n",
    "    # Create legend with explicit handles in the last subplot\n",
    "    if num_regions < rows * cols:\n",
    "        last_ax = axs.flatten()[-1]\n",
    "        last_ax.axis('off')\n",
    "        legend_elements = [mlines.Line2D([0], [0], color='blue', marker='o', markersize=10, label='Residuals (Observed - Predicted)', linestyle='None'),\n",
    "                           mlines.Line2D([0], [0], color='gray', alpha=0.2, linewidth=10, label='Prediction Uncertainty (±1 Std. Dev.)'),\n",
    "                           mlines.Line2D([0], [0], color='red', lw=2, label='Zero Residual Line')]\n",
    "        last_ax.legend(handles=legend_elements, loc='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee7fb3-e0e6-478a-bf69-07cf13d8299e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "assess_model_performance_all_regions(ds, predictions, std_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc9c73-d454-470a-aa9d-b6f0263815ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119068a-e003-4c1d-be14-bd4ece7f3ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b4713-6a18-44bc-858c-5326c84f2f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5c8f9-d806-4510-817f-0a792542cbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1b0fb-5031-497b-8929-1f6e53f1985e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a75213-ca64-4d5f-8d74-8442e001845a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e7582-e89f-4fc7-9133-9a6e948896ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f566e4-514c-482e-bc54-5574851968bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def assess_variable_importance_all_regions(ds, gp_models, variable_names):\n",
    "    region_names = ds.names.values\n",
    "    region_indices = ds.region.values\n",
    "    \n",
    "    # Determine the number of rows and columns for the subplots based on the number of regions\n",
    "    num_regions = len(region_names)\n",
    "    cols = 3  # Assuming you'd like 3 columns\n",
    "    rows = math.ceil(num_regions / cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    \n",
    "    for i, region_index in enumerate(region_indices):\n",
    "        region_name = region_names[i]\n",
    "        \n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        \n",
    "        ax = axs[row, col]\n",
    "        \n",
    "        # Extract the ARD kernel length scales from the trained GP model for the region\n",
    "        length_scales = gp_models[region_name].kernel_.k1.k2.length_scale\n",
    "        \n",
    "        # Plot\n",
    "        ax.bar(variable_names, length_scales)\n",
    "        ax.set_title(region_name)\n",
    "        ax.set_xlabel('Variable Name')\n",
    "        ax.set_ylabel('Length Scale')\n",
    "        ax.tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better visibility\n",
    "        \n",
    "    # Remove empty subplots\n",
    "    for i in range(num_regions, rows*cols):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a364539-eb02-46a0-a1e2-50de4bca458a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "variable_names = ['pr', 'vpd', 'evspsbl', 'mrro', 'mrso', 'tran', 'lai', 'gpp']\n",
    "\n",
    "assess_variable_importance_all_regions(ds, gp_models, variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9838d6-ae01-4537-8873-3af90f298aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def assess_permutation_importance(ds, gp_models, variable_names, predictions, n_repeats=30):\n",
    "    \"\"\"\n",
    "    Assess the permutation importance of features for all regions.\n",
    "\n",
    "    Parameters:\n",
    "    - ds: Dataset containing region names, indices, and bgws values.\n",
    "    - gp_models: Dictionary containing trained GP models for each region.\n",
    "    - variable_names: List of variable names.\n",
    "    - predictions: Predicted values for each region.\n",
    "    - n_repeats: Number of times to permute a feature.\n",
    "\n",
    "    Returns:\n",
    "    - None. Plots the importance.\n",
    "    \"\"\"\n",
    "    \n",
    "    region_names = ds.names.values\n",
    "    region_indices = ds.region.values\n",
    "    y = ds.bgws.values  # Extracting target values from ds\n",
    "    \n",
    "    # Determine the number of rows and columns for the subplots based on the number of regions\n",
    "    num_regions = len(region_names)\n",
    "    cols = 3  # Assuming you'd like 3 columns\n",
    "    rows = math.ceil(num_regions / cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    \n",
    "    for region_name in region_names:\n",
    "        region_index = (ds['region'].values == region_name)\n",
    "\n",
    "        # Assuming ds has columns for each predictor variable and they are named consistently\n",
    "        # Extract all predictor variables for this region\n",
    "        X_region = ds[variable_names][region_index]\n",
    "\n",
    "        y_region = y[region_index]  \n",
    "        result = permutation_importance(gp_models[region_name], X_region, y_region, n_repeats=n_repeats)\n",
    "\n",
    "        # Sort variables by importance\n",
    "        sorted_idx = result.importances_mean.argsort()\n",
    "        \n",
    "        # Plot\n",
    "        ax.boxplot(result.importances[sorted_idx].T,\n",
    "                   vert=False, labels=np.array(variable_names)[sorted_idx])\n",
    "        ax.set_title(region_name)\n",
    "        ax.set_xlabel('Importance Score')\n",
    "        \n",
    "    # Remove empty subplots\n",
    "    for i in range(num_regions, rows*cols):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460149fd-d33d-4757-a98e-b5b2c54f8460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "assess_permutation_importance(ds, gp_models, variable_names, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed10b1-abad-4a67-8141-3058bbc83928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "mypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
