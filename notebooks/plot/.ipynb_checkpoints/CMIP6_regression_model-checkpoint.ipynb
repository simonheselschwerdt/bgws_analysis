{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7716ffb9-a564-44c7-87d4-2d4caeecdb1c",
   "metadata": {},
   "source": [
    "# CMIP6 Statistics and Plots\n",
    "\n",
    "**Following steps are included in this script:**\n",
    "\n",
    "1. Load netCDF files\n",
    "2. Compute statistics\n",
    "3. Plot statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5a4f3-40bb-459a-88f0-f180c8b3ee8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854af429-43a1-489d-8513-dca8c1466663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== Packages ==========\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import dask\n",
    "import matplotlib.cm\n",
    "from matplotlib import rcParams\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from cftime import DatetimeNoLeap\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams[\"mathtext.default\"] = 'regular'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5daa16-4811-4773-b98d-6b69626ff6cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb57b43-bbbe-414b-8075-8cda5d1ea4ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9326b-048c-48c2-84e0-5af87d06a120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Helper function to open the dataset ========\n",
    "def open_dataset(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ca849-c1f4-431c-ae0f-e6a3fb6ba92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to open and merge datasets\n",
    "def open_and_merge_datasets(folder, model, experiment_id, variables):\n",
    "    filepaths = []\n",
    "    for var in variables:\n",
    "        path = f'../../data/CMIP6/{experiment_id}/{folder}/{var}'\n",
    "        fp = glob.glob(os.path.join(path, f'CMIP.{model}.{experiment_id}.{var}_regridded.nc'))\n",
    "        if fp:\n",
    "            filepaths.append(fp[0])\n",
    "        else:\n",
    "            #print(f\"No file found for variable '{var}' in model '{model}'.\")\n",
    "            print(fp)\n",
    "\n",
    "    datasets = [xr.open_dataset(fp) for fp in filepaths]\n",
    "    ds = xr.merge(datasets)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9148cef-f033-40ac-aeef-dec5ccdb277d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0755d44-865c-439e-b702-f2ccb30e1010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_var(ds_dict, var):\n",
    "    for name, ds in ds_dict.items():\n",
    "        ds_dict[name] = ds.drop(var)\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc153f-499b-47e8-94a0-d21c9e11e507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_period(ds_dict, start_year=None, end_year=None, period=None):\n",
    "    '''\n",
    "    Helper function to select periods.\n",
    "    \n",
    "    Parameters:\n",
    "    ds_dict (dict): Dictionary with xarray datasets.\n",
    "    start_year (int): The start year of the period.\n",
    "    end_year (int): The end year of the period.\n",
    "    period (int, list, str): Single month (int), list of months (list), multiple seasons (str) to select.\n",
    "    '''\n",
    "    # Define season to month mapping for northern hemisphere\n",
    "    seasons_to_months = {\n",
    "        'nh_winter': [12, 1, 2],\n",
    "        'nh_spring': [3, 4, 5],\n",
    "        'nh_summer': [6, 7, 8],\n",
    "        'nh_fall': [9, 10, 11]\n",
    "    }\n",
    "\n",
    "    months = []\n",
    "\n",
    "    # If a single month is provided as an integer, convert it to a list\n",
    "    if isinstance(period, int):\n",
    "        months = [period]\n",
    "    elif isinstance(period, str):\n",
    "        # Check if the input is a single season or multiple seasons\n",
    "        if 'and' in period:\n",
    "            # Split the string into individual seasons\n",
    "            seasons = period.lower().split('and')\n",
    "            # Extend the months list with months of each season\n",
    "            for season in seasons:\n",
    "                season = season.strip()  # Remove leading/trailing whitespace\n",
    "                months.extend(seasons_to_months.get(season, []))\n",
    "        else:\n",
    "            # If a season is provided as a string, map it to the corresponding list of months\n",
    "            months = seasons_to_months.get(period.lower(), [])\n",
    "    elif isinstance(period, list):\n",
    "        # If a list is provided, use it directly\n",
    "        months = period\n",
    "    else:\n",
    "        raise ValueError(\"Period must be an integer, a string representing a single season, \"\n",
    "                         \"a string with multiple seasons separated by 'and', or a list of integers.\")\n",
    "\n",
    "    for k, ds in ds_dict.items():\n",
    "        # Select the time slice based on start and end year\n",
    "        if start_year and end_year:\n",
    "            start_date = f'{start_year}-01-01'\n",
    "            end_date = f'{end_year}-12-31'\n",
    "            ds = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # If months are specified, select those months\n",
    "        if months:\n",
    "            # This creates a boolean mask that is True for the selected months\n",
    "            month_mask = DataArray(ds['time.month'].isin(months), coords=ds['time'].coords)\n",
    "            ds = ds.where(month_mask, drop=True)\n",
    "\n",
    "        # Update the dictionary with the selected data\n",
    "        ds_dict[k] = ds\n",
    "    \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2c4af-d2f1-45fb-b6cc-67e938461c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_period(ds_dict, start_year=None, end_year=None, period=None, yearly_sum=False):\n",
    "    '''\n",
    "    Helper function to select periods and optionally compute yearly sums.\n",
    "    \n",
    "    Parameters:\n",
    "    ds_dict (dict): Dictionary with xarray datasets.\n",
    "    start_year (int): The start year of the period.\n",
    "    end_year (int): The end year of the period.\n",
    "    period (int, list, str): Single month (int), list of months (list), multiple seasons (str) to select.\n",
    "    yearly_sum (bool): If True, compute the yearly sum over the selected period.\n",
    "    '''\n",
    "    # Define season to month mapping for northern hemisphere\n",
    "    seasons_to_months = {\n",
    "        'nh_winter': [12, 1, 2],\n",
    "        'nh_spring': [3, 4, 5],\n",
    "        'nh_summer': [6, 7, 8],\n",
    "        'nh_fall': [9, 10, 11]\n",
    "    }\n",
    "\n",
    "    months = []\n",
    "\n",
    "    # If a single month is provided as an integer, convert it to a list\n",
    "    if isinstance(period, int):\n",
    "        months = [period]\n",
    "    elif isinstance(period, str):\n",
    "        # Check if the input is a single season or multiple seasons\n",
    "        if 'and' in period:\n",
    "            # Split the string into individual seasons\n",
    "            seasons = period.lower().split('and')\n",
    "            # Extend the months list with months of each season\n",
    "            for season in seasons:\n",
    "                season = season.strip()  # Remove leading/trailing whitespace\n",
    "                months.extend(seasons_to_months.get(season, []))\n",
    "        else:\n",
    "            # If a season is provided as a string, map it to the corresponding list of months\n",
    "            months = seasons_to_months.get(period.lower(), [])\n",
    "    elif isinstance(period, list):\n",
    "        # If a list is provided, use it directly\n",
    "        months = period\n",
    "    else:\n",
    "        raise ValueError(\"Period must be an integer, a string representing a single season, \"\n",
    "                         \"a string with multiple seasons separated by 'and', or a list of integers.\")\n",
    "\n",
    "    for k, ds in ds_dict.items():\n",
    "        # Select the time slice based on start and end year\n",
    "        if start_year and end_year:\n",
    "            start_date = f'{start_year}-01-01'\n",
    "            end_date = f'{end_year}-12-31'\n",
    "            ds = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # If months are specified, select those months\n",
    "        if months:\n",
    "            # This creates a boolean mask that is True for the selected months\n",
    "            month_mask = DataArray(ds['time.month'].isin(months), coords=ds['time'].coords)\n",
    "            ds = ds.where(month_mask, drop=True)\n",
    "\n",
    "        # If yearly_sum is True, sum over 'time' dimension to get yearly sum\n",
    "        if yearly_sum:\n",
    "            ds = ds.resample(time='AS').sum(dim='time')\n",
    "\n",
    "        # Update the dictionary with the selected data\n",
    "        ds_dict[k] = ds\n",
    "    \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402da3c7-b918-4a5c-a00e-d8b6004d9893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_period(ds_dict, start_year=None, end_year=None, period=None, yearly_sum=False):\n",
    "    '''\n",
    "    Helper function to select periods and optionally compute yearly sums.\n",
    "    \n",
    "    Parameters:\n",
    "    ds_dict (dict): Dictionary with xarray datasets.\n",
    "    start_year (int): The start year of the period.\n",
    "    end_year (int): The end year of the period.\n",
    "    period (int, list, str, None): Single month (int), list of months (list), multiple seasons (str) to select,\n",
    "                                   or None to not select any specific period.\n",
    "    yearly_sum (bool): If True, compute the yearly sum over the selected period.\n",
    "    '''\n",
    "    # Define season to month mapping for northern hemisphere\n",
    "    seasons_to_months = {\n",
    "        'nh_winter': [12, 1, 2],\n",
    "        'nh_spring': [3, 4, 5],\n",
    "        'nh_summer': [6, 7, 8],\n",
    "        'nh_fall': [9, 10, 11]\n",
    "    }\n",
    "\n",
    "    months = []\n",
    "\n",
    "    # If no specific period is selected, all data will be used.\n",
    "    if period is None:\n",
    "        months = None\n",
    "    elif isinstance(period, int):\n",
    "        months = [period]\n",
    "    elif isinstance(period, str):\n",
    "        # Check if the input is a single season or multiple seasons\n",
    "        if 'and' in period:\n",
    "            seasons = period.lower().split('and')\n",
    "            for season in seasons:\n",
    "                season = season.strip()\n",
    "                months.extend(seasons_to_months.get(season, []))\n",
    "        else:\n",
    "            months = seasons_to_months.get(period.lower(), [])\n",
    "    elif isinstance(period, list):\n",
    "        months = period\n",
    "    else:\n",
    "        raise ValueError(\"Period must be None, an integer, a string representing a single season, \"\n",
    "                         \"a string with multiple seasons separated by 'and', or a list of integers.\")\n",
    "\n",
    "    for k, ds in ds_dict.items():\n",
    "        if start_year and end_year:\n",
    "            start_date = f'{start_year}-01-01'\n",
    "            end_date = f'{end_year}-12-31'\n",
    "            ds = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # If months are specified, select those months\n",
    "        if months is not None:\n",
    "            month_mask = DataArray(ds['time.month'].isin(months), coords=ds['time'].coords)\n",
    "            ds = ds.where(month_mask, drop=True)\n",
    "\n",
    "        # If yearly_sum is True, sum over 'time' dimension to get yearly sum\n",
    "        if yearly_sum:\n",
    "            ds = ds.resample(time='AS').sum(dim='time')\n",
    "\n",
    "        ds_dict[k] = ds\n",
    "\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a690d-ac2f-4875-96dc-66e8424a4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Standardize ========\n",
    "def standardize(ds_dict):\n",
    "    '''\n",
    "    Helper function to standardize datasets of a dictionary\n",
    "    '''\n",
    "    ds_dict_stand = {}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        ds_stand = (ds - ds.mean()) / ds.std()\n",
    "\n",
    "        # Preserve variable attributes from the original dataset\n",
    "        for var in ds.variables:\n",
    "            if var in ds_stand.variables:\n",
    "                ds_stand[var].attrs = ds[var].attrs\n",
    "\n",
    "        ds_stand.attrs = attrs\n",
    "        ds_dict_stand[name] = ds_stand\n",
    "        \n",
    "    return ds_dict_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090c127-204b-4d69-9e33-5769b28e4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_args_and_get_info(ds_dict, variable):\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if not isinstance(variable, str):\n",
    "        raise TypeError('variable must be a string.')\n",
    "        \n",
    "    # Dictionary to store plot titles for each statistic\n",
    "    titles = {\"mean\": \"Mean\", \"std\": \"Standard deviation of yearly means\", \"min\": \"Minimum\", \"max\": \"Maximum\", \"median\": \"Median\", \"time\": \"Time\", \"space\": \"Space\"}\n",
    "    freq = {\"mon\": \"Monthly\"}\n",
    "    \n",
    "    long_name = {\n",
    "        'Precipitation': 'Precipitation',\n",
    "        'Total Runoff': 'Total Runoff',\n",
    "        'Vapor Pressure Deficit': 'Vapor Pressure Deficit',\n",
    "        'Evaporation Including Sublimation and Transpiration': 'Evapotranspiration',\n",
    "        'Transpiration': 'Transpiration',\n",
    "        'Leaf Area Index': 'Leaf Area Index',\n",
    "        'Carbon Mass Flux out of Atmosphere Due to Gross Primary Production on Land [kgC m-2 s-1]': 'Gross Primary Production',\n",
    "        'Total Liquid Soil Moisture Content of 1 m Column': '1 m Soil Moisture',\n",
    "        'Total Liquid Soil Moisture Content of 2 m Column': '2 m Soil Moisture',\n",
    "        'Runoff - Precipitation': 'Runoff - Precipitation',\n",
    "        'Transpiration - Precipitation': 'Transpiration - Precipitation',\n",
    "        '(Runoff + Transpiration) - Precipitation':  '(Runoff + Transpiration) - Precipitation',\n",
    "        'ET - Precipitation':  'ET - Precipitation', \n",
    "        'Negative Runoff': 'Negative Runoff',\n",
    "    }\n",
    "   \n",
    "    # Data information\n",
    "    var_long_name = ds_dict[list(ds_dict.keys())[0]][variable].long_name\n",
    "    period = f\"{ds_dict[list(ds_dict.keys())[0]].attrs['period'][0]}-{ds_dict[list(ds_dict.keys())[0]].attrs['period'][1]}\"\n",
    "    experiment_id =  ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    unit = ds_dict[list(ds_dict.keys())[0]][variable].units\n",
    "    statistic_dim = ds_dict[list(ds_dict.keys())[0]].statistic_dimension\n",
    "    statistic = ds_dict[list(ds_dict.keys())[0]].attrs['statistic']\n",
    "    frequency = freq[ds_dict[list(ds_dict.keys())[0]].frequency]\n",
    "\n",
    "    return var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e4851-532d-4dd6-af51-15e12b7761f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regionmask\n",
    "\n",
    "def apply_region_mask(ds_dict):\n",
    "    \"\"\"\n",
    "    Applies the AR6 land region mask to datasets in the provided dictionary and adds a region dimension.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary where keys are the same as in the input dictionary,\n",
    "              and each value is an xarray Dataset with a region dimension added to each variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    land_regions = regionmask.defined_regions.ar6.land\n",
    "    ds_masked_dict = {}\n",
    "    \n",
    "    for ds_name, ds in ds_dict.items():\n",
    "        ds_out = xr.Dataset()  # Initiate an empty Dataset for the masked data\n",
    "        \n",
    "        # Get attributes\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        for var in ds:\n",
    "            # Get the binary mask\n",
    "            mask = land_regions.mask_3D(ds[var])\n",
    "            \n",
    "            var_attrs = ds[var].attrs\n",
    "\n",
    "            # Multiply the original data with the mask to get the masked data\n",
    "            masked_var = ds[var] * mask\n",
    "\n",
    "            # Replace 0s with NaNs, if desired\n",
    "            masked_var = masked_var.where(masked_var != 0)\n",
    "\n",
    "            # Add the masked variable to the output Dataset\n",
    "            ds_out[var] = masked_var\n",
    "            \n",
    "            ds_out[var].attrs = var_attrs\n",
    "            \n",
    "        # Add the attributes\n",
    "        ds_out.attrs = attrs\n",
    "\n",
    "        # Add the Dataset to the output dictionary\n",
    "        ds_masked_dict[ds_name] = ds_out\n",
    "\n",
    "    return ds_masked_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647b6ec-2204-4764-b325-187ddfa48751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_change(ds_dict_hist_mean, ds_dict_ssp370_mean, relative_change=False):\n",
    "    ds_dict_change = {}\n",
    "\n",
    "    for name, ds in ds_dict_hist_mean.items():\n",
    "        \n",
    "        # Pre-Filter to keep only variables present in both datasets\n",
    "        common_vars = [var for var in ds.variables if var in ds_dict_ssp370_mean[name].variables]\n",
    "        ds = ds.drop_vars([var for var in ds.variables if var not in common_vars])\n",
    "        ds_dict_ssp370_mean[name] = ds_dict_ssp370_mean[name].drop_vars([var for var in ds_dict_ssp370_mean[name].variables if var not in common_vars])\n",
    "        \n",
    "        # Compute either absolute or relative change\n",
    "        if relative_change:\n",
    "            # Add a constant to the denominator to avoid division by zero\n",
    "            epsilon = 1\n",
    "            ds_nonzero = ds.where(ds != 0, epsilon)\n",
    "            \n",
    "            ds_dict_change[name] = ((ds_dict_ssp370_mean[name] - ds) / ds_nonzero) * 100\n",
    "            \n",
    "        else:\n",
    "            ds_dict_change[name] = ds_dict_ssp370_mean[name] - ds\n",
    "            \n",
    "        ds_dict_change[name].attrs = {'period': 'SSP370 - Historical',\n",
    "                                      'statistic': ds_dict_ssp370_mean[name].statistic,\n",
    "                                      'statistic_dimension':  ds_dict_ssp370_mean[name].statistic_dimension,\n",
    "                                      'experiment_id': 'ssp370-historical',\n",
    "                                      'source_id': ds_dict_ssp370_mean[name].source_id\n",
    "                                    }\n",
    "        \n",
    "        for variables in ds:\n",
    "            ds_dict_change[name][variables].attrs = ds_dict_ssp370_mean[name][variables].attrs\n",
    "            if relative_change:\n",
    "                ds_dict_change[name][variables].attrs['units_rel'] = '%'\n",
    "                ds_dict_change[name].attrs['change'] = 'Relative Change'\n",
    "            else:\n",
    "                ds_dict_change[name].attrs['change'] = 'Absolute Change'\n",
    "             \n",
    "    return ds_dict_change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfd1d3-d787-4484-aa0e-fbedcd496cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_ensemble(ds_dict_change, metric='mean'):\n",
    "    for key in ['Ensemble mean', 'Ensemble median']:\n",
    "        if key in ds_dict_change:\n",
    "            ds_dict_change.pop(key)\n",
    "\n",
    "    # Drop 'member_id' coordinate if it exists in any of the datasets\n",
    "    for ds_key in ds_dict_change:\n",
    "        if 'member_id' in ds_dict_change[ds_key].coords:\n",
    "            ds_dict_change[ds_key] = ds_dict_change[ds_key].drop('member_id')\n",
    "\n",
    "    combined = xr.concat(ds_dict_change.values(), dim='ensemble')\n",
    "    ds_dict_change[f'Ensemble {metric}'] = getattr(combined, metric)(dim='ensemble')\n",
    "    \n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a2736-eb0f-466e-b547-4d293e8a5af5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2f45a-7c54-4e4b-a62d-c43186adb78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_spatial_mean(ds_dict):\n",
    "    ds_dict_mean = {}\n",
    "    for key, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        for var in list(ds.data_vars.keys()):\n",
    "            var_attrs = ds[var].attrs\n",
    "            \n",
    "            ds_dict_mean[key][var] = ds.mean(['lon', 'lat'])\n",
    "            ds_dict_mean[key][var].attrs = var_attrs\n",
    "        \n",
    "        ds_dict_mean[key].attrs = attrs\n",
    "        \n",
    "    return ds_dict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766cd9d-393f-4db9-b0a3-744388ad88c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic_single(ds, statistic, dimension, yearly_mean=True):\n",
    "    if dimension == \"time\":\n",
    "        stat_ds = getattr(ds, statistic)(\"time\", keep_attrs=True, skipna=True)\n",
    "        stat_ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "    if dimension == \"space\":\n",
    "        # Assign the period attribute before grouping by year\n",
    "        ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "        if yearly_mean:\n",
    "            ds = ds.groupby('time.year').mean('time', keep_attrs=True, skipna=True)\n",
    "            ds.attrs['mean'] = 'yearly mean'\n",
    "            \n",
    "        \n",
    "        #get the weights, apply on data, and compute statistic\n",
    "        weights = np.cos(np.deg2rad(ds.lat))\n",
    "        weights.name = \"weights\"\n",
    "        ds_weighted = ds.weighted(weights)\n",
    "        stat_ds = getattr(ds_weighted, statistic)((\"lon\", \"lat\"), keep_attrs=True, skipna=True)\n",
    "    \n",
    "    stat_ds.attrs['statistic'] = statistic\n",
    "    stat_ds.attrs['statistic_dimension'] = dimension\n",
    "\n",
    "    return stat_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadad33e-375e-4f6f-94b1-918dce7e2f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic(ds_dict, statistic, dimension, start_year=None, end_year=None, yearly_mean=True):\n",
    "    \"\"\"\n",
    "    Computes the specified statistic for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        statistic (str): The statistic to compute, which can be one of 'mean', 'std', 'min', 'var', or 'median'.\n",
    "        dimension (str): The dimension to compute over, which can be 'time' or 'space'.\n",
    "        start_year (str, optional): The start year of the period to compute the statistic over.\n",
    "        end_year (str, optional): The end year of the period to compute the statistic over.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with computed statistic for each dataset.\n",
    "    \"\"\"\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if statistic not in [\"mean\", \"std\", \"min\", \"max\", \"var\", \"median\"]:\n",
    "        raise ValueError(f\"Invalid statistic '{statistic}' specified.\")\n",
    "    if dimension not in [\"time\", \"space\"]:\n",
    "        raise ValueError(f\"Invalid dimension '{dimension}' specified.\")\n",
    "\n",
    "    # Select period\n",
    "    if start_year is not None and end_year is not None:\n",
    "        select_period(ds_dict, start_year=start_year, end_year=end_year)\n",
    "        \n",
    "        \n",
    "    # Use multiprocessing to compute the statistic for each dataset in parallel\n",
    "    with mp.Pool() as pool:\n",
    "        results = pool.starmap(compute_statistic_single, [(ds, statistic, dimension, yearly_mean) for ds in ds_dict.values()])\n",
    "\n",
    "    return dict(zip(ds_dict.keys(), results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d176a36-95a5-4a18-b46f-a311e49243f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def precompute_metrics(ds_dict, variables, metrics=['pearson']):\n",
    "    # Initialize the results dictionary\n",
    "    results_dict = {metric: {} for metric in metrics}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        # Create a DataFrame with all the variables\n",
    "        df = pd.DataFrame({var: ds[var].values.flatten() for var in variables})\n",
    "        \n",
    "        # Define all pairs of variables\n",
    "        pairs = list(permutations(variables, 2))  # <-- Change here\n",
    "        args = [(df, var1, var2, metrics) for var1, var2 in pairs]\n",
    "\n",
    "        # Use a multiprocessing pool to compute the metrics for all pairs\n",
    "        with Pool() as p:\n",
    "            results = p.map(compute_metrics_for_pair, args)\n",
    "        \n",
    "        # Store the results in the results_dict\n",
    "        for var1, var2, metric_dict in results:\n",
    "            for metric, value in metric_dict.items():\n",
    "                # Ensure the keys exist in the dictionary\n",
    "                results_dict[metric].setdefault(name, {}).setdefault(f'{var1}_{var2}', value)\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9381cb-6944-4552-b6c7-aac1b6a3c6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_stats(ds_dict):\n",
    "    \"\"\"\n",
    "    Compute yearly mean of each variable in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds_dict (dict): The input dictionary of xarray.Dataset.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are the dataset names and the values are another dictionary.\n",
    "          This inner dictionary has keys as variable names and values as DataArray of yearly means.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    for model, ds in ds_dict.items():\n",
    "        # Compute the yearly mean\n",
    "        yearly_ds = ds.resample(time='1Y').mean()\n",
    "\n",
    "        stats[model] = {}\n",
    "        for var in yearly_ds.data_vars:\n",
    "            # Compute the spatial mean\n",
    "            spatial_mean = yearly_ds[var].mean(dim=['lat', 'lon'])\n",
    "            \n",
    "            # Store the yearly mean values\n",
    "            stats[model][var] = spatial_mean\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43015f97-e54e-4f20-aa0c-3e10f0c8d748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_yearly_means(ds_dict):\n",
    "    yearly_means_dict = {}\n",
    "\n",
    "    # For each dataset, compute the yearly mean over the 'time', 'lat', and 'lon' dimensions\n",
    "    for name, ds in ds_dict.items():  \n",
    "        ds_yearly = ds.groupby('time.year').mean('time')    \n",
    "        \n",
    "        yearly_means_dict[name] = ds_yearly\n",
    "\n",
    "    return yearly_means_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbbc45-b47c-461b-99db-d20d9fa0625d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_yearly_regional_means(ds_dict_region):\n",
    "    yearly_means_dict = {}\n",
    "\n",
    "    # For each dataset, compute the yearly mean over the 'time', 'lat', and 'lon' dimensions\n",
    "    for region, ds_dict in ds_dict_region.items():\n",
    "        yearly_means_dict[region] = {}\n",
    "        for ds_name, ds in ds_dict.items():\n",
    "            # Compute the yearly mean\n",
    "            ds_yearly = ds.groupby('time.year').mean('time')\n",
    "            \n",
    "            # Create weights\n",
    "            weights = np.cos(np.deg2rad(ds.lat))\n",
    "            # Apply the weights and calculate the spatial mean\n",
    "            ds_weighted = ds_yearly.weighted(weights)\n",
    "            yearly_means_dict[region][ds_name] = ds_weighted.mean(('lat', 'lon'))\n",
    "\n",
    "    return yearly_means_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36692801-5a4b-4e4d-bd3e-268e3de553d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_change(ds_dict_hist, ds_dict_fut, var_rel_change=None):\n",
    "    ds_dict_change = {}\n",
    "    \n",
    "    if var_rel_change == \"ALL\":\n",
    "        var_rel_change = list(ds_dict_hist[next(iter(ds_dict_hist))].data_vars.keys())\n",
    "    elif var_rel_change is None:\n",
    "        var_rel_change = []\n",
    "\n",
    "    for name, ds in ds_dict_hist.items():\n",
    "        if name in ds_dict_fut:\n",
    "            ds_f = ds_dict_fut[name]\n",
    "            fut_scenario = ds_f.experiment_id\n",
    "            \n",
    "            change_ds = xr.Dataset()\n",
    "            \n",
    "            for variable in ds.data_vars:\n",
    "                if variable in var_rel_change:\n",
    "                    # Compute relative change only where ds is not 0\n",
    "                    rel_change = ds[variable].where(ds[variable] != 0)\n",
    "                    rel_change = ((ds_f[variable] - rel_change) / abs(rel_change)) * 100\n",
    "                    \n",
    "                    # Where ds was 0, set the corresponding relative change to np.nan\n",
    "                    rel_change = rel_change.where(ds[variable] != 0)\n",
    "                    \n",
    "                    change_ds[variable] = rel_change\n",
    "                    \n",
    "                else:\n",
    "                    change_ds[variable] = ds_f[variable] - ds[variable]\n",
    "                    \n",
    "                change_ds[variable].attrs = ds_f[variable].attrs\n",
    "\n",
    "            ds_dict_change[name] = change_ds\n",
    "\n",
    "            ds_dict_change[name].attrs = {\n",
    "                'period': f'Change {fut_scenario} - Historical',\n",
    "                'statistic': ds_f.statistic,\n",
    "                'statistic_dimension': ds_f.statistic_dimension,\n",
    "                'experiment_id': fut_scenario.lower() + '-historical',\n",
    "                'source_id': ds_f.source_id,\n",
    "                'frequency': ds_f.frequency,\n",
    "                'change': 'Mixed Change',\n",
    "            }\n",
    "\n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b71e4e-b89b-4169-bac4-97f010c2f2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_spatial_mean(ds_dict):\n",
    "    ds_dict_mean = {}\n",
    "    \n",
    "    for key, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        # Initialize a new Dataset for this key\n",
    "        ds_dict_mean[key] = xr.Dataset()\n",
    "        \n",
    "        for var in list(ds.data_vars.keys()):\n",
    "            var_attrs = ds[var].attrs\n",
    "            \n",
    "            ds_dict_mean[key][var] = ds[var].mean(['lon', 'lat'])\n",
    "            ds_dict_mean[key][var].attrs = var_attrs\n",
    "        \n",
    "        ds_dict_mean[key].attrs = attrs\n",
    "        \n",
    "    return ds_dict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354754c-b13c-4650-b31b-7ae9e9e424ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_bgws(ds_dict):\n",
    "\n",
    "    for model, ds in ds_dict.items():\n",
    "        bgws = (ds['mrro']-ds['tran'])/ds['pr']\n",
    "\n",
    "        # Replace infinite values with NaN\n",
    "        bgws = xr.where(np.isinf(bgws), float('nan'), bgws)\n",
    "\n",
    "        # Set all values above 2 and below -2 to NaN\n",
    "        bgws = xr.where(bgws > 2, float('nan'), bgws)\n",
    "        bgws = xr.where(bgws < -2, float('nan'), bgws)\n",
    "\n",
    "        ds_dict[model]['bgws'] = bgws\n",
    "        ds_dict[model]['bgws'].attrs = {'long_name': 'Blue Green Water Share',\n",
    "                             'units': ''}\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c6728-aa37-48ed-88ee-2bca84fa1521",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39238c09-3874-41ae-a859-a782bb16917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_correlations(yearly_correlations, variable_pairs, target_variable):\n",
    "    \"\"\"\n",
    "    Plots the time series of correlations for each variable pair that includes the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    yearly_correlations (dict): The output from calculate_yearly_correlations.\n",
    "    variable_pairs (list of tuples): The pairs of variables that the correlations were calculated for.\n",
    "    target_variable (str): The variable that must be included in a pair for it to be plotted.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the number of plots\n",
    "    n_plots = sum([var1 == target_variable or var2 == target_variable for var1, var2 in variable_pairs])\n",
    "\n",
    "    # Calculate the dimensions of the grid of subplots\n",
    "    grid_size = math.ceil(math.sqrt(n_plots))\n",
    "    \n",
    "    # Create the figure\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15), sharex=True, sharey=True)\n",
    "\n",
    "    # Flatten the axes\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Create an index for the current plot\n",
    "    i_plot = 0\n",
    "\n",
    "    # Prepare a list to store handles and labels for the legend\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for var1, var2 in variable_pairs:\n",
    "        if var1 == target_variable or var2 == target_variable:\n",
    "            # Select the current axes\n",
    "            ax = axs[i_plot]\n",
    "            \n",
    "            # Construct the correlation variable name\n",
    "            corr_var = f'{var1}-{var2}'\n",
    "\n",
    "            # Prepare a list to store correlations of all models\n",
    "            all_corrs = []\n",
    "\n",
    "            for name, ds in yearly_correlations.items():\n",
    "                # Check if this variable exists in the Dataset\n",
    "                if corr_var in ds:\n",
    "                    # Plot the time series of the correlation\n",
    "                    line, = ax.plot(ds['time'], ds[corr_var], label=name)\n",
    "\n",
    "                    # Append to all_corrs\n",
    "                    all_corrs.append(ds[corr_var])\n",
    "\n",
    "                    # Append to handles and labels if not already present\n",
    "                    if name not in labels:\n",
    "                        handles.append(line)\n",
    "                        labels.append(name)\n",
    "\n",
    "            # Compute the mean correlation across all models\n",
    "            mean_corr = xr.concat(all_corrs, dim='model').mean(dim='model')\n",
    "            mean_line, = ax.plot(mean_corr['time'], mean_corr, color='black', linestyle='--')\n",
    "\n",
    "            ax.set_xlabel('Year')\n",
    "            ax.set_ylabel('Correlation')\n",
    "            ax.set_title(f'{var1} vs {var2}')\n",
    "            ax.grid(True)\n",
    "\n",
    "            # Increment the plot index\n",
    "            i_plot += 1\n",
    "\n",
    "    # Add 'Mean' to the legend\n",
    "    handles.append(mean_line)\n",
    "    labels.append('Mean')\n",
    "\n",
    "    # Show the figure with a legend\n",
    "    fig.legend(handles, labels, loc='center right', bbox_to_anchor=(1.1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10db210-8757-4858-ae1e-4f89e7949440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_mean_correlations(correlations_hist, correlations_ssp370, variable_pairs, target_variable, scale_axis=False, variable_captions=None):\n",
    "    \"\"\"\n",
    "    Plots the mean correlations for each variable pair that includes the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    correlations_hist (dict): The output from calculate_correlations for the historical period.\n",
    "    correlations_ssp370 (dict): The output from calculate_correlations for the SSP370 scenario.\n",
    "    variable_pairs (list of tuples): The pairs of variables that the correlations were calculated for.\n",
    "    target_variable (str): The variable that must be included in a pair for it to be plotted.\n",
    "    scale_axis (bool): Whether to scale the y-axis according to metric value ranges. Default is False.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get info\n",
    "    metric = correlations_hist[list(correlations_hist.keys())[0]].Metric\n",
    "    metric_sign = correlations_hist[list(correlations_hist.keys())[0]].Metric_sign\n",
    "    \n",
    "    # Filter variable pairs\n",
    "    variable_pairs = [(var1, var2) for var1, var2 in variable_pairs if var1 == target_variable or var2 == target_variable]\n",
    "\n",
    "    # Calculate the number of plots and dimensions of the grid of subplots\n",
    "    n_plots = len(variable_pairs)\n",
    "    n_cols = min(n_plots, 3)  # Maximum 3 plots in a row\n",
    "    n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "    # Create the figure\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows), squeeze=False)\n",
    "    axs = axs.flatten()  # Flatten the axes array\n",
    "    \n",
    "    for i, (var1, var2) in enumerate(variable_pairs):\n",
    "        # Prepare lists to store yearly correlation values\n",
    "        yearly_corr_hist, yearly_corr_ssp370 = [], []\n",
    "\n",
    "        for name in correlations_hist.keys():\n",
    "            # Extract the yearly mean values for the historical period\n",
    "            yearly_corr_hist.append(correlations_hist[name][f'{var1}-{var2}'].values)\n",
    "\n",
    "            # Extract the yearly mean values for the SSP370 scenario\n",
    "            yearly_corr_ssp370.append(correlations_ssp370[name][f'{var1}-{var2}'].values)\n",
    "\n",
    "        # Compute the box plot positions\n",
    "        positions = np.arange(len(correlations_hist.keys()))\n",
    "\n",
    "        # Select the current axes\n",
    "        ax = axs[i]\n",
    "\n",
    "        # Define an offset for x-values to place box plots for different periods side by side\n",
    "        offset = 0.15\n",
    "\n",
    "        # Plot the box plots for the historical period\n",
    "        ax.boxplot(yearly_corr_hist, positions=positions-offset, widths=0.3, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='cornflowerblue'), medianprops=dict(color='black'), \n",
    "                   showfliers=True)\n",
    "\n",
    "        # Plot the box plots for the SSP370 scenario\n",
    "        ax.boxplot(yearly_corr_ssp370, positions=positions+offset, widths=0.3, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='sandybrown'), medianprops=dict(color='black'), \n",
    "                   showfliers=True)\n",
    "\n",
    "        # Set the x-ticks labels and the title\n",
    "        ax.set_ylabel(f'{metric_sign}')\n",
    "        ax.set_title(f'{variable_captions.get(var1, var1)} x {variable_captions.get(var2, var2)}', fontsize=9)\n",
    "        ax.set_xticks(positions)\n",
    "        ax.set_xticklabels(list(correlations_hist.keys()), rotation=90)\n",
    "        \n",
    "    fig.suptitle(f'Yearly {metric} for Historical (1985-2014) and SSP370 (2071-2100) Period', fontsize=12, y=1.0)\n",
    "    \n",
    "    # Set a legend\n",
    "    axs[0].legend([Patch(facecolor='cornflowerblue'), Patch(facecolor='sandybrown')], ['Historical', 'SSP370'])\n",
    "\n",
    "    # Handle empty subplots in case n_plots is less than n_rows * n_cols\n",
    "    for i in range(n_plots, n_rows*n_cols):\n",
    "        fig.delaxes(axs[i])\n",
    "    \n",
    "    # Handle y-axis scaling\n",
    "    if scale_axis:\n",
    "        for ax in axs:\n",
    "            ax.set_ylim([-1, 1] if metric != 'r2_lr' else [0, 1])\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    suffix = \"_scaled_axis\" if scale_axis else \"\"\n",
    "    filename = f\"{metric}_changes_{target_variable}{suffix}.png\"\n",
    "    \n",
    "    savepath = f'../../results/CMIP6/yearly_metrics_comparison'\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    filepath = os.path.join(savepath, filename)\n",
    "\n",
    "    fig.savefig(filepath, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555de63-1237-4ed3-b496-48f9f835eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_corr_change_plot(ds_dict, target_variable, full_var_names_and_unit, cmap='viridis', save_fig=False, file_format='png'):\n",
    "    \"\"\"\n",
    "    Plots a map of the specified statistic of the given variable for the Ensemble_mean dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        target_variable (str): The target variable to plot correlations with.\n",
    "        cmap (str): The name of the colormap to use for the plot. Default is 'viridis'.\n",
    "        save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "        file_format (str): The format of the saved figure. Default is 'png'.\n",
    "    \"\"\"\n",
    "    # Info\n",
    "    experiment_id = ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    metric = ds_dict[list(ds_dict.keys())[0]].Metric\n",
    "    metric_sign = ds_dict[list(ds_dict.keys())[0]].Metric_sign\n",
    "    means = ds_dict[list(ds_dict.keys())[0]].attrs['means']\n",
    "    target_var_long_name = full_var_names_and_unit[target_variable][0]\n",
    "    \n",
    "    # Create a figure\n",
    "    n_cols = 3  # Set number of columns to 3\n",
    "    n_rows = 3  # Set number of rows to 3\n",
    "    \n",
    "    fig = plt.figure(figsize=[12 * n_cols, 6.5 * n_rows])  # Start with a blank figure, without subplots\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Loop over datasets and plot the requested statistic\n",
    "    subplot_counter = 0\n",
    "\n",
    "    # Get the Ensemble_mean dataset\n",
    "    ensemble_ds = ds_dict.get(\"Ensemble_mean\", None)\n",
    "\n",
    "    if ensemble_ds is None:\n",
    "        print(\"Ensemble_mean dataset not found.\")\n",
    "        return None\n",
    "\n",
    "    for variable in ensemble_ds.variables:\n",
    "        if not (f'{target_variable} x ' in variable or f' x {target_variable}' in variable):\n",
    "            continue\n",
    "\n",
    "        # Add a new subplot with a cartopy projection\n",
    "        ax = fig.add_subplot(n_rows, n_cols, subplot_counter+1, projection=ccrs.Robinson())\n",
    "\n",
    "        data_to_plot = ensemble_ds[variable]\n",
    "        im = data_to_plot.plot(ax=ax, cmap=cmap, vmin = -1, vmax = 1, transform=ccrs.PlateCarree(), add_colorbar=False)  # Added a cartopy transform to the plot and cmap parameter\n",
    "        \n",
    "        if f'{target_variable} x ' in variable:\n",
    "            corr_var = variable.replace(f'{target_variable} x ', '')\n",
    "        elif f' x {target_variable}' in variable:\n",
    "            corr_var = variable.replace(f' x {target_variable}', '')\n",
    "        else:\n",
    "            continue\n",
    "        corr_var_long_name = full_var_names_and_unit[corr_var][0]\n",
    "        ax.set_title(f'{target_var_long_name} x {corr_var_long_name}', fontsize=18)  # Use the long names in the title\n",
    "        ax.coastlines()  # Adds lines around the continents\n",
    "\n",
    "        subplot_counter += 1\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # Add a common colorbar at the bottom of the plots\n",
    "    cbar = fig.colorbar(im, ax=fig.axes, orientation='horizontal', fraction=0.02, pad=0.04, aspect=75, shrink=0.4)\n",
    "    \n",
    "\n",
    "    # Set colorbar ticks\n",
    "    cbar.set_ticks([-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75])\n",
    "    \n",
    "    \n",
    "    # Set tick size\n",
    "    cbar.ax.tick_params(labelsize=20)  # Adjust size as needed\n",
    "    \n",
    "    # Set colorbar label\n",
    "    cbar.set_label(f'{metric_sign} change', size=26)  # Adjust size as needed\n",
    "    \n",
    "    # Set figure title with first and last year of dataset \n",
    "    if experiment_id == 'historical' or experiment_id == 'ssp370':\n",
    "        fig.suptitle(f\"{metric} ({experiment_id}) of Ensemble Mean for Variable Combinations with {target_var_long_name} ({means})\", fontsize=20, y=0.9)\n",
    "    elif experiment_id == 'ssp370-historical':\n",
    "        fig.suptitle(f\"{metric} Change ({experiment_id}) of Ensemble Mean for Variable Combinations with {target_var_long_name} ({means})\", fontsize=20, y=0.9)\n",
    "    \n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        if experiment_id == 'historical' or experiment_id == 'ssp370':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename = f'ensmean_{target_variable}_correlations.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "        elif experiment_id == 'ssp370-historical':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename =f'ensmean_{target_variable}_correlations_change.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c30d8-7d61-497f-9fd6-689ad3634fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_maps(ds_dict, target_variable_combination, cmap='viridis', save_fig=False, file_format='png'):\n",
    "    \"\"\"\n",
    "    Plots a map of the specified variable combination for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        target_variable_combination (str): The target variable combination to plot.\n",
    "        cmap (str): The name of the colormap to use for the plot. Default is 'viridis'.\n",
    "        save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "        file_format (str): The format of the saved figure. Default is 'png'.\n",
    "    \"\"\"\n",
    "    # Info\n",
    "    experiment_id = ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    metric = ds_dict[list(ds_dict.keys())[0]].Metric\n",
    "    metric_sign = ds_dict[list(ds_dict.keys())[0]].Metric_sign\n",
    "    means = ds_dict[list(ds_dict.keys())[0]].attrs['means']\n",
    "    \n",
    "    # Create a figure\n",
    "    n_datasets_with_var = sum([1 for ds in ds_dict.values() if target_variable_combination in ds.variables])\n",
    "    n_cols = 3  # Set number of columns to 3\n",
    "    n_rows = math.ceil(n_datasets_with_var / n_cols)  # Calculate rows\n",
    "    \n",
    "    fig = plt.figure(figsize=[12 * n_cols, 6.5 * n_rows])  # Start with a blank figure, without subplots\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Loop over datasets and plot the requested variable combination\n",
    "    subplot_counter = 0\n",
    "    for i, (name, ds) in enumerate(ds_dict.items()):\n",
    "        if target_variable_combination not in ds.variables:\n",
    "            continue\n",
    "\n",
    "        # Add a new subplot with a cartopy projection\n",
    "        ax = fig.add_subplot(n_rows, n_cols, subplot_counter+1, projection=ccrs.Robinson())\n",
    "        \n",
    "        data_to_plot = ds[target_variable_combination]\n",
    "        im = data_to_plot.plot(ax=ax, cmap=cmap, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "        ax.set_title(name, fontsize=18)\n",
    "        ax.coastlines()  # Adds lines around the continents\n",
    "\n",
    "        subplot_counter += 1\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # Add a common colorbar at the bottom of the plots\n",
    "    cbar = fig.colorbar(im, ax=fig.axes, orientation='horizontal', fraction=0.02, pad=0.04, aspect=75, shrink=0.4)\n",
    "    \n",
    "    # Set tick size\n",
    "    cbar.ax.tick_params(labelsize=14)  # Adjust size as needed\n",
    "    \n",
    "    # Set colorbar label\n",
    "    cbar.set_label(metric_sign, size=18)  # Adjust size as needed\n",
    "    \n",
    "    # Set figure title with first and last year of dataset \n",
    "    if experiment_id == 'historical' or 'ssp370':\n",
    "        fig.suptitle(f\"{metric} ({experiment_id}) for Variable Combination {target_variable_combination} ({means})\", fontsize=20, y=0.9)\n",
    "    elif experiment_id == 'ssp370-historical':\n",
    "        fig.suptitle(f\"{metric} Change ({experiment_id}) for Variable Combination {target_variable_combination} ({means})\", fontsize=20, y=0.9)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        if experiment_id == 'historical' or 'ssp370':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename = f'{target_variable_combination}_correlation.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "        elif experiment_id == 'ssp370-historical':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', 'comparison', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename =f'{target_variable_combination}_correlation_change.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173177e7-1327-46be-9952-4ba5d9d45155",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343c372-5fab-4276-9c97-171ecae7340b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "#variables=['tas', 'pr', 'vpd', 'evspsbl', 'mrro', 'lmrso_1m', 'lmrso_2m', 'tran', 'lai', 'gpp', 'EI', 'wue']\n",
    "variables=['tas', 'pr', 'vpd', 'evapo', 'mrro', 'mrso', 'tran', 'lai', 'gpp']\n",
    "folder='preprocessed'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# Create dictionary using a dictionary comprehension and Dask\n",
    "experiment_id = 'historical'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','GISS-E2-1-G','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','TaiESM1','UKESM1-0-LL']\n",
    "ds_dict_hist = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, variables) for model in source_id})[0]\n",
    "\n",
    "experiment_id = 'ssp370'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','GISS-E2-1-G','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','TaiESM1','UKESM1-0-LL']\n",
    "ds_dict_ssp370 = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, variables) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caaffb5-1821-4cee-800d-44a2bfe87dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363b461-3c5d-45f7-8e71-2bd3bb7f5153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def monthly_mean(ds, variable):\n",
    "    \"\"\"\n",
    "    Calculate the mean for each month across all years for a given variable in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The input dataset.\n",
    "    variable (str): The name of the variable to calculate the monthly mean for.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: A dataset containing the monthly mean for the specified variable.\n",
    "    \"\"\"\n",
    "    # Calculate the monthly mean\n",
    "    monthly_means = ds[variable].groupby('time.month').mean('time')\n",
    "\n",
    "    # Create a new dataset for the monthly means\n",
    "    monthly_mean_ds = xr.Dataset({variable: monthly_means})\n",
    "\n",
    "    # Copy attributes from the original dataset and variable\n",
    "    monthly_mean_ds.attrs = ds.attrs\n",
    "    monthly_mean_ds[variable].attrs = ds[variable].attrs\n",
    "    monthly_mean_ds[variable].attrs['description'] = f'Monthly mean of {variable}'\n",
    "\n",
    "    return monthly_mean_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2de85-49f5-4846-a017-d3f73014f155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_monthly_mean = monthly_mean(ds, 'lai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd0d7d-a840-43dd-b105-13983f9a828e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_monthly_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a883cb-56e1-4196-8ed5-4ba4dc0728ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_growing_season_length(ds):\n",
    "    # Assuming 'lai' is the key for Leaf Area Index in the dataset\n",
    "    lai = ds['lai']\n",
    "    \n",
    "    # Initialize an empty DataArray to store the growing season length with yearly time dimension\n",
    "    years = range(ds.time.dt.year.min().values, ds.time.dt.year.max().values + 1)\n",
    "    growing_season_length = xr.DataArray(\n",
    "        np.nan, \n",
    "        dims=('year', 'lat', 'lon'), \n",
    "        coords={'year': ds.time, 'lat': ds.lat, 'lon': ds.lon}\n",
    "    )\n",
    "\n",
    "    for year in years:\n",
    "        lai_yearly = lai.sel(time=str(year))\n",
    "\n",
    "        # Perform calculations for each grid cell\n",
    "        for lat in ds.lat:\n",
    "            for lon in ds.lon:\n",
    "                lai_ts = lai_yearly.sel(lat=lat, lon=lon, method='nearest')\n",
    "                \n",
    "                # Calculate monthly mean LAI\n",
    "                monthly_mean = lai_ts.groupby('time.month').mean('time')\n",
    "\n",
    "                # Compute percentage change including December to January\n",
    "                monthly_pct_change = ((monthly_mean - monthly_mean.roll(month=1)) / monthly_mean.roll(month=1)) * 100\n",
    "                monthly_pct_change[0] = ((monthly_mean[0] - monthly_mean[-1]) / monthly_mean[-1]) * 100\n",
    "\n",
    "                # Compute the midpoint value for the December to January transition\n",
    "                dec_to_jan_midpoint_lai = (monthly_mean[-1] + monthly_mean[0]) / 2\n",
    "                dec_to_jan_midpoint_pct_change = (monthly_pct_change[-1] + monthly_pct_change[0]) / 2\n",
    "\n",
    "                # Adjust growing season start and end detection\n",
    "                starts, ends = [], []\n",
    "                # Check if December ends a growing season and January starts a new one\n",
    "                if monthly_pct_change[-2] > 0 and monthly_pct_change[-1] < 0:\n",
    "                    ends.append(12)  # December ends a growing season\n",
    "                if monthly_pct_change[-1] <= 0 and monthly_pct_change[0] > 0:\n",
    "                    starts.append(1)  # January starts a growing season\n",
    "\n",
    "                #Check if January ends a growing season (wrap-around to the previous December)\n",
    "                if monthly_pct_change[0] < 0 and monthly_pct_change[-1] > 0:\n",
    "                    ends.append(1)  # January ends a growing season\n",
    "\n",
    "                # Now handle February to November for starts and ends\n",
    "                for i in range(1, len(monthly_pct_change) - 1):\n",
    "                    if monthly_pct_change[i-1] <= 0 and monthly_pct_change[i] > 0:\n",
    "                        starts.append(i + 1)\n",
    "                    elif monthly_pct_change[i-1] > 0 and monthly_pct_change[i] <= 0:\n",
    "                        ends.append(i + 1)\n",
    "\n",
    "                # Calculate the growing season length(s)\n",
    "                if len(starts) > 1:\n",
    "                    if any(end > starts[0] for end in ends):\n",
    "                        closest_end = min(filter(lambda end: end > starts[0], ends), key=lambda end: end - starts[0], default=None)\n",
    "                        growing_season_length = closest_end - starts[0]\n",
    "                    else:\n",
    "                        growing_season_length = min(ends) + (12 - starts[0])\n",
    "\n",
    "                    if any(end > starts[1] for end in ends):\n",
    "                        growing_season_length = growing_season_length + max(ends) - starts[1]\n",
    "                    else:\n",
    "                        growing_season_length = growing_season_length + min(ends) + (12 - starts[1])\n",
    "                elif len(starts) == 1:\n",
    "                    if starts[0] < ends[0]:\n",
    "                        growing_season_length = ends[0] - starts[0]\n",
    "                    elif starts[0] > ends[0]:\n",
    "                        growing_season_length = (12 - starts[0]) + ends[0]\n",
    "                \n",
    "                # Calculate the growing season length for this cell\n",
    "                growing_season_length_cell = None\n",
    "                \n",
    "                # Store the calculated value in the DataArray\n",
    "                growing_season_length.loc[dict(time=str(year), lat=lat, lon=lon)] = growing_season_length_cell\n",
    "\n",
    "    # Update dataset with new variable\n",
    "    ds['growing_season_length'] = growing_season_length\n",
    "    ds['growing_season_length'].attrs = {\n",
    "        'description': 'Length of the growing season in months',\n",
    "        'calculated_from': 'LAI'\n",
    "    }\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4768c-ab14-4662-86ce-60391100845d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_gs = calculate_growing_season_length(ds_monthly_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09774d15-e663-4ccc-be15-73dd2c6077be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_growing_season_length(ds):\n",
    "    lai = ds['lai']\n",
    "    \n",
    "    years = range(ds.time.dt.year.min().values, ds.time.dt.year.max().values + 1)\n",
    "    growing_season_length = xr.DataArray(\n",
    "        np.nan, \n",
    "        dims=('year', 'lat', 'lon'), \n",
    "        coords={'year': list(years), 'lat': ds.lat, 'lon': ds.lon}\n",
    "    )\n",
    "    \n",
    "    years = range(ds.time.dt.year.min().values, ds.time.dt.year.max().values + 1)\n",
    "    growing_season_length = xr.DataArray(\n",
    "        np.nan, \n",
    "        dims=('year', 'lat', 'lon'), \n",
    "        coords={'year': list(years), 'lat': ds.lat, 'lon': ds.lon}\n",
    "    )\n",
    "\n",
    "\n",
    "    for year in years:\n",
    "        lai_yearly = lai.sel(time=str(year))\n",
    "        for lat in ds.lat.values:\n",
    "            for lon in ds.lon.values:\n",
    "                lai_ts = lai_yearly.sel(lat=lat, lon=lon, method='nearest')\n",
    "                monthly_mean, monthly_pct_change = calculate_monthly_changes(lai_ts)\n",
    "                starts, ends = detect_season_starts_and_ends(monthly_pct_change)\n",
    "                length = calculate_season_length(starts, ends)\n",
    "                growing_season_length.loc[dict(year=year, lat=lat, lon=lon)] = length\n",
    "\n",
    "    ds['growing_season_length'] = growing_season_length\n",
    "    ds['growing_season_length'].attrs = {\n",
    "        'description': 'Length of the growing season in months',\n",
    "        'calculated_from': 'LAI'\n",
    "    }\n",
    "\n",
    "    return ds\n",
    "\n",
    "def calculate_monthly_changes(lai_ts):\n",
    "    monthly_mean = lai_ts.groupby('time.month').mean('time')\n",
    "    monthly_pct_change = ((monthly_mean - monthly_mean.roll(month=1)) / monthly_mean.roll(month=1)) * 100\n",
    "    monthly_pct_change[0] = ((monthly_mean[0] - monthly_mean[-1]) / monthly_mean[-1]) * 100\n",
    "    return monthly_mean, monthly_pct_change\n",
    "\n",
    "def detect_season_starts_and_ends(monthly_pct_change):\n",
    "    starts, ends = [], []\n",
    "    \n",
    "    # Check if December ends a growing season and January starts a new one\n",
    "    if monthly_pct_change[-2] > 0 and monthly_pct_change[-1] < 0:\n",
    "        ends.append(12)  # December ends a growing season\n",
    "    if monthly_pct_change[-1] <= 0 and monthly_pct_change[0] > 0:\n",
    "        starts.append(1)  # January starts a growing season\n",
    "\n",
    "    #Check if January ends a growing season (wrap-around to the previous December)\n",
    "    if monthly_pct_change[0] < 0 and monthly_pct_change[-1] > 0:\n",
    "        ends.append(1)  # January ends a growing season\n",
    "\n",
    "    # Now handle February to November for starts and ends\n",
    "    for i in range(1, len(monthly_pct_change) - 1):\n",
    "        if monthly_pct_change[i-1] <= 0 and monthly_pct_change[i] > 0:\n",
    "            starts.append(i + 1)\n",
    "        elif monthly_pct_change[i-1] > 0 and monthly_pct_change[i] <= 0:\n",
    "            ends.append(i + 1)\n",
    "            \n",
    "    return starts, ends\n",
    "\n",
    "def calculate_season_length(starts, ends):\n",
    "    growing_season_length = 0\n",
    "    \n",
    "    # Calculate the growing season length(s)\n",
    "    if len(starts) > 1:\n",
    "        if any(end > starts[0] for end in ends):\n",
    "            closest_end = min(filter(lambda end: end > starts[0], ends), key=lambda end: end - starts[0], default=None)\n",
    "            growing_season_length = closest_end - starts[0]\n",
    "        else:\n",
    "            growing_season_length = min(ends) + (12 - starts[0])\n",
    "\n",
    "        if any(end > starts[1] for end in ends):\n",
    "            growing_season_length = growing_season_length + max(ends) - starts[1]\n",
    "        else:\n",
    "            growing_season_length = growing_season_length + min(ends) + (12 - starts[1])\n",
    "    elif len(starts) == 1:\n",
    "        if starts[0] < ends[0]:\n",
    "            growing_season_length = ends[0] - starts[0]\n",
    "        elif starts[0] > ends[0]:\n",
    "            growing_season_length = (12 - starts[0]) + ends[0]\n",
    "    \n",
    "    return growing_season_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17a7df-36cf-4776-bed9-e45d01b0e15f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_growing_season_length(ds_monthly_mean):\n",
    "    lai = ds_monthly_mean['lai']\n",
    "\n",
    "    # Create a DataArray for storing the growing season length\n",
    "    growing_season_length = xr.DataArray(\n",
    "        np.nan, \n",
    "        dims=('lat', 'lon'), \n",
    "        coords={'lat': ds_monthly_mean.lat, 'lon': ds_monthly_mean.lon}\n",
    "    )\n",
    "\n",
    "    for lat in ds_monthly_mean.lat.values:\n",
    "        for lon in ds_monthly_mean.lon.values:\n",
    "            lai_ts = lai.sel(lat=lat, lon=lon)\n",
    "            monthly_pct_change = calculate_monthly_pct_change(lai_ts)\n",
    "            starts, ends = detect_season_starts_and_ends(monthly_pct_change)\n",
    "            length = calculate_season_length(starts, ends)\n",
    "            growing_season_length.loc[dict(lat=lat, lon=lon)] = length\n",
    "\n",
    "    return growing_season_length\n",
    "\n",
    "def calculate_monthly_pct_change(lai_ts):\n",
    "    monthly_pct_change = ((lai_ts - lai_ts.roll(month=1)) / lai_ts.roll(month=1)) * 100\n",
    "    monthly_pct_change[0] = ((lai_ts[0] - lai_ts[-1]) / lai_ts[-1]) * 100\n",
    "    return monthly_pct_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c3ba2-5620-4301-a74f-b6a61afef8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_growing_season_length(ds):\n",
    "    # Convert the dataset to use Dask\n",
    "    ds = ds.chunk({'time': -1, 'lat': 'auto', 'lon': 'auto'})\n",
    "\n",
    "    # Apply the function to each chunk\n",
    "    lai = ds['lai']\n",
    "    years = range(ds.time.dt.year.min().values, ds.time.dt.year.max().values + 1)\n",
    "    gsl_template = xr.DataArray(\n",
    "        np.nan, \n",
    "        dims=('year', 'lat', 'lon'), \n",
    "        coords={'year': list(years), 'lat': ds.lat, 'lon': ds.lon}\n",
    "    )\n",
    "    gsl = xr.apply_ufunc(calculate_gsl_for_chunk, lai, dask='parallelized', output_dtypes=[float], kwargs={'years': years})\n",
    "\n",
    "    # Add the result to the dataset\n",
    "    ds['growing_season_length'] = gsl\n",
    "\n",
    "    # Trigger the computation\n",
    "    ds = ds.compute()\n",
    "\n",
    "    return ds\n",
    "\n",
    "def calculate_gsl_for_chunk(lai_chunk, years):\n",
    "\n",
    "    for year in years:\n",
    "        lai_yearly = lai_chunk.sel(time=str(year))\n",
    "        for lat in ds.lat.values:\n",
    "            for lon in ds.lon.values:\n",
    "                lai_ts = lai_yearly.sel(lat=lat, lon=lon, method='nearest')\n",
    "                monthly_mean, monthly_pct_change = calculate_monthly_changes(lai_ts)\n",
    "                starts, ends = detect_season_starts_and_ends(monthly_pct_change)\n",
    "                length = calculate_season_length(starts, ends)\n",
    "                growing_season_length_chunk.loc[dict(year=year, lat=lat, lon=lon)] = length\n",
    "\n",
    "    return growing_season_length_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f749cba-efb8-4532-9e4b-a7b880530fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= Have a look into the data ==============\n",
    "#print(ds_dict_ssp126.keys())\n",
    "#ds_dict_ssp370[list(ds_dict_hist.keys())[3]].parent_variant_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d51101-503b-4586-a60f-e6332bad2e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locations = {\n",
    "    \"Manaus, Brazil (Tropical Rainforest)\": {\"lat\": -3, \"lon\": -60},\n",
    "    \"Sahara Desert (Desert)\": {\"lat\": 23, \"lon\": 13},\n",
    "    \"Madrid, Spain (Mediterranean)\": {\"lat\": 40, \"lon\": -4},\n",
    "    \"Moscow, Russia (Continental)\": {\"lat\": 56, \"lon\": 37},\n",
    "    \"Northern Siberia, Russia (Tundra)\": {\"lat\": 70, \"lon\": 78},\n",
    "    \"Hanoi, Vietnam (Multiple Growing Seasons)\": {\"lat\": 21, \"lon\": 105},\n",
    "    \"New Delhi, India (Monsoon)\": {\"lat\": 28, \"lon\": 77},\n",
    "    \"Central Greenland (Greenland)\": {\"lat\": 72, \"lon\": -40},\n",
    "    \"Melbourne, Australia (Temperate)\": {\"lat\": -38, \"lon\": 145},\n",
    "    \"Cape Town, South Africa (Mediterranean)\": {\"lat\": -33, \"lon\": 19},\n",
    "    \"Buenos Aires, Argentina (Pampas)\": {\"lat\": -34, \"lon\": -58},\n",
    "    \"Central Sweden (Evergreen Forests)\": {\"lat\": 60, \"lon\": 15},\n",
    "    \"San Jos, Costa Rica (Tropical)\": {\"lat\": 9.93, \"lon\": -84.08},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0b548-7303-4662-99b2-cbbfe3244993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "growing_season_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ebe736-7bd5-4186-b4b6-94bb26601033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, coords in locations.items():\n",
    "    lai_ts = lai.sel(lat=coords['lat'], lon=coords['lon'], method='nearest')\n",
    "    \n",
    "    # Calculate monthly mean LAI\n",
    "    monthly_mean = lai_ts.groupby('time.month').mean('time')\n",
    "\n",
    "    # Compute percentage change including December to January\n",
    "    monthly_pct_change = ((monthly_mean - monthly_mean.roll(month=1)) / monthly_mean.roll(month=1)) * 100\n",
    "    monthly_pct_change[0] = ((monthly_mean[0] - monthly_mean[-1]) / monthly_mean[-1]) * 100\n",
    "    \n",
    "    # Compute the midpoint value for the December to January transition\n",
    "    dec_to_jan_midpoint_lai = (monthly_mean[-1] + monthly_mean[0]) / 2\n",
    "    dec_to_jan_midpoint_pct_change = (monthly_pct_change[-1] + monthly_pct_change[0]) / 2\n",
    "\n",
    "    # Adjust growing season start and end detection\n",
    "    starts, ends = [], []\n",
    "    # Check if December ends a growing season and January starts a new one\n",
    "    if monthly_pct_change[-2] > 0 and monthly_pct_change[-1] < 0:\n",
    "        ends.append(12)  # December ends a growing season\n",
    "    if monthly_pct_change[-1] <= 0 and monthly_pct_change[0] > 0:\n",
    "        starts.append(1)  # January starts a growing season\n",
    "    \n",
    "    #Check if January ends a growing season (wrap-around to the previous December)\n",
    "    if monthly_pct_change[0] < 0 and monthly_pct_change[-1] > 0:\n",
    "        ends.append(1)  # January ends a growing season\n",
    "\n",
    "    # Now handle February to November for starts and ends\n",
    "    for i in range(1, len(monthly_pct_change) - 1):\n",
    "        if monthly_pct_change[i-1] <= 0 and monthly_pct_change[i] > 0:\n",
    "            starts.append(i + 1)\n",
    "        elif monthly_pct_change[i-1] > 0 and monthly_pct_change[i] <= 0:\n",
    "            ends.append(i + 1)\n",
    "            \n",
    "    # Calculate the growing season length(s)\n",
    "    if len(starts) > 1:\n",
    "        if any(end > starts[0] for end in ends):\n",
    "            closest_end = min(filter(lambda end: end > starts[0], ends), key=lambda end: end - starts[0], default=None)\n",
    "            growing_season_length = closest_end - starts[0]\n",
    "        else:\n",
    "            growing_season_length = min(ends) + (12 - starts[0])\n",
    "\n",
    "        if any(end > starts[1] for end in ends):\n",
    "            growing_season_length = growing_season_length + max(ends) - starts[1]\n",
    "        else:\n",
    "            growing_season_length = growing_season_length + min(ends) + (12 - starts[1])\n",
    "    elif len(starts) == 1:\n",
    "        if starts[0] < ends[0]:\n",
    "            growing_season_length = ends[0] - starts[0]\n",
    "        elif starts[0] > ends[0]:\n",
    "            growing_season_length = (12 - starts[0]) + ends[0]\n",
    "            \n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot Monthly Mean LAI including the wrap-around from December to January\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Month')\n",
    "    ax1.set_ylabel('Mean LAI', color=color)\n",
    "    ax1.plot(range(1, 13), monthly_mean, color=color)\n",
    "    ax1.plot([12, 12.5], [monthly_mean[-1], dec_to_jan_midpoint_lai], color=color)\n",
    "    ax1.plot([0.5, 1], [dec_to_jan_midpoint_lai, monthly_mean[0]], color=color)\n",
    "    \n",
    "    # Plot the starts and ends for growing seasons\n",
    "    for start in starts:\n",
    "        ax1.axvline(x=start-0.5, color='green', linestyle='--', label='Start of Growing Season')\n",
    "    for end in ends:\n",
    "        ax1.axvline(x=end-0.5, color='brown', linestyle='--', label='End of Growing Season')\n",
    "\n",
    "    # Plot vertical lines for each month\n",
    "    for month in range(1, 13): \n",
    "        ax1.axvline(x=month, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax1.set_xticks(range(1, 13)) \n",
    "    ax1.set_xticklabels([str(month) for month in range(1, 13)])\n",
    "\n",
    "    # Plot Monthly Percentage Change on secondary y-axis\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('% Change', color=color)  \n",
    "    ax2.plot(range(1, 13), monthly_pct_change, color=color)\n",
    "    ax2.axhline(y=0, color='gray', linestyle='--', linewidth=1)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    # Plot the December to January transition for percentage change\n",
    "    ax2.plot([12, 12.5], [monthly_pct_change[-1], dec_to_jan_midpoint_pct_change], color='tab:red')\n",
    "    ax2.plot([0.5, 1], [dec_to_jan_midpoint_pct_change, monthly_pct_change[0]], color='tab:red')\n",
    "    \n",
    "    plt.title(f\"{name} - Growing Season: {growing_season_length} Months\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f417b3-2b33-4081-b1d8-6d3a1c21d269",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29262fe-a12e-44ff-894e-6d851abac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_period = select_period(ds_dict_hist, start_year=1985, end_year=2014, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa7c5a-9b33-4b1a-893e-bd0d8222b1b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = (compute_ensemble(ds_dict_hist_period))['Ensemble mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a12260-1df3-4232-bf91-196fb8144269",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period = select_period(ds_dict_ssp370, start_year=2071, end_year=2100, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de9ead-0d36-4f06-9430-9efeae7cc9bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b6390-f4b2-41d0-951d-c6bae78ce192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_hist_period_metric = compute_statistic(ds_dict_hist_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a9c21-8094-4700-b972-36e3f720d629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_ssp370_period_metric = compute_statistic(ds_dict_ssp370_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6bc0a-0837-4359-bd61-a6ad7408853e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute BGWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e99e6-709a-410c-abaa-c9a087e0346f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric = compute_bgws(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b485b-57a9-4d84-9581-52e28e732963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric_ensemble = compute_ensemble(ds_dict_hist_period_metric, metric='median')\n",
    "ds_ensmed_glob = ds_dict_hist_period_metric_ensemble['Ensemble median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d56759-5317-4815-936e-9b352c10589a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric = compute_bgws(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059612f5-0cdc-49bb-8bab-a8908037a89c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute global change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d33b29-a3a8-4be4-bb46-c410d4dfb0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Change\n",
    "ds_dict_change = compute_change(ds_dict_hist_period_metric, ds_dict_ssp370_period_metric, var_rel_change=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cf701-4d57-434c-ada9-d6fd836a8009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute ensemble median for the change\n",
    "ds_dict_change_ensemble = compute_ensemble(ds_dict_change, metric='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81befddc-a8da-482f-970c-d1a16431da05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only use ensemble median for further analysis\n",
    "ds_ensmed_glob = ds_dict_change_ensemble['Ensemble median']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913e646-e441-4989-b975-a12023b0baeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef8139-5a2c-43ce-902a-c19a7112355b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric_regions = {}\n",
    "ds_dict_hist_period_metric_regions = apply_region_mask(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbb35e-e9ae-4879-b94d-98d345b56487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric_regions = {}\n",
    "ds_dict_ssp370_period_metric_regions = apply_region_mask(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac685bca-d855-4d61-8deb-391f216db314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dff0a4-5120-4ab6-b5d7-30d5bc890203",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f3039-69a2-42fb-9ba9-b1413889fd37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Change\n",
    "ds_dict_region_change = compute_change(ds_dict_hist_period_metric_regions, ds_dict_ssp370_period_metric_regions, var_rel_change=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864d4d8-3d32-4427-9335-1fc653986222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute ensemble median for the change\n",
    "ds_dict_region_change_ensemble = compute_ensemble(ds_dict_region_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d953358-8a27-4833-b632-459a7d90f168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only use ensemble median for further analysis\n",
    "ds_dict_region_change_ensemble_metric = {}\n",
    "ds_dict_region_change_ensemble_metric['Ensemble mean'] = ds_dict_region_change_ensemble['Ensemble mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdb344-a81a-454d-9929-56f6273f688f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds_dict_region_change_ensemble_metric['Ensemble mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627f8da-5cea-456d-8da6-7ed4eb5a1834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_region_ensemble = compute_ensemble(ds_dict_hist_period_metric_regions)\n",
    "ds_hist_ensemble_metric = ds_dict_hist_region_ensemble['Ensemble mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b99e4e-a726-4ce8-b26a-26023f23222f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extreme Gradient Boositng Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7f259-5e26-4f37-83f7-b33502c671bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build extreme gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c9b97-dbfb-4f02-a88d-ad37526d7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00d2ee-b4eb-4801-8500-d2226b5cd21d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_scale(data):\n",
    "    # Find the absolute maximum value in the data\n",
    "    max_val = np.max(np.abs(data), axis=0)\n",
    "\n",
    "    # Scale data by dividing by the max value\n",
    "    scaled_data = data / max_val\n",
    "\n",
    "    return scaled_data, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b578b-2636-4e9e-927d-e87f04a9f1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_data(X, method):\n",
    "    scaler_data = {}\n",
    "\n",
    "    if method == 'std':\n",
    "        scaler = StandardScaler()\n",
    "        X_standardized = scaler.fit_transform(X)\n",
    "        scaler_data = {'mean': scaler.mean_, 'std': scaler.scale_}\n",
    "\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        X_standardized = scaler.fit_transform(X)\n",
    "        scaler_data = {'min': scaler.data_min_, 'max': scaler.data_max_}\n",
    "        \n",
    "    elif method == 'max':\n",
    "        X_standardized, max_val = custom_scale(X)\n",
    "        scaler_data = {'max': max_val}\n",
    "\n",
    "    elif method == 'no_scaling':\n",
    "        X_standardized = X\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Scaling method not known')\n",
    "\n",
    "    return X_standardized, scaler_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d5b6f-8c51-4978-9059-2c5311bf56f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, ParameterGrid, RandomizedSearchCV\n",
    "\n",
    "def train_xgb_models(ds, predictor_vars, predictant, scaling_method, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Train XGBoost models for each region with Grid Search, Cross-Validation, and Train/Test Split.\n",
    "\n",
    "    Parameters:\n",
    "    - ds: xarray dataset\n",
    "    - predictor_vars: List of predictor variable names\n",
    "    - predictant: Name of the predictant variable\n",
    "    - scaling_method: Method for scaling features\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing trained models, best parameters, best scores, CV scores (R2 and MSE), feature importances, and performance metrics for each region.\n",
    "    \"\"\"\n",
    "    # Initialize dictionaries\n",
    "    xgb_models = {}\n",
    "    best_params = {}\n",
    "    best_scores = {}\n",
    "    feature_importances = {}\n",
    "    permutation_importances_test = {}\n",
    "    permutation_importances_train = {}\n",
    "    performance_metrics_test = {}\n",
    "    performance_metrics_train = {}\n",
    "    cv_scores_r2 = {}\n",
    "    cv_scores_mse = {}\n",
    "    cv_scores_r2_test = {}\n",
    "    cv_scores_mse_test = {}\n",
    "    cv_scores_r2_train = {}\n",
    "    cv_scores_mse_train = {}\n",
    "    scaled_data = {}\n",
    "\n",
    "    # Parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "    'n_estimators': [100, 200, 400, 800, 1000, 1100, 1200, 1300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [5, 7, 10, 14],\n",
    "    'min_child_weight': [0.01, 0.1, 1, 10, 12],\n",
    "    'lambda': [1, 3],\n",
    "    'alpha': [0.001, 0.01],\n",
    "    #'gamma': [0.05, 0.1]\n",
    "    }\n",
    "    \n",
    "\n",
    "    for region in ds.region.values:\n",
    "        # Data preparation\n",
    "        df = ds.sel(region=region).to_dataframe().reset_index()\n",
    "        region_name = ds.names.sel(region=region).values\n",
    "        df.dropna(inplace=True)\n",
    "        X = df[predictor_vars]\n",
    "        y = df[predictant]\n",
    "        \n",
    "        X_scaled, scaled_data[f'{region_name}'] = scale_data(X, method=scaling_method)\n",
    "\n",
    "        # Train/Test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Further split the training set into training and validation sets for hyperparameter tuning\n",
    "        X_train_tuning, X_val, y_train_tuning, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Initialize XGBoost model\n",
    "        xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "        # GridSearchCV for hyperparameter tuning on training data\n",
    "        grid_search = GridSearchCV(xgb, param_grid, cv=cv_folds, n_jobs=-1, scoring='r2')\n",
    "        grid_search.fit(X_train_tuning, y_train_tuning)\n",
    "\n",
    "        # Store the best model, parameters, and score for the region\n",
    "        xgb_models[f'{region_name}'] = grid_search.best_estimator_\n",
    "        best_params[f'{region_name}'] = grid_search.best_params_\n",
    "        best_scores[f'{region_name}'] = -grid_search.best_score_ \n",
    "\n",
    "        # Update feature_importances assignment\n",
    "        feature_importances[f'{region_name}'] = xgb_models[f'{region_name}'].feature_importances_\n",
    "        \n",
    "        #Compute permutation importance\n",
    "        perm_importance = permutation_importance(\n",
    "            xgb_models[f'{region_name}'], X_test, y_test, n_repeats=20, random_state=42\n",
    "        )\n",
    "        permutation_importances_test[f'{region_name}'] = perm_importance['importances']\n",
    "        \n",
    "        perm_importance_train = permutation_importance(\n",
    "            xgb_models[f'{region_name}'], X_train, y_train, n_repeats=20, random_state=42\n",
    "        )\n",
    "        permutation_importances_train[f'{region_name}'] = perm_importance_train['importances']\n",
    "\n",
    "        # Performance metrics\n",
    "        y_pred = xgb_models[f'{region_name}'].predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        performance_metrics_test[f'{region_name}'] = {'MSE': mse, 'R2': r2}\n",
    "        \n",
    "        y_pred_train = xgb_models[f'{region_name}'].predict(X_val)\n",
    "        mse_train = mean_squared_error(y_val, y_pred_train)\n",
    "        r2_train = r2_score(y_val, y_pred_train)\n",
    "        performance_metrics_train[f'{region_name}'] = {'MSE': mse_train, 'R2': r2_train}\n",
    "\n",
    "        # Perform cross-validation and store results\n",
    "        cv_scores_r2[f'{region_name}'] = cross_val_score(xgb_models[f'{region_name}'], X_scaled, y, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse[f'{region_name}'] = -cross_val_score(xgb_models[f'{region_name}'], X_scaled, y, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        cv_scores_r2_train[f'{region_name}'] = cross_val_score(xgb_models[f'{region_name}'], X_train, y_train, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse_train[f'{region_name}'] = -cross_val_score(xgb_models[f'{region_name}'], X_train, y_train, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        cv_scores_r2_test[f'{region_name}'] = cross_val_score(xgb_models[f'{region_name}'], X_test, y_test, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse_test[f'{region_name}'] = -cross_val_score(xgb_models[f'{region_name}'], X_test, y_test, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        \n",
    "    feature_importances_df = pd.DataFrame.from_dict(feature_importances, orient='index', columns=predictor_vars)\n",
    "\n",
    "    return feature_importances_df, permutation_importances_test, permutation_importances_train, best_params, xgb_models, best_scores, performance_metrics_test, performance_metrics_train, cv_scores_r2, cv_scores_mse, cv_scores_r2_train, cv_scores_mse_train, cv_scores_r2_test, cv_scores_mse_test, scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6474a-14c0-4f34-b712-361fb6235235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data and predictor variables\n",
    "predictor_vars = ['tas', 'pr', 'vpd','evapo', 'mrso','lai', 'gpp'] #'pr',  'gpp' 'evspsbl'  'mrro', 'tran', \n",
    "predictant = 'bgws'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e45445-0acc-43e0-ad77-8d7125c6a753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_reduced_regions = ds.sel(region=slice(10, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064eaf2-7ed4-4d00-ba57-937af7c9d6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances_df_xgb, permutation_importances_test_xgb, permutation_importances_train_xgb, best_params_xgb, x_gradient_boosting_models, best_scores_xgb, performance_metrics_test_xgb, performance_metrics_train_xgb, cv_scores_r2_xgb, cv_scores_mse_xgb, cv_scores_r2_train_xgb, cv_scores_mse_train_xgb, cv_scores_r2_test_xgb, cv_scores_mse_test_xgb, scaler_data_xgb = train_xgb_models(ds_reduced_regions, predictor_vars, predictant, scaling_method='no_scaling', cv_folds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0de453a-a961-4be1-b7cd-5eb3e9a1d48e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6698a1-4096-4f44-834b-eade3cb478af",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91b0a3-3daa-4327-813f-489d558cb9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curves_for_all_regions(models, ds, predictor_vars, predictant, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Plots learning curves for each region's model using the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - models: Dictionary of trained models, one for each region.\n",
    "    - ds: xarray dataset used in training models.\n",
    "    - predictor_vars: List of predictor variable names.\n",
    "    - predictant: Name of the predictant variable.\n",
    "    - region_names: List of region names.\n",
    "    - cv_folds: Number of folds for cross-validation.\n",
    "    \"\"\"\n",
    "    for region in ds.region.values:\n",
    "        # Extract data for the region\n",
    "        df = ds.sel(region=region).to_dataframe().reset_index()\n",
    "        df.dropna(inplace=True)\n",
    "        X = df[predictor_vars]\n",
    "        y = df[predictant]\n",
    "        \n",
    "        region_name = ds.names.sel(region=region).values\n",
    "\n",
    "        # Learning curve computation\n",
    "        train_sizes, train_scores, validation_scores = learning_curve(\n",
    "            estimator=models[f'{region_name}'],\n",
    "            X=X, \n",
    "            y=y, \n",
    "            train_sizes=np.linspace(0.1, 1.0, 10, 550),\n",
    "            cv=cv_folds,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Calculate mean and standard deviation for training and validation set scores\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        validation_mean = np.mean(validation_scores, axis=1)\n",
    "        validation_std = np.std(validation_scores, axis=1)\n",
    "\n",
    "        # Plot learning curves\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_sizes, train_mean, label='Training error', color='blue', marker='o')\n",
    "        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.15)\n",
    "        plt.plot(train_sizes, validation_mean, label='Validation error', color='green', marker='o')\n",
    "        plt.fill_between(train_sizes, validation_mean - validation_std, validation_mean + validation_std, color='green', alpha=0.15)\n",
    "\n",
    "        plt.title(f'Learning Curves for {region_name}')\n",
    "        plt.xlabel('Training Data Size')\n",
    "        plt.ylabel('R^2')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f6234-4251-44fa-8ee2-f3f0cab9d62e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_learning_curves_for_all_regions(x_gradient_boosting_models, ds_reduced_regions, predictor_vars, predictant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b135a45-e0ca-4dcd-b4d3-7e0532605225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_performance_metrics(performance_metrics_train, performance_metrics_test, cv_scores_r2, cv_scores_mse, cv_scores_r2_train, cv_scores_mse_train, cv_scores_r2_test, cv_scores_mse_test):\n",
    "    regions = list(performance_metrics_train.keys())\n",
    "    mse_train = [performance_metrics_train[region]['MSE'] for region in regions]\n",
    "    mse_test = [performance_metrics_test[region]['MSE'] for region in regions]\n",
    "    r2_train = [performance_metrics_train[region]['R2'] for region in regions]\n",
    "    r2_test = [performance_metrics_test[region]['R2'] for region in regions]\n",
    "   \n",
    "    fig, ax = plt.subplots(6, 1, figsize=(15, 30))\n",
    "\n",
    "    # Plot 1: R2 Comparison\n",
    "    ax[0].plot(regions, r2_train, label='Train R2', marker='o')\n",
    "    ax[0].plot(regions, r2_test, label='Test R2', marker='o')\n",
    "    ax[0].set_title('R2 Comparison')\n",
    "    ax[0].set_xlabel('Regions')\n",
    "    ax[0].set_ylabel('R2')\n",
    "    ax[0].set_ylim([0,1])\n",
    "    ax[0].legend()\n",
    "    ax[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # Plot 2: MSE Comparison\n",
    "    ax[1].plot(regions, mse_train, label='Train MSE', marker='o')\n",
    "    ax[1].plot(regions, mse_test, label='Test MSE', marker='o')\n",
    "    ax[1].set_title('MSE Comparison')\n",
    "    ax[1].set_xlabel('Regions')\n",
    "    ax[1].set_ylabel('MSE')\n",
    "    ax[1].set_ylim([0,0.006])\n",
    "    ax[1].legend()\n",
    "    ax[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # Function to plot mean or standard deviation\n",
    "    def plot_cv_scores(ax, cv_scores_whole, cv_scores_train, cv_scores_test, title, ylabel):\n",
    "        means_whole = [np.mean(scores) for scores in cv_scores_whole]\n",
    "        means_train = [np.mean(scores) for scores in cv_scores_train]\n",
    "        means_test = [np.mean(scores) for scores in cv_scores_test]\n",
    "        if ylabel == 'Mean Scores':\n",
    "            ax.plot(regions, means_whole, label='Whole Data', marker='o')\n",
    "            ax.plot(regions, means_train, label='Train Data', marker='o')\n",
    "            ax.plot(regions, means_test, label='Test Data', marker='o')\n",
    "        else:\n",
    "            stds_whole = [np.std(scores) for scores in cv_scores_whole]\n",
    "            stds_train = [np.std(scores) for scores in cv_scores_train]\n",
    "            stds_test = [np.std(scores) for scores in cv_scores_test]\n",
    "            ax.plot(regions, stds_whole, label='Whole Data', marker='o')\n",
    "            ax.plot(regions, stds_train, label='Train Data', marker='o')\n",
    "            ax.plot(regions, stds_test, label='Test Data', marker='o')\n",
    "        \n",
    "        # Set y-axis limits for R2 plots\n",
    "        if title == 'Mean CV R2 Scores' or title == 'Std CV R2 Scores':\n",
    "            ax.set_ylim([0, 1]) \n",
    "        else:\n",
    "            ax.set_ylim([0, 0.006]) \n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Regions')\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.legend()\n",
    "        ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # Plot 3: Mean CV R2 Scores\n",
    "    plot_cv_scores(ax[2], [cv_scores_r2[region] for region in regions], [cv_scores_r2_train[region] for region in regions], [cv_scores_r2_test[region] for region in regions], 'Mean CV R2 Scores', 'Mean Scores')\n",
    "\n",
    "    # Plot 4: Std CV R2 Scores\n",
    "    plot_cv_scores(ax[3], [cv_scores_r2[region] for region in regions], [cv_scores_r2_train[region] for region in regions], [cv_scores_r2_test[region] for region in regions], 'Std CV R2 Scores', 'Standard Deviation')\n",
    "\n",
    "    # Plot 5: Mean CV MSE Scores\n",
    "    plot_cv_scores(ax[4], [cv_scores_mse[region] for region in regions], [cv_scores_mse_train[region] for region in regions], [cv_scores_mse_test[region] for region in regions], 'Mean CV MSE Scores', 'Mean Scores')\n",
    "\n",
    "    # Plot 6: Std CV MSE Scores\n",
    "    plot_cv_scores(ax[5], [cv_scores_mse[region] for region in regions], [cv_scores_mse_train[region] for region in regions], [cv_scores_mse_test[region] for region in regions], 'Std CV MSE Scores', 'Standard Deviation')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70025ea4-c82f-471c-aba9-4c452809bf33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_performance_metrics(performance_metrics_train_xgb, performance_metrics_test_xgb, cv_scores_r2_xgb, cv_scores_mse_xgb, cv_scores_r2_train_xgb, cv_scores_mse_train_xgb, cv_scores_r2_test_xgb, cv_scores_mse_test_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ea344-736b-4e8b-a4b9-a7281b2297a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_permutation_importances(permutation_importances_test, permutation_importances_train, predictor_vars):\n",
    "    # Number of regions\n",
    "    num_regions = len(permutation_importances_test)\n",
    "\n",
    "    # Create subplots - one row per region\n",
    "    fig, axs = plt.subplots(num_regions, 2, figsize=(15, num_regions * 4))\n",
    "\n",
    "    for idx, region in enumerate(permutation_importances_test):\n",
    "        # Convert arrays to DataFrames for easy plotting\n",
    "        test_df = pd.DataFrame(permutation_importances_test[region].T, columns=predictor_vars)\n",
    "        train_df = pd.DataFrame(permutation_importances_train[region].T, columns=predictor_vars)\n",
    "        \n",
    "        # Plot for test data\n",
    "        sns.boxplot(data=test_df, orient='h', ax=axs[idx, 0])\n",
    "        axs[idx, 0].axvline(0, color='grey', linestyle='--')\n",
    "        axs[idx, 0].set_title(f'{region} - Test Data')\n",
    "        axs[idx, 0].set_xlabel('Decrease in Accuracy Score')\n",
    "        axs[idx, 0].set_ylabel('Variables')\n",
    "\n",
    "        # Plot for training data\n",
    "        sns.boxplot(data=train_df, orient='h', ax=axs[idx, 1])\n",
    "        axs[idx, 1].axvline(0, color='grey', linestyle='--')\n",
    "        axs[idx, 1].set_title(f'{region} - Training Data')\n",
    "        axs[idx, 1].set_xlabel('Decrease in Accuracy Score')\n",
    "        axs[idx, 1].set_ylabel('Variables')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e857c84-f455-4997-8252-c6b78f028fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_permutation_importances(permutation_importances_test_xgb, permutation_importances_train_xgb, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c6bc9-4aad-4664-88d2-32b7d1a2b829",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Assess variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59ae94-ef1b-421d-85e6-0cc435bf410a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "permutation_importance_df_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acad1df-45fd-4e9d-870d-5cad9c61ada0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances_df * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32681dd8-0598-49bf-95d0-de6345fab979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances_df * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c64ad7a-fec1-411e-b326-4ff435971e0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb58ac-0320-40f6-b533-60e3796b1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "def test_regression_assumptions_scikit(regression_models, test_data, predictor_vars):\n",
    "    results = []\n",
    "\n",
    "    for region_name in regression_models:\n",
    "        model = regression_models[region_name]\n",
    "        X_test, y_test = test_data[region_name]\n",
    "\n",
    "        # Predict and calculate residuals\n",
    "        predictions = model.predict(X_test)\n",
    "        residuals = y_test - predictions\n",
    "\n",
    "        # Add a constant term for the Breusch-Pagan test\n",
    "        X_test_with_constant = np.column_stack((np.ones(X_test.shape[0]), X_test))\n",
    "\n",
    "        # Test for normality of residuals\n",
    "        shapiro_stat, shapiro_p = shapiro(residuals)\n",
    "\n",
    "        # Test for homoscedasticity\n",
    "        _, _, _, bp_pvalue = het_breuschpagan(residuals, X_test_with_constant)\n",
    "\n",
    "        # VIF for multicollinearity\n",
    "        vif = [variance_inflation_factor(X_test, i) for i in range(X_test.shape[1])]\n",
    "\n",
    "        # Prepare plot data\n",
    "        plot_data = {\n",
    "            'Region': region_name,\n",
    "            'Predictions': predictions,\n",
    "            'Residuals': residuals,\n",
    "            'Shapiro-Wilk': shapiro_stat,\n",
    "            'Shapiro-Wilk p-value': shapiro_p,\n",
    "            'Breusch-Pagan p-value': bp_pvalue,\n",
    "            'VIF': vif\n",
    "        }\n",
    "        results.append(plot_data)\n",
    "    \n",
    "    # Plotting\n",
    "    num_regions = len(results)\n",
    "    fig, axs = plt.subplots(num_regions, 3, figsize=(22, 5 * num_regions)) # Changed to 3 subplots for VIF\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        sns.residplot(x=result['Predictions'], y=result['Residuals'], lowess=True, ax=axs[i, 0])\n",
    "        axs[i, 0].set_title(f'Residuals vs Predictions for {result[\"Region\"]}')\n",
    "        axs[i, 0].set_xlabel('Predicted values')\n",
    "        axs[i, 0].set_ylabel('Residuals')\n",
    "\n",
    "        # Adding text for statistical tests\n",
    "        axs[i, 0].text(0.05, 0.95, f\"Shapiro-Wilk: {result['Shapiro-Wilk']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "        axs[i, 0].text(0.05, 0.90, f\"Shapiro-Wilk p-value: {result['Shapiro-Wilk p-value']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "        axs[i, 0].text(0.05, 0.85, f\"Breusch-Pagan p-value: {result['Breusch-Pagan p-value']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "\n",
    "        sns.histplot(result['Residuals'], kde=True, ax=axs[i, 1])\n",
    "        axs[i, 1].set_title(f'Residual Distribution for {result[\"Region\"]}')\n",
    "        axs[i, 1].set_xlabel('Residuals')\n",
    "        axs[i, 1].set_ylabel('Frequency')\n",
    "\n",
    "        # VIF bar plot\n",
    "        sns.barplot(x=predictor_vars, y=result['VIF'], ax=axs[i, 2])\n",
    "        axs[i, 2].hlines(5, xmin=-0.5, xmax=len(result['VIF'])-0.5, colors='orange', linestyles='dashed')\n",
    "        axs[i, 2].hlines(10, xmin=-0.5, xmax=len(result['VIF'])-0.5, colors='r', linestyles='dashed')\n",
    "        axs[i, 2].set_title(f'VIF for {result[\"Region\"]}')\n",
    "        axs[i, 2].set_xlabel('Predictor Variables')\n",
    "        axs[i, 2].set_ylabel('VIF Value')\n",
    "        axs[i, 2].set_ylim(0, max(result['VIF']) + 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f835f9b-fd63-49b1-80df-e0fcef9413dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assumptions_df = test_regression_assumptions_scikit(regression_models, test_data, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f876b-31e1-4566-b3e5-c43fee81a59f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845e157-982f-4644-ab79-107e9867e0c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_model_performance_all_regions(ds, regression_models, best_params, best_scores, scalers, predictor_vars):\n",
    "    region_names = ds.names.values\n",
    "    region_indices = ds.region.values\n",
    "    \n",
    "    # Determine the number of rows and columns for the subplots based on the number of regions\n",
    "    num_regions = len(region_names)\n",
    "    cols = 3  # We are keeping 3 columns as per your requirement\n",
    "    rows = math.ceil(num_regions / cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 5 * rows), squeeze=False)\n",
    "    \n",
    "    # Initialize an empty dictionary to store performance metrics for each region\n",
    "    performance_metrics = {}\n",
    "\n",
    "    for idx, (ax, region) in enumerate(zip(axs.flatten(), region_indices)):\n",
    "        region_name = ds.names.sel(region=region).values.item()\n",
    "        # Prepare the data for the region\n",
    "        df = ds.sel(region=region).to_dataframe().dropna()\n",
    "        X = df[predictor_vars]\n",
    "        y_true = df['bgws'].values\n",
    "\n",
    "        # Retrieve the scaler for the region\n",
    "        scaler = scalers[region_name]\n",
    "        X_standardized = scaler.transform(X)  # Use transform here, not fit_transform\n",
    "        \n",
    "        # Predict using the trained model\n",
    "        model = regression_models[region_name]\n",
    "        y_pred = model.predict(X_standardized)\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = y_true - y_pred\n",
    "        \n",
    "        # Calculate and store performance metrics\n",
    "        mse_value = mean_squared_error(y_true, y_pred)\n",
    "        r2_value = r2_score(y_true, y_pred)\n",
    "        best_param = best_params[region_name]\n",
    "        best_cv_score = best_scores[region_name]\n",
    "        \n",
    "        performance_metrics[region_name] = {\n",
    "            'MSE': mse_value,\n",
    "            'R^2': r2_value,\n",
    "            'Best Parameters': best_param,\n",
    "            'Best CV Score': best_cv_score\n",
    "        }\n",
    "        \n",
    "        # Plot the residuals\n",
    "        ax.scatter(y_true, residuals, c='blue', alpha=0.5, s=10)\n",
    "        ax.axhline(0, color='red', lw=2)\n",
    "        ax.text(0.05, 0.95, f'MSE: {mse_value:.6f}\\nR^2: {r2_value:.2f}\\nBest CV: {best_cv_score:.4f}', \n",
    "                transform=ax.transAxes, verticalalignment='top')\n",
    "        ax.set_title(f'{region_name}\\n{best_param}')\n",
    "        ax.set_xlabel('True Values')\n",
    "        ax.set_ylabel('Residuals')\n",
    "        \n",
    "        # Hide axes for subplots that are not used (if num_regions < rows*cols)\n",
    "        if idx >= num_regions:\n",
    "            ax.set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224c1a8-8798-47e2-948f-a03fb4c51ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_performance_metrics = assess_model_performance_all_regions(ds, gradient_boosting_models, best_params, best_scores, scaler_data, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abe33ab-3bdd-4e4f-be6d-a301a313c54c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multiple Linear Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20110909-55f2-4799-bb61-38482b75388a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187b7bd-c237-4732-b9d5-999e869dc3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_standardized_data(X_standardized, y, predictor_vars):\n",
    "    # Summarize predictors\n",
    "    X_summary = pd.DataFrame(X_standardized, columns=predictor_vars).describe().transpose()\n",
    "    # Summarize response variable\n",
    "    y_summary = pd.DataFrame(y, columns=['bgws']).describe().transpose()\n",
    "    # Combine summaries\n",
    "    summary = pd.concat([X_summary, y_summary])\n",
    "    return summary[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c2d47-980b-45c8-ae3b-188f5091ddde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarize_standardized_data(X, y, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de39e1e-8e43-4cf3-bd55-fe76aff4f95d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot data correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db9302-7ae6-4afc-b4bb-de5778256460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_spearman_correlation(ds, predictor_vars, predictant):\n",
    "    # Number of regions\n",
    "    n_regions = len(ds.region)\n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    n_cols = 3\n",
    "    n_rows = np.ceil(n_regions / n_cols).astype(int)\n",
    "\n",
    "    # Initialize the subplot figure\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4))\n",
    "    axes = axes.flatten()  # Flatten the array for easy iteration\n",
    "\n",
    "    for index, region in enumerate(ds.region.values):\n",
    "        # Select the current region data\n",
    "        df = ds.sel(region=region).to_dataframe().dropna()\n",
    "\n",
    "        # Concatenate predictor variables and predictant for correlation\n",
    "        data = df[predictor_vars + [predictant]]\n",
    "\n",
    "        # Compute the Spearman correlation matrix\n",
    "        corr_matrix = data.corr(method='spearman')\n",
    "\n",
    "        # Plot the heatmap\n",
    "        sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1, cbar=index == 0, ax=axes[index])\n",
    "\n",
    "        # Set the title with the region name\n",
    "        region_name = ds.names.sel(region=region).values\n",
    "        axes[index].set_title(f\"Region: {region_name}\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for i in range(n_regions, n_rows * n_cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fed71-f1d1-4301-8bc9-203a737ed591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_spearman_correlation(ds, predictor_vars, predictant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b309650-e20e-4ac2-870a-f1627239b557",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203ddc2-e72d-4c07-856f-2c9a57ae4913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_all_distributions(ds, predictor_vars):\n",
    "    num_regions = len(ds.region)\n",
    "    # Set up the matplotlib figure with a certain number of columns\n",
    "    cols = 3\n",
    "    rows = (num_regions // cols) + (num_regions % cols > 0)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5), constrained_layout=True)\n",
    "    axes = axes.flatten()  # Flatten to 1D array for easy iteration\n",
    "    \n",
    "    for idx, region in enumerate(ds.region.values):\n",
    "        # Select data for the region\n",
    "        df = ds.sel(region=region).to_dataframe().reset_index()\n",
    "        df = df.dropna(subset=predictor_vars + ['bgws'])  # Drop NaN values for relevant columns only\n",
    "        \n",
    "        # Extract predictor variables and the target variable\n",
    "        X = df[predictor_vars]\n",
    "        y = df['bgws']\n",
    "        \n",
    "        # Standardize the predictors\n",
    "        scaler = StandardScaler()\n",
    "        X_standardized = scaler.fit_transform(X)\n",
    "        \n",
    "        # Create DataFrame from the standardized predictors\n",
    "        df_standardized = pd.DataFrame(X_standardized, columns=predictor_vars)\n",
    "        df_standardized['bgws'] = y.values  # Add non-standardized 'bgws'\n",
    "        \n",
    "        # Plotting on the respective subplot\n",
    "        ax = axes[idx]\n",
    "        # Create a list to store handles for the legend\n",
    "        handles = []\n",
    "        for col in df_standardized.columns:\n",
    "            # Plot each variable and get the handle\n",
    "            handle = sns.histplot(df_standardized[col], kde=True, ax=ax, label=col)\n",
    "            handles.append(handle)\n",
    "\n",
    "        region_name = ds.names.sel(region=region).values\n",
    "        ax.set_title(f'Region {region_name}')\n",
    "\n",
    "        # Only add legend to the first subplot\n",
    "        if idx == 0:\n",
    "            ax.legend(title='Variable')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for ax in axes[num_regions:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6c7d2-f969-41d3-9c82-7490093f3993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_all_distributions(ds, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec07de-31f8-4912-b821-4bcd580364c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Build regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1ff62-2afb-4285-ab20-fbb6e016a41a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "def lr_models(ds_change, predictor_vars, predictant, scaling_method, regression_type='ridge', cv_folds=5, scaling_back=False):\n",
    "    \"\"\"\n",
    "    Train regularized linear regression models (Ridge, Lasso, or ElasticNet) for each region using Grid Search for hyperparameter tuning,\n",
    "    perform cross-validation, compute permutation importance, and gather performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - ds_change: xarray datasets\n",
    "    - predictor_vars: List of predictor variable names\n",
    "    - predictant: Name of the predictant variable\n",
    "    - regression_type: Type of regression ('ridge', 'lasso', 'elasticnet')\n",
    "    - cv_folds: Number of folds for cross-validation\n",
    "    - scaling_method: Scaling method (std or minmax)\n",
    "    - scaling_back: Boolean flag to scale back coefficients\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing trained models, best hyperparameters, coefficients, cross-validation scores,\n",
    "      permutation importances, and performance metrics for each region.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionaries\n",
    "    regression_models = {}\n",
    "    best_hyperparams = {}\n",
    "    best_scores = {}\n",
    "    regression_coeffs = {}\n",
    "    cv_scores_r2 = {}\n",
    "    cv_scores_mse = {}\n",
    "    cv_scores_r2_test = {}\n",
    "    cv_scores_mse_test = {}\n",
    "    cv_scores_r2_train = {}\n",
    "    cv_scores_mse_train = {}\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    residuals = {}\n",
    "    performance_metrics_test = {}\n",
    "    performance_metrics_train = {}\n",
    "    permutation_importances_test = {}\n",
    "    permutation_importances_train = {}\n",
    "    scaled_data = {}\n",
    "\n",
    "    # Define parameter grid based on regression type\n",
    "    if regression_type == 'ridge':\n",
    "        model = Ridge(random_state=42)\n",
    "        param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    elif regression_type == 'lasso':\n",
    "        model = Lasso(random_state=42)\n",
    "        param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    elif regression_type == 'elasticnet':\n",
    "        model = ElasticNet(random_state=42)\n",
    "        param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100], 'l1_ratio': [0.2, 0.5, 0.8]}\n",
    "    else:\n",
    "        raise ValueError(\"Invalid regression type. Choose 'ridge', 'lasso', or 'elasticnet'.\")\n",
    "\n",
    "    for region in ds_change.region.values:\n",
    "        # Data preparation\n",
    "        df = ds_change.sel(region=region).to_dataframe().reset_index()\n",
    "        df.dropna(inplace=True)\n",
    "        X = df[predictor_vars]\n",
    "        y = df[predictant]\n",
    "        \n",
    "        # Get region name\n",
    "        region_name = ds.names.sel(region=region).values\n",
    "\n",
    "        # Scale the data\n",
    "        X_scaled, scaled_data[f'{region_name}'] = scale_data(X, method=scaling_method)\n",
    "\n",
    "        # Train/Test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Grid search\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=cv_folds, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Store best model and hyperparameters\n",
    "        best_model = grid_search.best_estimator_\n",
    "        regression_models[f'{region_name}'] = best_model\n",
    "        best_hyperparams[f'{region_name}'] = grid_search.best_params_\n",
    "        best_scores[f'{region_name}'] = grid_search.best_score_\n",
    "        regression_coeffs[f'{region_name}'] = best_model.coef_\n",
    "        \n",
    "        # Perform cross-validation and store results\n",
    "        cv_scores_r2[f'{region_name}'] = cross_val_score(best_model, X_scaled, y, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse[f'{region_name}'] = -cross_val_score(best_model, X_scaled, y, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        cv_scores_r2_train[f'{region_name}'] = cross_val_score(best_model, X_train, y_train, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse_train[f'{region_name}'] = -cross_val_score(best_model, X_train, y_train, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        cv_scores_r2_test[f'{region_name}'] = cross_val_score(best_model, X_test, y_test, cv=cv_folds, scoring='r2')\n",
    "        cv_scores_mse_test[f'{region_name}'] = -cross_val_score(best_model, X_test, y_test, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        # Compute and store performance metrics\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "        r2_test = r2_score(y_test, y_pred_test)\n",
    "        performance_metrics_test[f'{region_name}'] = {'MSE': mse_test, 'R2': r2_test}\n",
    "\n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "        r2_train = r2_score(y_train, y_pred_train)\n",
    "        performance_metrics_train[f'{region_name}'] = {'MSE': mse_train, 'R2': r2_train}\n",
    "\n",
    "        # Compute permutation importance\n",
    "        perm_importance_test = permutation_importance(best_model, X_test, y_test, n_repeats=20, random_state=42)\n",
    "        permutation_importances_test[f'{region_name}'] = perm_importance_test['importances']\n",
    "\n",
    "        perm_importance_train = permutation_importance(best_model, X_train, y_train, n_repeats=20, random_state=42)\n",
    "        permutation_importances_train[f'{region_name}'] = perm_importance_train['importances']\n",
    "\n",
    "        # Store train and test data\n",
    "        train_data[f'{region}'] = (X_train, y_train)\n",
    "        test_data[f'{region}'] = (X_test, y_test)\n",
    "\n",
    "    # Convert regression coefficients to DataFrame\n",
    "    coeffs_df = pd.DataFrame.from_dict(regression_coeffs, orient='index', columns=predictor_vars)\n",
    "\n",
    "    return coeffs_df, regression_models, best_hyperparams, best_scores, train_data, test_data, cv_scores_r2, cv_scores_mse, cv_scores_r2_train, cv_scores_mse_train, cv_scores_r2_test, cv_scores_mse_test, performance_metrics_test, performance_metrics_train, permutation_importances_test, permutation_importances_train, scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221ccfa-58e1-4409-99fb-1012bd125ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr_models(ds_change, predictor_vars, predictant, scaling_method, cv_folds=5, scaling_back=False):\n",
    "    \"\"\"\n",
    "    Train multivariate regression models for each region, perform cross-validation, \n",
    "    and compute performance metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - ds_change, ds_hist: xarray datasets\n",
    "    - predictor_vars: List of predictor variable names\n",
    "    - predictant: Name of the predictant variable\n",
    "    - cv_folds: Number of folds for cross-validation (default is 5)\n",
    "    - scaling: Scaling method std or minmax\n",
    "    - scaling_back: Boolean flag to scale back coefficients\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary containing trained models, coefficients, cross-validation scores,\n",
    "      and performance metrics for each region.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize dictionaries\n",
    "    regression_models = {}\n",
    "    regression_coeffs = {}\n",
    "    cv_scores = {}\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    residuals = {}\n",
    "    performance_metrics = {}\n",
    "    scaled_data = {}\n",
    "\n",
    "    for region in ds_change.region.values:\n",
    "        # Convert xarray data to pandas DataFrame\n",
    "        df = ds_change.sel(region=region).to_dataframe().reset_index()\n",
    "        df = df.dropna()  # Drop rows with NaN values\n",
    "        \n",
    "        # Get region name\n",
    "        region_name = ds.names.sel(region=region).values\n",
    "\n",
    "        X = df[predictor_vars]\n",
    "        y = df[predictant]\n",
    "        \n",
    "        # Scale the data\n",
    "        X_scaled, scaled_data[f'{region_name}'] = scale_data(X, method=scaling_method)\n",
    "        \n",
    "        # Train/Test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # 30% of the data are used to test the model\n",
    "\n",
    "        # Train the regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Store model and coefficients\n",
    "        regression_models[f'{region_name}'] = model\n",
    "        regression_coeffs[f'{region_name}'] = model.coef_\n",
    "        train_data[f'{region_name}'] = (X_train, y_train)\n",
    "        test_data[f'{region_name}'] = (X_test, y_test)\n",
    "\n",
    "        # Predict using the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate residuals\n",
    "        resid = y_test - y_pred\n",
    "        residuals[f'{region_name}'] = resid\n",
    "\n",
    "        # Compute and store performance metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        performance_metrics[f'{region_name}'] = {'MSE': mse, 'R2': r2}\n",
    "\n",
    "        # Perform cross-validation and store results\n",
    "        cv_score = cross_val_score(model, X, y, cv=cv_folds) \n",
    "        cv_scores[f'{region_name}'] = cv_score\n",
    "\n",
    "    # Convert regression coefficients to DataFrame\n",
    "    coeffs_list = []\n",
    "\n",
    "    if scaling_back:\n",
    "        if scaling_method == 'std':\n",
    "            for region, coefs in regression_coeffs.items():\n",
    "                feature_means, feature_stds = scaled_data[region]['mean'], scaled_data[region]['std']\n",
    "                # Scale back coefficients\n",
    "                original_coefs = coefs / feature_stds\n",
    "                intercept = model.intercept_ - np.sum(original_coefs * feature_means)\n",
    "                coeffs_list.append(np.concatenate(coefs))\n",
    "                \n",
    "        elif scaling_method == 'minmax':\n",
    "            for region, coefs in regression_coeffs.items():\n",
    "                feature_mins, feature_maxs = scaled_data[region]['min'], scaled_data[region]['max']\n",
    "                feature_ranges = feature_maxs - feature_mins\n",
    "                original_coefs = coefs / feature_ranges\n",
    "                intercept = model.intercept_ - np.sum(original_coefs * feature_mins / feature_ranges)\n",
    "                coeffs_list.append(np.concatenate(coefs))\n",
    "        else:\n",
    "            for region, coefs in regression_coeffs.items():\n",
    "                coeffs_list.append(np.concatenate(coefs))\n",
    "    else:\n",
    "        for region, coefs in regression_coeffs.items():\n",
    "            coeffs_list.append(coefs)\n",
    "\n",
    "    coeffs_df = pd.DataFrame(coeffs_list, index=regression_coeffs.keys(), columns=predictor_vars)\n",
    "\n",
    "    return coeffs_df, regression_models, train_data, test_data, cv_scores, performance_metrics, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701e7f7-9d2d-4484-8e00-bf58e94bcf0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define data and predictor variables\n",
    "predictor_vars = ['tas','pr', 'vpd', 'mrro', 'tran', 'evapo', 'mrso','lai', 'gpp'] #'pr',  'gpp' 'evspsbl' \n",
    "predictant = 'bgws'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78588bf1-830e-4a76-bebd-5cdf1d15e2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coeffs_df_lr, linear_regression_models, best_hyperparams_lr, best_scores_lr, train_data_lr, test_data_lr, cv_scores_r2_lr, cv_scores_mse_lr, cv_scores_r2_train_lr, cv_scores_mse_train_lr, cv_scores_r2_test_lr, cv_scores_mse_test_lr, performance_metrics_test_lr, performance_metrics_train_lr, permutation_importances_test_lr, permutation_importances_train_lr, scaled_data_lr = lr_models(ds, predictor_vars, predictant, scaling_method='no_scaling', regression_type='elasticnet', cv_folds=4, scaling_back=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37920169-3c67-4034-b971-8b31dc68ccae",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520d941-733b-43ae-8a1d-4ad1b4a0467f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_performance_metrics(performance_metrics_train_lr, performance_metrics_test_lr, cv_scores_r2_lr, cv_scores_mse_lr, cv_scores_r2_train_lr, cv_scores_mse_train_lr, cv_scores_r2_test_lr, cv_scores_mse_test_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dec3d9-7209-44f0-90f7-77b65a12d33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_permutation_importances(permutation_importances_test_lr, permutation_importances_train_lr, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd4d1b-00f3-4168-a049-7b29bd47a325",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4d244-ca3c-4b6d-be65-4f7ae8cdc2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coefficients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168da516-836e-4c78-b824-72a961cb80b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coefficients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2a1da-bb4a-4442-b566-71cf5fcfcf7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coefficients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de8020-d7f1-4f82-b9aa-ef9579aa7c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coefficients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230da56-c6ea-4bf5-b826-76c75a79e3e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294bb34-6542-44aa-9d0f-d0aa04792257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "def test_regression_assumptions_scikit(regression_models, test_data, predictor_vars):\n",
    "    results = []\n",
    "\n",
    "    for region_name in regression_models:\n",
    "        model = regression_models[region_name]\n",
    "        X_test, y_test = test_data[region_name]\n",
    "\n",
    "        # Predict and calculate residuals\n",
    "        predictions = model.predict(X_test)\n",
    "        residuals = y_test - predictions\n",
    "\n",
    "        # Add a constant term for the Breusch-Pagan test\n",
    "        X_test_with_constant = np.column_stack((np.ones(X_test.shape[0]), X_test))\n",
    "\n",
    "        # Test for normality of residuals\n",
    "        shapiro_stat, shapiro_p = shapiro(residuals)\n",
    "\n",
    "        # Test for homoscedasticity\n",
    "        _, _, _, bp_pvalue = het_breuschpagan(residuals, X_test_with_constant)\n",
    "\n",
    "        # VIF for multicollinearity\n",
    "        vif = [variance_inflation_factor(X_test, i) for i in range(X_test.shape[1])]\n",
    "\n",
    "        # Prepare plot data\n",
    "        plot_data = {\n",
    "            'Region': region_name,\n",
    "            'Predictions': predictions,\n",
    "            'Residuals': residuals,\n",
    "            'Shapiro-Wilk': shapiro_stat,\n",
    "            'Shapiro-Wilk p-value': shapiro_p,\n",
    "            'Breusch-Pagan p-value': bp_pvalue,\n",
    "            'VIF': vif\n",
    "        }\n",
    "        results.append(plot_data)\n",
    "    \n",
    "    # Plotting\n",
    "    num_regions = len(results)\n",
    "    fig, axs = plt.subplots(num_regions, 3, figsize=(22, 5 * num_regions)) # Changed to 3 subplots for VIF\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        sns.residplot(x=result['Predictions'], y=result['Residuals'], lowess=True, ax=axs[i, 0])\n",
    "        axs[i, 0].set_title(f'Residuals vs Predictions for {result[\"Region\"]}')\n",
    "        axs[i, 0].set_xlabel('Predicted values')\n",
    "        axs[i, 0].set_ylabel('Residuals')\n",
    "\n",
    "        # Adding text for statistical tests\n",
    "        axs[i, 0].text(0.05, 0.95, f\"Shapiro-Wilk: {result['Shapiro-Wilk']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "        axs[i, 0].text(0.05, 0.90, f\"Shapiro-Wilk p-value: {result['Shapiro-Wilk p-value']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "        axs[i, 0].text(0.05, 0.85, f\"Breusch-Pagan p-value: {result['Breusch-Pagan p-value']:.2f}\", transform=axs[i, 0].transAxes)\n",
    "\n",
    "        sns.histplot(result['Residuals'], kde=True, ax=axs[i, 1])\n",
    "        axs[i, 1].set_title(f'Residual Distribution for {result[\"Region\"]}')\n",
    "        axs[i, 1].set_xlabel('Residuals')\n",
    "        axs[i, 1].set_ylabel('Frequency')\n",
    "\n",
    "        # VIF bar plot\n",
    "        sns.barplot(x=predictor_vars, y=result['VIF'], ax=axs[i, 2])\n",
    "        axs[i, 2].hlines(5, xmin=-0.5, xmax=len(result['VIF'])-0.5, colors='orange', linestyles='dashed')\n",
    "        axs[i, 2].hlines(10, xmin=-0.5, xmax=len(result['VIF'])-0.5, colors='r', linestyles='dashed')\n",
    "        axs[i, 2].set_title(f'VIF for {result[\"Region\"]}')\n",
    "        axs[i, 2].set_xlabel('Predictor Variables')\n",
    "        axs[i, 2].set_ylabel('VIF Value')\n",
    "        axs[i, 2].set_ylim(0, max(result['VIF']) + 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b5be9-7f5c-4e04-871c-ac4658adfc37",
   "metadata": {
    "tags": []
   },
   "source": [
    "Linearity: Random pattern without any discernible pattern --> non-linearity\n",
    "\n",
    "Independence of Errors: Durbin-Watson values between 1.5 and 2.5 are relatively normal\n",
    "\n",
    "Homoscedasticity (Equal Variance of Errors):  Variance of the residuals is consistent across all levels of the predicted values +\n",
    "                                              Breusch-Pagan test: A small p-value (typically <= 0.05)  suggests heteroscedasticity, which can invalidate                                                   some of the statistical conclusions of the regression.\n",
    "                                              \n",
    "Normality of Errors: Shapiro-Wilk test: The closer this value is to 1, the more the residuals follow a normal distribution. \n",
    "                     Sapiro-Wilk p-value: The p-value from the Shapiro-Wilk test. A small p-value (typically <= 0.05) indicates that                        the residuals do not follow a normal distribution.\n",
    "                     \n",
    "Variance Inflation Factor (VIF): Measures multicollinearity among the independent variables in the regression model. A VIF value                                        greater than 10 is typically considered an indicator of serious multicollinearity that could affect                                    the model's estimates.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0b0f4-5140-4b39-acfe-f40b8465190c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assumptions_df = test_regression_assumptions_scikit(regression_models, test_data, predictor_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739efcc9-d79d-48c1-b8eb-823cc66ad9d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54190025-097b-495b-96e4-a240ea3a7450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_model_performance_all_regions(ds_change, ds_hist, regression_models, test_data, performance_metrics, predictor_vars, residuals, cv_scores):\n",
    "    region_names = ds_change.names.values\n",
    "    region_indices = ds_change.region.values\n",
    "    \n",
    "    num_regions = len(region_names)\n",
    "    plots_per_region = 3  # Number of plots per region\n",
    "    total_plots = num_regions * plots_per_region\n",
    "    cols = 3  # Number of columns (one for each type of plot)\n",
    "    rows = math.ceil(total_plots / cols)  # Calculate the total number of rows needed\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(20, rows * 5), squeeze=False)\n",
    "\n",
    "    plot_idx = 0  # Initialize plot index\n",
    "    for region in region_indices:\n",
    "        region_name = ds_change.names.sel(region=region).values.item()\n",
    "        X_test, y_test = test_data[region_name]\n",
    "        model = regression_models[region_name]\n",
    "        y_pred = model.predict(X_test)\n",
    "        resids = residuals[region_name]\n",
    "\n",
    "        mse_value = performance_metrics[region_name]['MSE']\n",
    "        r2_value = performance_metrics[region_name]['R2']\n",
    "        cv_score = np.mean(cv_scores[region_name])  # Average CV score\n",
    "\n",
    "        # Plot 1: Residuals\n",
    "        ax_resid = axs[plot_idx // cols, plot_idx % cols]\n",
    "        ax_resid.scatter(y_test, resids, c='blue', alpha=0.5, s=10, label='Residuals')\n",
    "        ax_resid.axhline(0, color='red', lw=2, label='Zero Residual')\n",
    "        ax_resid.set_xlabel('True Values')\n",
    "        ax_resid.set_ylabel('Residuals')\n",
    "        ax_resid.set_title(f\"{region_name} - Residuals\")\n",
    "        ax_resid.legend()\n",
    "        plot_idx += 1\n",
    "\n",
    "        # Plot 2: Predictions vs True Values\n",
    "        ax_pred = axs[plot_idx // cols, plot_idx % cols]\n",
    "        ax_pred.scatter(y_test, y_pred, c='green', alpha=0.5, s=10, label='Predictions')\n",
    "        ax_pred.plot(y_test, y_test, color='orange', label='Ideal Prediction')\n",
    "        ax_pred.set_xlabel('True Values')\n",
    "        ax_pred.set_ylabel('Predicted Values')\n",
    "        ax_pred.set_title(f\"{region_name} - Predictions\")\n",
    "        ax_pred.legend()\n",
    "        plot_idx += 1\n",
    "\n",
    "        # Plot 3: Density Plot\n",
    "        ax_density = axs[plot_idx // cols, plot_idx % cols]\n",
    "        sns.kdeplot(y_test, ax=ax_density, label='Actual Values', fill=True)\n",
    "        sns.kdeplot(y_pred, ax=ax_density, label='Predicted Values', fill=True)\n",
    "        ax_density.set_xlabel('Values')\n",
    "        ax_density.set_title(f\"{region_name} - Density Plot\")\n",
    "        ax_density.legend()\n",
    "        plot_idx += 1\n",
    "\n",
    "        # Include performance metrics in the text or title\n",
    "        ax_density.text(0.05, 0.95, f'MSE: {mse_value:.6f}\\nR^2: {r2_value:.2f}\\nCV: {cv_score:.2f}', transform=ax_density.transAxes, verticalalignment='top')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f195a9-76c9-44b0-86d8-a4de4b9cab52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# relative change scaled\n",
    "assess_model_performance_all_regions(ds, ds_hist_ensemble_metric, regression_models, test_data, performance_metrics, predictor_vars, residuals, cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fe453-5ea1-48f6-840f-23c70db2fbd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assess_model_performance_all_regions(ds, ds_hist_ensemble_metric, regression_models, test_data, performance_metrics, predictor_vars, residuals, cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd280fe-f3ee-4897-89d8-efa39f1fc9de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assess_model_performance_all_regions(ds, ds_hist_ensemble_metric, regression_models, test_data, performance_metrics, predictor_vars, residuals, cv_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20765f-92f2-446a-8b9e-364243a61bd0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test optimal variable subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4446c6-b0ce-473f-a625-da48b9ac8f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef977dfd-2fde-4c63-bdb7-70ec7c241c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_combinations = list(all_subsets(predictor_vars)) \n",
    "print(f\"Number of variable combinations with at least 4 variables: {len(all_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a18c3-2755-49f6-8409-be67142163b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def all_subsets(ss, min_length=4):\n",
    "    \"\"\"Generate all combinations of the elements in `ss` with a minimum length of `min_length`.\"\"\"\n",
    "    return chain(*map(lambda x: combinations(ss, x), range(min_length, len(ss)+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df62cd-c63b-4ad1-91d0-ea3a0ad34d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_variable_combinations(ds, all_vars, predictant):\n",
    "    # Get all possible combinations of predictor variables with at least 4 variables\n",
    "    all_combinations = list(all_subsets(all_vars))\n",
    "    print(f\"Number of variable combinations: {len(all_combinations)}\")\n",
    "\n",
    "    # Prepare a list to store the performance metrics\n",
    "    performance_list = []\n",
    "\n",
    "    for combination in all_combinations:\n",
    "        # Convert the tuple to a list of variables for this combination\n",
    "        current_vars = list(combination)\n",
    "\n",
    "        # Train the models using the current combination of variables\n",
    "        _, regression_models, _, _, _, _ = train_multivariate_models(ds, current_vars, predictant)\n",
    "       \n",
    "        # Assess model performance for all regions\n",
    "        for region in ds.region.values:\n",
    "            region_name = ds.names.sel(region=region).values.item()\n",
    "            \n",
    "            # Prepare the data for the region\n",
    "            df = ds.sel(region=region).to_dataframe().dropna()\n",
    "            X = df[current_vars]\n",
    "            y_true = df[predictant].values\n",
    "\n",
    "            # Standardize the predictor variables\n",
    "            scaler = StandardScaler()\n",
    "            X_standardized = scaler.fit_transform(X) # standarized using this method might be wrong cause we need the direction. Standarize \n",
    "            #with mean \n",
    "            \n",
    "            # Predict using the trained model\n",
    "            model = regression_models[region_name]\n",
    "            y_pred = model.predict(X_standardized)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            \n",
    "            # Append the performance metrics for this region and variable combination to the list\n",
    "            performance_list.append({\n",
    "                'Variables': ', '.join(current_vars),\n",
    "                'MSE': mse,\n",
    "                'R^2': r2\n",
    "            })\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    performance_summary = pd.DataFrame(performance_list)\n",
    "    \n",
    "    # Calculate the aggregated performance for each variable combination across all regions\n",
    "    aggregated_performance = performance_summary.groupby('Variables').agg(['mean', 'min', 'max']).reset_index()\n",
    "    aggregated_performance.columns = [' '.join(col).strip() for col in aggregated_performance.columns.values]\n",
    "\n",
    "    # Sort the results by Mean MSE and Mean R^2\n",
    "    aggregated_performance.sort_values(by=['MSE mean', 'R^2 mean'], ascending=[True, False], inplace=True)\n",
    "\n",
    "    # Select and rename the columns to only include the required statistics\n",
    "    final_table = aggregated_performance[['Variables', 'MSE mean', 'MSE min', 'MSE max', 'R^2 mean', 'R^2 min', 'R^2 max']]\n",
    "    final_table.rename(columns={\n",
    "        'MSE mean': 'Mean MSE',\n",
    "        'MSE min': 'Min MSE',\n",
    "        'MSE max': 'Max MSE',\n",
    "        'R^2 mean': 'Mean R^2',\n",
    "        'R^2 min': 'Min R^2',\n",
    "        'R^2 max': 'Max R^2'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a331d62-6de2-41bf-99b5-45dc25417f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "pd.set_option('display.max_rows', None)  \n",
    "test_variable_combinations(ds, predictor_vars, 'bgws')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9cb559-e022-4ce0-9729-864ebc2a3ced",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23b317-affe-4764-a6a9-7109b463ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_df*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccddc8ef-a7e1-4fcb-bf29-1d75cc216562",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Cluster regions based on regression coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8df60-5047-4cbb-9413-f6661a0850d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def cluster_regions(coefficients_df, max_clusters=40):\n",
    "    \"\"\"\n",
    "    This function finds the optimal number of clusters using the elbow method\n",
    "    and clusters regions based on their coefficients.\n",
    "    \n",
    "    :param coefficients_df: A pandas DataFrame containing the coefficients with regions as the index.\n",
    "    :param max_clusters: The maximum number of clusters to test for the elbow method.\n",
    "    :return: A tuple of pandas DataFrames with the first containing the coefficients and an additional\n",
    "             column 'Cluster', and the second containing the centroids of each cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the optimal number of clusters using the elbow method\n",
    "    sum_of_squared_distances = []\n",
    "    K = range(1, max_clusters + 1)\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k,  n_init=10, random_state=42)\n",
    "        km = km.fit(coefficients_df)\n",
    "        sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "    # Plot the elbow curve\n",
    "    plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum of squared distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    # Ask user for the optimal number of clusters\n",
    "    n_clusters = int(input(\"Enter the optimal number of clusters: \"))\n",
    "    \n",
    "    # Use the KMeans algorithm to find clusters with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    kmeans.fit(coefficients_df)\n",
    "\n",
    "    # Assign the clusters to each region\n",
    "    clusters = kmeans.labels_\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Calculate the distance of each region's coefficients to the centroid of its cluster\n",
    "    distances_to_centroid = cdist(coefficients_df, centroids, 'euclidean')\n",
    "    min_distances = distances_to_centroid.min(axis=1)\n",
    "\n",
    "    # Add the cluster labels and distances to the DataFrame\n",
    "    df = coefficients_df.copy()\n",
    "    df['Cluster'] = clusters\n",
    "    df['Distance_to_Centroid'] = (min_distances * 100).round(decimals=2)\n",
    "    df.index.name = 'Region'\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Order the dataframe by cluster label and distance to centroid\n",
    "    clustered_df = df.sort_values(['Cluster', 'Distance_to_Centroid'])\n",
    "\n",
    "    # Get the centroids\n",
    "    centroids_df = pd.DataFrame(centroids, columns=coefficients_df.columns)\n",
    "    centroids_df['Cluster'] = range(n_clusters)\n",
    "\n",
    "    return clustered_df, centroids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9390a-3f10-4f89-b5da-97baaf850f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_regions(coefficients_df, max_clusters=40):\n",
    "    \"\"\"\n",
    "    This function finds the optimal number of clusters using the elbow method\n",
    "    and clusters regions based on their coefficients.\n",
    "\n",
    "    :param coefficients_df: A pandas DataFrame containing the coefficients with regions as the index.\n",
    "    :param max_clusters: The maximum number of clusters to test for the elbow method.\n",
    "    :return: A pandas DataFrame with an additional column 'Cluster' indicating the cluster for each region.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the optimal number of clusters using the elbow method\n",
    "    sum_of_squared_distances = []\n",
    "    K = range(1, max_clusters + 1)\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k,  n_init=10, random_state=42)\n",
    "        km = km.fit(coefficients_df)\n",
    "        sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "    # Plot the elbow curve\n",
    "    plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum of squared distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    # Ask user for the optimal number of clusters\n",
    "    n_clusters = int(input(\"Enter the optimal number of clusters: \"))\n",
    "    \n",
    "    # Use the KMeans algorithm to find clusters with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    kmeans.fit(coefficients_df)\n",
    "\n",
    "    # Assign the clusters to each region\n",
    "    clusters = kmeans.labels_\n",
    "\n",
    "    # Add the cluster labels to the DataFrame\n",
    "    df = coefficients_df.copy()\n",
    "    df['Cluster'] = clusters\n",
    "    df.index.name = 'Region'\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Order the dataframe by cluster label\n",
    "    clustered_df = df.sort_values('Cluster')\n",
    "\n",
    "    return clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbe0b8-664c-48cf-bcab-6b7c784f39e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "def custom_distance(u, v):\n",
    "    # Custom distance metric:\n",
    "    # If any signs differ, the distance is increased\n",
    "    sign_diff = np.sign(u) != np.sign(v)\n",
    "    if sign_diff.any():  # If any signs differ, apply the penalty\n",
    "        return np.linalg.norm(u - v) * 1\n",
    "    else:\n",
    "        return np.linalg.norm(u - v)\n",
    "    \n",
    "def cluster_regions(coefficients_df, n_clusters=12):\n",
    "    # Create a new DataFrame for clustering to avoid modifying the original data\n",
    "    X = coefficients_df.copy()\n",
    "\n",
    "    # Compute the custom distance matrix\n",
    "    dist_matrix = distance_matrix(X.values, X.values, p=2)\n",
    "    for i in range(dist_matrix.shape[0]):\n",
    "        for j in range(dist_matrix.shape[1]):\n",
    "            if i != j:  # No need to penalize the diagonal\n",
    "                dist_matrix[i, j] = custom_distance(X.values[i], X.values[j])\n",
    "\n",
    "    # Perform Agglomerative Clustering with the precomputed distance matrix\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, affinity='precomputed', linkage='complete')\n",
    "    clusters = clustering.fit_predict(dist_matrix)\n",
    "\n",
    "    # Assign the clusters to each region\n",
    "    coefficients_df['Cluster'] = clusters\n",
    "    \n",
    "    # Order the dataframe by cluster label\n",
    "    clustered_df = coefficients_df.sort_values('Cluster')\n",
    "    clustered_df.index.name = 'Region'\n",
    "    clustered_df.reset_index(inplace=True)\n",
    "    \n",
    "    return clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1060349-0757-4abb-9b5f-c8450a86daec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_df, centroids_df = cluster_regions(coefficients_df)\n",
    "clustered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b338d45-d31e-4f18-9a27-5e6bbdcee56b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot clusters on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac1398-83d2-405a-a467-9b8ba965f461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "colors = [(34/255, 139/255, 34/255), (1, 1, 1), (60/255, 145/255, 230/255)]  # Green -> White -> Blue\n",
    "n_bins = [3]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom_div_cmap'\n",
    "\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)\n",
    "\n",
    "# To test and display the colormap\n",
    "fig, ax = plt.subplots(figsize=(6, 1))\n",
    "ax.set_title('Custom Diverging Colormap')\n",
    "plt.imshow(np.linspace(0, 1, 256).reshape(1, -1), aspect='auto', cmap=cm)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6bb7a-ed6a-4c0f-9566-beb8f4af6cad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import ShapelyFeature\n",
    "import regionmask\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956d530-fad6-429d-9af7-17147c713db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "def split_polygon(polygon, meridian=180):\n",
    "    \"\"\"\n",
    "    Splits a Shapely polygon into two polygons at a specified meridian\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    if maxx > meridian and minx < -meridian:\n",
    "        # Polygon crosses the antimeridian\n",
    "        left_poly = []\n",
    "        right_poly = []\n",
    "        for x, y in polygon.exterior.coords:\n",
    "            if x >= meridian:\n",
    "                right_poly.append((x - 360, y))  # Wraparound for the right side\n",
    "            else:\n",
    "                left_poly.append((x, y))\n",
    "        return [Polygon(left_poly), Polygon(right_poly)]\n",
    "    else:\n",
    "        return [polygon]  # Wrap the single polygon in a list for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da892dc0-63f5-4868-8d2f-c222339ab036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_clusters_and_regions(ds_ensmed_glob, ds, clustered_df):\n",
    "    # Initialize the plot with a cartopy projection\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "    ax_main = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "    \n",
    "    # Plot the 'bgws' variable from the dataset\n",
    "    ds_ensmed_glob['bgws'].plot(ax=ax_main, vmin=-0.2, vmax=0.2, cmap=cm, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "    \n",
    "    # Add coastlines and gridlines\n",
    "    ax_main.coastlines()\n",
    "    ax_main.tick_params(axis='both', which='major', labelsize=20)\n",
    "    gridlines = ax_main.gridlines(draw_labels=True, color='black', alpha=0.2, linestyle='--')\n",
    "    gridlines.top_labels = gridlines.right_labels = False\n",
    "    gridlines.xlabel_style = {'size': 18}\n",
    "    gridlines.ylabel_style = {'size': 18}\n",
    "    \n",
    "    # Get region bounds using regionmask\n",
    "    land_regions = regionmask.defined_regions.ar6.land\n",
    "    \n",
    "    # Create a mapping from region names to region numbers\n",
    "    region_name_to_number = dict(zip(ds.names.values, ds.region.values))\n",
    "    \n",
    "    # Map the region names to the same order as ds.names.values\n",
    "    region_to_cluster_map = dict(zip(clustered_df['Region'], clustered_df['Cluster']))\n",
    "\n",
    "    # Now create a new column in ds that maps the region names to cluster numbers\n",
    "    # This assumes ds.names.values has the same region names as in clustered_df['Region']\n",
    "    ds['Cluster'] = [region_to_cluster_map[name] for name in ds.names.values]\n",
    "\n",
    "    # Convert the 'Cluster' DataArray to a NumPy array and get unique values\n",
    "    unique_clusters = np.unique(ds['Cluster'].values)\n",
    "\n",
    "    # Prepare colors for clusters - this assumes a finite number of clusters\n",
    "    cluster_colors = plt.cm.tab20b(np.linspace(0, 1, len(unique_clusters)))\n",
    "\n",
    "\n",
    "    # Loop over the regions and plot the cluster numbers with abbreviations\n",
    "    for region_name, cluster_number in zip(ds.names.values, ds['Cluster']):\n",
    "        reg_num = region_name_to_number[region_name]\n",
    "        region_polygons = land_regions[reg_num].polygon\n",
    "        \n",
    "        region_abbr = ds.abbrevs.values[ds.region.values == reg_num][0]  # Assuming this gives us the correct abbreviation\n",
    "        cluster_color = cluster_colors[cluster_number]  # Get the color for the cluster\n",
    "        \n",
    "        # Fetch the polygon or polygons for this region\n",
    "        region_obj = land_regions[reg_num]\n",
    "        if hasattr(region_obj, 'polygons'):\n",
    "            # If the attribute is 'polygons', we assume it's iterable (e.g., a list of Polygon objects)\n",
    "            region_polygons = region_obj.polygons\n",
    "        elif hasattr(region_obj, 'polygon'):\n",
    "            # If there's only one Polygon, we wrap it in a list to make it iterable\n",
    "            region_polygons = [region_obj.polygon]\n",
    "        else:\n",
    "            raise AttributeError(f\"The region object does not have 'polygons' or 'polygon' attribute.\")\n",
    "\n",
    "        for region_polygon in region_polygons:\n",
    "            # If the polygon crosses the antimeridian, split it\n",
    "            split_polys = split_polygon(region_polygon)\n",
    "\n",
    "            # Handle both Polygon and MultiPolygon types after splitting\n",
    "            for poly in split_polys:\n",
    "                if isinstance(poly, Polygon):\n",
    "                    features_to_plot = [poly]\n",
    "                elif isinstance(poly, MultiPolygon):\n",
    "                    features_to_plot = list(poly.geoms)\n",
    "                else:\n",
    "                    raise TypeError(f\"Unhandled geometry type: {type(poly)}\")\n",
    "\n",
    "                for feature_poly in features_to_plot:\n",
    "                    feature = ShapelyFeature([feature_poly], ccrs.PlateCarree(), edgecolor=cluster_color, facecolor='none', linewidth=2)\n",
    "                    ax_main.add_feature(feature)\n",
    "\n",
    "            # Calculate the centroid for text placement using features_to_plot\n",
    "            centroids = [feature_poly.centroid for feature_poly in features_to_plot]\n",
    "    \n",
    "            text_lon, text_lat = max(centroids, key=lambda c: c.x).coords[0]  # Use the easternmost centroid\n",
    "        \n",
    "            # Ensure cluster_number is a plain integer if it's a single-item array or DataArray\n",
    "            if isinstance(cluster_number, np.ndarray) and cluster_number.size == 1:\n",
    "                cluster_number = cluster_number.item()  # Converts a one-element array to a scalar\n",
    "            elif isinstance(cluster_number, xr.DataArray) and cluster_number.ndim == 0:\n",
    "                cluster_number = cluster_number.values.item()  # Gets the scalar value from a 0-dim DataArray\n",
    "\n",
    "            # Annotate the cluster number for each region\n",
    "            ax_main.text(text_lon, text_lat, f\"{region_abbr}\\n{cluster_number}\",\n",
    "                         horizontalalignment='center', verticalalignment='center', transform=ccrs.PlateCarree(),\n",
    "                         fontsize=20, bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71db8b-8bee-4dd8-bf6c-15673089129a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_clusters_and_regions(ds_ensmed_glob, ds, clustered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03c495-fab5-4094-8cf2-b66258c891eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a242050-5cba-4f75-b7f6-4f9e9fb1a321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_simple_map(ds_ensmed_glob, projection, save_fig=False):\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "    ax_main = fig.add_subplot(1, 1, 1, projection=projection)\n",
    "    \n",
    "    # Plot the 'bgws' variable from the dataset\n",
    "    img = ds_ensmed_glob['bgws'].plot(ax=ax_main, vmin=-0.6, vmax=0.6,  cmap=cm, transform=ccrs.PlateCarree(), add_colorbar=False) \n",
    "    \n",
    "    # Add coastlines and gridlines\n",
    "    ax_main.coastlines()\n",
    "    ax_main.tick_params(axis='both', which='major', labelsize=20)\n",
    "    gridlines = ax_main.gridlines(draw_labels=True, color='black', alpha=0.2, linestyle='--')\n",
    "    gridlines.top_labels = gridlines.right_labels = False\n",
    "    gridlines.xlabel_style = {'size': 18}\n",
    "    gridlines.ylabel_style = {'size': 18}\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar_ax = fig.add_axes([0.314, 0.1, 0.4, 0.025]) #left, bottom, width, height\n",
    "    cbar = fig.colorbar(img, cax=cbar_ax, extend='both', orientation='horizontal')\n",
    "    cbar.set_label(\"Blue-Green Water Share\", fontsize=22, weight='bold', labelpad=15) \n",
    "    cbar.ax.tick_params(labelsize=18)\n",
    "   \n",
    "    plt.show()\n",
    "    \n",
    "    # Safe figure\n",
    "    if save_fig:\n",
    "        savepath = os.path.join('../..', 'results', 'CMIP6', 'historical', 'time', 'median', 'bgws')\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        filename = f'Ensemble_median.1985-2014.bgws.historical.pdf'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        fig.savefig(filepath, dpi=600, bbox_inches='tight', format='pdf')\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830683f-e43c-47f4-8feb-b877525d64e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_simple_map(ds_ensmed_glob, ccrs.Robinson(), save_fig=True) # Robinson PlateCarree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890934ec-5181-4729-b988-5b7ac0321d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_simple_map(ds_ensmed_glob, ccrs.PlateCarree(), save_fig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449211a-b442-4503-a52b-98fc8ee19228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d4e8ecb-11c5-41a6-8818-e99123f72e1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare Permutation Importance of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd8172-1b1f-4bc1-8c7b-b9ff32b52d68",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Compare LR test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5b647-7ce1-429d-b450-7bc710888ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_permutation_importance(permutation_importances_train, permutation_importances_test, performance_metrics_train, performance_metrics_test, predictor_vars):\n",
    "    comparison_results = {}\n",
    "\n",
    "    for region in permutation_importances_train.keys():\n",
    "        mean_importance_train = pd.Series(permutation_importances_train[region].mean(axis=1), index=predictor_vars)\n",
    "        mean_importance_test = pd.Series(permutation_importances_test[region].mean(axis=1), index=predictor_vars)\n",
    "\n",
    "        # Rank the variables, excluding negative importances\n",
    "        rank_train = mean_importance_train.rank(method='dense', ascending=False).where(mean_importance_train >= 0, np.nan)\n",
    "        rank_test = mean_importance_test.rank(method='dense', ascending=False).where(mean_importance_test >= 0, np.nan)\n",
    "\n",
    "        # Special rank for zero importance in both training and test\n",
    "        zero_importance = (mean_importance_train == 0) & (mean_importance_test == 0)\n",
    "        max_rank = max(rank_train.max(), rank_test.max()) + 1\n",
    "        rank_train[zero_importance] = max_rank\n",
    "        rank_test[zero_importance] = max_rank\n",
    "\n",
    "        # Calculate agreement\n",
    "        valid_indices = (mean_importance_train >= 0) | (mean_importance_test >= 0)\n",
    "        agreement = np.mean(rank_train[valid_indices] == rank_test[valid_indices]) * 100\n",
    "\n",
    "        # Prepare data for dataframe\n",
    "        data = {\n",
    "            'Variable': predictor_vars,\n",
    "            'Train_Rank': rank_train,\n",
    "            'Test_Rank': rank_test,\n",
    "            'Train_Mean_Importance': mean_importance_train,\n",
    "            'Test_Mean_Importance': mean_importance_test,\n",
    "            'Train_R2': performance_metrics_train[region]['R2'],\n",
    "            'Test_R2': performance_metrics_test[region]['R2'],\n",
    "            'Agreement (%)': agreement\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(data).sort_values(by='Train_Rank')\n",
    "        comparison_results[region] = df\n",
    "\n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6841e-4031-4f58-be5f-27bafd7f8711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_permutation_importance(permutation_importances_train, permutation_importances_test, performance_metrics_train, performance_metrics_test, predictor_vars, zero_threshold=0.001):\n",
    "    comparison_results = {}\n",
    "\n",
    "    for region in permutation_importances_train.keys():\n",
    "        mean_importance_train = pd.Series(permutation_importances_train[region].mean(axis=1), index=predictor_vars)\n",
    "        mean_importance_test = pd.Series(permutation_importances_test[region].mean(axis=1), index=predictor_vars)\n",
    "\n",
    "        # Separate zero and non-zero importance variables\n",
    "        is_zero_train = mean_importance_train.abs() <= zero_threshold\n",
    "        is_zero_test = mean_importance_test.abs() <= zero_threshold\n",
    "\n",
    "        # Rank the variables, treating near-zero importance variables separately\n",
    "        rank_train = mean_importance_train.rank(method='dense', ascending=False, na_option='bottom').astype('Int64')\n",
    "        rank_test = mean_importance_test.rank(method='dense', ascending=False, na_option='bottom').astype('Int64')\n",
    "\n",
    "        # Adjust ranks for zero importance variables\n",
    "        max_rank = max(rank_train.max(), rank_test.max()) + 1\n",
    "        rank_train[is_zero_train & is_zero_test] = max_rank\n",
    "        rank_test[is_zero_train & is_zero_test] = max_rank\n",
    "\n",
    "        # Calculate agreement\n",
    "        agreement = np.mean(rank_train == rank_test) * 100\n",
    "\n",
    "        # Prepare data for dataframe\n",
    "        data = {\n",
    "            'Variable': predictor_vars,\n",
    "            'Train_Rank': rank_train,\n",
    "            'Test_Rank': rank_test,\n",
    "            'Train_Mean_Importance': mean_importance_train,\n",
    "            'Test_Mean_Importance': mean_importance_test,\n",
    "            'Train_R2': performance_metrics_train[region]['R2'],\n",
    "            'Test_R2': performance_metrics_test[region]['R2'],\n",
    "            'Agreement (%)': agreement\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(data).sort_values(by='Train_Rank')\n",
    "        comparison_results[region] = df\n",
    "\n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27797b-f7a8-4706-9f7d-f8bb4a8ce2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "comparison_results = compare_permutation_importance(permutation_importances_train_lr, permutation_importances_test_lr, performance_metrics_train_lr, performance_metrics_test_lr, predictor_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f09ee-b3eb-455d-9b94-642b27625abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(comparison_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54323b-2015-40c1-815d-a597cbf5ff76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Displaying the results for one region as an example\n",
    "region_number = 8\n",
    "print(list(comparison_results.keys())[region_number])\n",
    "comparison_results[list(comparison_results.keys())[region_number]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bf976-55dc-44c9-959a-e6f650d87d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean overall agreement and r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95257d65-7dc2-4c22-9431-27fd9af6287c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_agreement_metrics(comparison_results):\n",
    "    overall_agreement = []\n",
    "    first_rank_agreement = []\n",
    "\n",
    "    for region, df in comparison_results.items():\n",
    "        # Calculate overall agreement for the region\n",
    "        agreement = df['Agreement (%)'].iloc[0]\n",
    "        overall_agreement.append(agreement)\n",
    "\n",
    "        # Check if the top-ranked variable is the same in training and test data\n",
    "        top_train = df[df['Train_Rank'] == 1.0]['Variable'].iloc[0] if any(df['Train_Rank'] == 1.0) else None\n",
    "        top_test = df[df['Test_Rank'] == 1.0]['Variable'].iloc[0] if any(df['Test_Rank'] == 1.0) else None\n",
    "        first_rank_agreement.append(top_train == top_test)\n",
    "\n",
    "    # Calculate average agreement\n",
    "    avg_overall_agreement = sum(overall_agreement) / len(overall_agreement)\n",
    "    avg_first_rank_agreement = sum(first_rank_agreement) / len(first_rank_agreement) * 100\n",
    "\n",
    "    return avg_overall_agreement, avg_first_rank_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77cea22-a82d-4138-8ab2-8152a86c87db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_overall_agreement, avg_first_rank_agreement = compute_agreement_metrics(comparison_results)\n",
    "print(\"Average Overall Agreement:\", avg_overall_agreement)\n",
    "print(\"Average Agreement on First Rank:\", avg_first_rank_agreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a8b8e-607a-4c89-bf82-abab6ae8990a",
   "metadata": {},
   "source": [
    "##### Compare xgb test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb73b00-5f9f-4e39-b5e0-bc143879a077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "comparison_results_xgb = compare_permutation_importance(permutation_importances_train_xgb, permutation_importances_test_xgb, performance_metrics_train_xgb, performance_metrics_test_xgb, predictor_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7522d9-e451-4858-8f70-44099726bf9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Displaying the results for one region as an example\n",
    "region_number = 15\n",
    "print(list(comparison_results_xgb.keys())[region_number])\n",
    "comparison_results_xgb[list(comparison_results_xgb.keys())[region_number]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e2c6ba-83d6-406c-b456-dd40cc2a9230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_overall_agreement, avg_first_rank_agreement = compute_agreement_metrics(comparison_results_xgb)\n",
    "print(\"Average Overall Agreement:\", avg_overall_agreement)\n",
    "print(\"Average Agreement on First Rank:\", avg_first_rank_agreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1087570-0785-456d-acc5-182c27251f82",
   "metadata": {},
   "source": [
    "##### Compare xgb and lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f6cb3-f735-4d31-b002-ef91db448173",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Build Gaussian Processes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76996ca-c3bf-493f-9910-68861753f0d5",
   "metadata": {},
   "source": [
    "Probabilistic Outputs: GPs not only provide a prediction for each data point but also give a measure of uncertainty (variance) associated with that prediction. This can help in understanding the confidence of the model in different regions of the input space.\n",
    "\n",
    "Non-Linear Relationships: GPs, with the right choice of kernel, can model complex non-linear relationships between inputs and outputs, making them more flexible than traditional linear regression models.\n",
    "\n",
    "Kernel Flexibility: The kernel in a GP defines the relationship between data points. By selecting or designing a kernel that captures the underlying structure of the data, GPs can be adapted to various types of data patterns.\n",
    "\n",
    "Finally, regarding variable importance: In the context of GPs, interpreting variable importance is not as straightforward as in linear regression. However, one common approach is to examine the sensitivity of the GP's predictions to changes in each input variable. The Automatic Relevance Determination (ARD) kernel, for instance, can adapt its length scale for each dimension of the input space, which can give an indication of the relative importance of each input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea48e9e-6c48-442b-8981-68199cac085a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_data_for_gp(ds, region_index):\n",
    "    # Extract data for the given region\n",
    "    region_data = ds.sel(region=region_index)\n",
    "    \n",
    "    # Prepare predictor and target variables\n",
    "    X = region_data[['pr', 'vpd', 'evspsbl', 'mrro', 'mrso', 'tran', 'lai', 'gpp']].to_array().transpose('lat', 'lon', 'variable')\n",
    "    y = region_data['bgws'].stack(z=(\"lat\", \"lon\"))\n",
    "    \n",
    "    # Convert X to a 2D array\n",
    "    X = X.stack(z=(\"lat\", \"lon\")).transpose('z', 'variable').values\n",
    "    \n",
    "    # Create a mask where either X or y has NaN values\n",
    "    mask = ~np.isnan(y) & ~np.any(np.isnan(X), axis=1)\n",
    "    \n",
    "    # Filter out rows using the mask\n",
    "    X_filtered = X[mask, :]\n",
    "    y_filtered = y[mask].values  # Convert to numpy array\n",
    "    \n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "def train_gp_for_region(ds, region_name):\n",
    "    # Prepare data\n",
    "    X, y = prepare_data_for_gp(ds, region_name)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X_standardized = scaler.transform(X)\n",
    "    \n",
    "    # Define the kernel: RBF kernel with ARD + constant term + white noise term for model noise\n",
    "    # Kernel = Covarianzfunction:\n",
    "    # Defines relation between input variables\n",
    "    # \n",
    "    kernel = C(1.0, (1e-3, 1e3)) * RBF(np.ones(8), (1e-2, 1e2)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
    "    \n",
    "    # Initialize and train GP regressor\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, normalize_y=True)\n",
    "    gp.fit(X_standardized, y)\n",
    "    \n",
    "    return gp, scaler\n",
    "\n",
    "def predict_with_gp(gp, scaler, X):\n",
    "    # Standardize the features\n",
    "    X_standardized = scaler.transform(X)\n",
    "    \n",
    "    # Predict using the GP regressor\n",
    "    y_pred, y_std = gp.predict(X_standardized, return_std=True)\n",
    "    \n",
    "    return y_pred, y_std\n",
    "\n",
    "def train_and_predict_for_all_regions(ds):\n",
    "    region_indices = ds.region.values\n",
    "    region_names = ds.names.values\n",
    "    gp_models = {}\n",
    "    scalers = {}\n",
    "    predictions = {}\n",
    "    std_devs = {}\n",
    "    \n",
    "    for i, region_index in enumerate(region_indices):\n",
    "        region_name = region_names[i]\n",
    "        gp, scaler = train_gp_for_region(ds, region_index)\n",
    "        X, y = prepare_data_for_gp(ds, region_index)\n",
    "        \n",
    "        y_pred, y_std = predict_with_gp(gp, scaler, X)\n",
    "        \n",
    "        gp_models[region_name] = gp\n",
    "        scalers[region_name] = scaler\n",
    "        predictions[region_name] = y_pred\n",
    "        std_devs[region_name] = y_std\n",
    "    \n",
    "    return gp_models, scalers, predictions, std_devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28ef89-e109-485e-b4e9-bcc08d62e320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp_models, scalers, predictions, std_devs = train_and_predict_for_all_regions(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0011b-1124-448e-ac82-b97467d1a07f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cc546-460e-45f0-a48d-4586b9ef1428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "def assess_model_performance_all_regions(ds, predictions, std_devs):\n",
    "    region_names = ds.names.values\n",
    "    region_indices = ds.region.values\n",
    "    \n",
    "    # Determine the number of rows and columns for the subplots based on the number of regions\n",
    "    num_regions = len(region_names)\n",
    "    cols = 3  # Assuming you'd like 3 columns\n",
    "    rows = math.ceil(num_regions / cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    \n",
    "    for i, region_index in enumerate(region_indices):\n",
    "        region_name = region_names[i]\n",
    "        \n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        \n",
    "        # Extract true values\n",
    "        _, y_true = prepare_data_for_gp(ds, region_index)\n",
    "        \n",
    "        # Extract predicted values and standard deviations for the region\n",
    "        y_pred = predictions[region_name]\n",
    "        y_std = std_devs[region_name]\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residuals = y_pred - y_true\n",
    "        \n",
    "        # Calculate the RMSE\n",
    "        mse_value = mean_squared_error(y_true, y_pred)\n",
    "        r2_s = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # Plot\n",
    "        ax = axs[row, col]\n",
    "        ax.scatter(y_true, residuals, c='blue', alpha=0.5, s=10)\n",
    "        ax.fill_between(y_true, residuals - y_std, residuals + y_std, color='gray', alpha=0.2)\n",
    "        ax.axhline(0, color='red', lw=2)\n",
    "        ax.text(0.05, 0.95, f'MSE: {mse_value:.6f}', transform=ax.transAxes, verticalalignment='top')\n",
    "        ax.text(0.05, 0.9, f'R2 score: {r2_s:.2f}', transform=ax.transAxes, verticalalignment='top')\n",
    "        ax.set_title(region_name)\n",
    "        ax.set_xlabel('True Values')\n",
    "        ax.set_ylabel('Residuals')\n",
    "        \n",
    "    # Create legend with explicit handles in the last subplot\n",
    "    if num_regions < rows * cols:\n",
    "        last_ax = axs.flatten()[-1]\n",
    "        last_ax.axis('off')\n",
    "        legend_elements = [mlines.Line2D([0], [0], color='blue', marker='o', markersize=10, label='Residuals (Observed - Predicted)', linestyle='None'),\n",
    "                           mlines.Line2D([0], [0], color='gray', alpha=0.2, linewidth=10, label='Prediction Uncertainty (1 Std. Dev.)'),\n",
    "                           mlines.Line2D([0], [0], color='red', lw=2, label='Zero Residual Line')]\n",
    "        last_ax.legend(handles=legend_elements, loc='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee7fb3-e0e6-478a-bf69-07cf13d8299e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "assess_model_performance_all_regions(ds, predictions, std_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc9c73-d454-470a-aa9d-b6f0263815ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119068a-e003-4c1d-be14-bd4ece7f3ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b4713-6a18-44bc-858c-5326c84f2f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5c8f9-d806-4510-817f-0a792542cbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1b0fb-5031-497b-8929-1f6e53f1985e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a75213-ca64-4d5f-8d74-8442e001845a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e7582-e89f-4fc7-9133-9a6e948896ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f566e4-514c-482e-bc54-5574851968bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def assess_variable_importance_all_regions(ds, gp_models, variable_names):\n",
    "    region_names = ds.names.values\n",
    "    region_indices = ds.region.values\n",
    "    \n",
    "    # Determine the number of rows and columns for the subplots based on the number of regions\n",
    "    num_regions = len(region_names)\n",
    "    cols = 3  # Assuming you'd like 3 columns\n",
    "    rows = math.ceil(num_regions / cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    \n",
    "    for i, region_index in enumerate(region_indices):\n",
    "        region_name = region_names[i]\n",
    "        \n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        \n",
    "        ax = axs[row, col]\n",
    "        \n",
    "        # Extract the ARD kernel length scales from the trained GP model for the region\n",
    "        length_scales = gp_models[region_name].kernel_.k1.k2.length_scale\n",
    "        \n",
    "        # Plot\n",
    "        ax.bar(variable_names, length_scales)\n",
    "        ax.set_title(region_name)\n",
    "        ax.set_xlabel('Variable Name')\n",
    "        ax.set_ylabel('Length Scale')\n",
    "        ax.tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better visibility\n",
    "        \n",
    "    # Remove empty subplots\n",
    "    for i in range(num_regions, rows*cols):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a364539-eb02-46a0-a1e2-50de4bca458a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "variable_names = ['pr', 'vpd', 'evspsbl', 'mrro', 'mrso', 'tran', 'lai', 'gpp']\n",
    "\n",
    "assess_variable_importance_all_regions(ds, gp_models, variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9838d6-ae01-4537-8873-3af90f298aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def assess_permutation_importance(ds, gp_models, variable_names, predictions, n_repeats=30):\n",
    "    \"\"\"\n",
    "    Assess the permutation importance of features for all regions.\n",
    "\n",
    "    Parameters:\n",
    "    - ds: Dataset containing region names, indices, and bgws values.\n",
    "    - gp_models: Dictionary containing trained GP models for each region.\n",
    "    - variable_names: List of variable names.\n",
    "    - predictions: Predicted values for each region.\n",
    "    - n_repeats: Number of times to permute a feature.\n",
    "\n",
    "    Returns:\n",
    "    - None. Plots the importance.\n",
    "    \"\"\"\n",
    "    \n",
    "    region_names = ds.names.values\n",
    "    region_indices = ds.region.values\n",
    "    y = ds.bgws.values  # Extracting target values from ds\n",
    "    \n",
    "    # Determine the number of rows and columns for the subplots based on the number of regions\n",
    "    num_regions = len(region_names)\n",
    "    cols = 3  # Assuming you'd like 3 columns\n",
    "    rows = math.ceil(num_regions / cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    \n",
    "    for region_name in region_names:\n",
    "        region_index = (ds['region'].values == region_name)\n",
    "\n",
    "        # Assuming ds has columns for each predictor variable and they are named consistently\n",
    "        # Extract all predictor variables for this region\n",
    "        X_region = ds[variable_names][region_index]\n",
    "\n",
    "        y_region = y[region_index]  \n",
    "        result = permutation_importance(gp_models[region_name], X_region, y_region, n_repeats=n_repeats)\n",
    "\n",
    "        # Sort variables by importance\n",
    "        sorted_idx = result.importances_mean.argsort()\n",
    "        \n",
    "        # Plot\n",
    "        ax.boxplot(result.importances[sorted_idx].T,\n",
    "                   vert=False, labels=np.array(variable_names)[sorted_idx])\n",
    "        ax.set_title(region_name)\n",
    "        ax.set_xlabel('Importance Score')\n",
    "        \n",
    "    # Remove empty subplots\n",
    "    for i in range(num_regions, rows*cols):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460149fd-d33d-4757-a98e-b5b2c54f8460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "assess_permutation_importance(ds, gp_models, variable_names, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed10b1-abad-4a67-8141-3058bbc83928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "mypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
