{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7716ffb9-a564-44c7-87d4-2d4caeecdb1c",
   "metadata": {},
   "source": [
    "# CMIP6 Neural Network Analysis\n",
    "\n",
    "**Following steps are included in this script:**\n",
    "\n",
    "1. Load netCDF files\n",
    "2. Preprocess data\n",
    "3. Initilize and train model\n",
    "4. Test performance and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ccded-3573-4198-b51a-dc279c553d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Packages ==========\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import dask\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a2969b-0f22-48a7-929e-4ce24b26d2f4",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa58cf-08f4-428e-aa46-f6051ad4d811",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba447d40-873b-4dc2-b076-1ec50cadf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Helper function to open the dataset ========\n",
    "def open_dataset(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    return ds\n",
    "\n",
    "# Define a helper function to open and merge datasets\n",
    "def open_and_merge_datasets(folder, model, experiment_id, variables):\n",
    "    filepaths = []\n",
    "    for var in variables:\n",
    "        path = f'../../data/CMIP6/{experiment_id}/{folder}/{var}'\n",
    "        fp = glob.glob(os.path.join(path, f'CMIP.{model}.{experiment_id}.{var}_regridded.nc'))\n",
    "        if fp:\n",
    "            filepaths.append(fp[0])\n",
    "        else:\n",
    "            #print(f\"No file found for variable '{var}' in model '{model}'.\")\n",
    "            print(fp)\n",
    "\n",
    "    datasets = [xr.open_dataset(fp) for fp in filepaths]\n",
    "    ds = xr.merge(datasets)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e611a-70a8-4dcf-820c-28ecfafacb7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce93d14-4df6-42a9-8543-1903e4b2ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Standardize ========\n",
    "def standardize(ds_dict):\n",
    "    '''\n",
    "    Helper function to standardize datasets of a dictionary\n",
    "    '''\n",
    "    for name, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        ds_stand = (ds - ds.mean('time')) / ds.std('time')\n",
    "\n",
    "        # Preserve variable attributes from the original dataset\n",
    "        for var in ds.variables:\n",
    "            if var in ds_stand.variables:\n",
    "                ds_stand[var].attrs = ds[var].attrs\n",
    "\n",
    "        ds_stand.attrs = attrs\n",
    "        ds_dict[name] = ds_stand\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da322da0-40c5-418f-96ca-de65a1c99c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_period(ds_dict, start_year=None, end_year=None):\n",
    "    '''\n",
    "    Helper function to select periods.\n",
    "    \n",
    "    Parameters:\n",
    "    ds_dict (dict): Dictionary with xarray datasets.\n",
    "    start_year (int): The start year of the period.\n",
    "    end_year (int): The end year of the period.\n",
    "    \"\"\"\n",
    "    '''\n",
    "    start_year = DatetimeNoLeap(start_year, 1, 16, 12, 0, 0, 0,has_year_zero=True) # 16th of January of start year\n",
    "    end_year = DatetimeNoLeap(end_year, 12, 16, 12, 0, 0, 0, has_year_zero=True) # 16th of December of end year\n",
    "    ds_dict = {k: v.sel(time=slice(start_year, end_year)) for k, v in ds_dict.items()}\n",
    "    \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270360cd-47f6-4ef8-80a3-bb22b1619da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_args_and_get_info(ds_dict, variable):\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if not isinstance(variable, str):\n",
    "        raise TypeError('variable must be a string.')\n",
    "        \n",
    "    # Dictionary to store plot titles for each statistic\n",
    "    titles = {\"mean\": \"Mean\", \"std\": \"Standard deviation of yearly means\", \"min\": \"Minimum\", \"max\": \"Maximum\", \"median\": \"Median\", \"time\": \"Time\", \"space\": \"Space\"}\n",
    "    \n",
    "    long_name = {\n",
    "        'Precipitation': 'Precipitation',\n",
    "        'Total Runoff': 'Total Runoff',\n",
    "        'Vapor Pressure Deficit': 'Vapor Pressure Deficit',\n",
    "        'Evaporation Including Sublimation and Transpiration': 'Evapotranspiration',\n",
    "        'Transpiration': 'Transpiration',\n",
    "        'Leaf Area Index': 'Leaf Area Index',\n",
    "        'Carbon Mass Flux out of Atmosphere Due to Gross Primary Production on Land [kgC m-2 s-1]': 'Gross Primary Production',\n",
    "        'Total Liquid Soil Moisture Content of 1 m Column': '1 m Soil Moisture',\n",
    "        'Total Liquid Soil Moisture Content of 2 m Column': '2 m Soil Moisture',\n",
    "    }\n",
    "    \n",
    "    # Data information\n",
    "    var_long_name = long_name[ds_dict[list(ds_dict.keys())[0]][variable].long_name]\n",
    "    period = f\"{ds_dict[list(ds_dict.keys())[0]].attrs['period']}\"\n",
    "    experiment_id =  ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    unit = ds_dict[list(ds_dict.keys())[0]][variable].units\n",
    "    statistic_dim = ds_dict[list(ds_dict.keys())[0]].statistic_dimension\n",
    "    statistic = ds_dict[list(ds_dict.keys())[0]].attrs['statistic']\n",
    "\n",
    "    return var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d6554-8a39-4217-bd64-63a03dc64132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_to_regions(ds_dict, regions):\n",
    "\n",
    "    ds_dict_region = {region: {} for region in regions.keys()}\n",
    "\n",
    "    # For each dataset, slice to each region and save in new dict\n",
    "    for ds_name, ds in ds_dict.items():\n",
    "        for region, bounds in regions.items():\n",
    "            ds_dict_region[region][ds_name] = ds.sel(lat=bounds['lat'], lon=bounds['lon'])\n",
    "            \n",
    "    return ds_dict_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3ab5b-0c12-42c8-9013-e339d3f917e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume all_keys is a list of keys representing the models in both dictionaries\n",
    "def create_consecutive_ts(ds_dict):\n",
    "    ds_dict_merged = {}\n",
    "    for key in ds_dict[list(ds_dict.keys())[0]].keys():\n",
    "        ds1 = ds_dict[list(ds_dict.keys())[0]][key]\n",
    "        ds2 = ds_dict[list(ds_dict.keys())[1]][key]\n",
    "        ds_dict_merged[key] = xr.concat([ds1, ds2], dim='time')\n",
    "    return ds_dict_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966336b1-e574-40fb-bdf7-a380dc6df613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(ds, variables):\n",
    "    flattened_data_dict = {}\n",
    "\n",
    "    for variable in variables:\n",
    "        # Flatten the data\n",
    "        flat_array = ds[variable].values.flatten()\n",
    "\n",
    "        # Add to dictionary\n",
    "        flattened_data_dict[variable] = flat_array\n",
    "\n",
    "    return pd.DataFrame(flattened_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed6d622-ac24-4e0a-a6ff-be3b88eb0ee6",
   "metadata": {},
   "source": [
    "#### Comuting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c45de74-f6b7-45ba-a128-6d8656985750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ens_metric(ds_dict, metric='mean'):\n",
    "    \n",
    "    # Get info\n",
    "    experiment = ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    first_year = [t.year for t in ds_dict[list(ds_dict.keys())[0]].time.values][0]\n",
    "    last_year = [t.year for t in ds_dict[list(ds_dict.keys())[0]].time.values][-1]\n",
    "    \n",
    "    # Combine all datasets into one larger dataset\n",
    "    combined = xr.concat(ds_dict.values(), dim='ensemble')\n",
    "    \n",
    "    # Compute the ensemble metric\n",
    "    ensemble_metric = getattr(combined, metric)(dim='ensemble', skipna=True) # use getattr to call method by string name\n",
    "\n",
    "    # Preserve variable attributes from the original dataset\n",
    "    for var in ds_dict[list(ds_dict.keys())[0]].variables:\n",
    "        if var in ensemble_metric.variables:\n",
    "            ensemble_metric[var].attrs = ds_dict[list(ds_dict.keys())[0]][var].attrs\n",
    "    \n",
    "    ensemble_metric.attrs = {\"period\" : [first_year, last_year],\n",
    "                           \"statistic\" : metric, # use variable metric here\n",
    "                           \"statistic_dimension\" : \"time\",\n",
    "                           \"experiment_id\": experiment, \n",
    "                           \"source_id\" : f\"Ensemble {metric}\"} \n",
    "        \n",
    "    ds_dict[f'Ensemble_{metric}'] = ensemble_metric\n",
    "    \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d65193d-81e9-4944-84e7-9b07290b0185",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4db383-2468-452f-8144-aa5248702a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "variables=['pr', 'vpd', 'evspsbl', 'tran',  'mrro', 'lmrso_2m', 'lai', 'gpp']\n",
    "experiment_id = ['historical', 'ssp370']\n",
    "source_id = ['TaiESM1', 'BCC-CSM2-MR',  'CanESM5', 'CNRM-CM6-1', 'CNRM-ESM2-1', 'IPSL-CM6A-LR', 'UKESM1-0-LL', 'MPI-ESM1-2-LR', 'CESM2-WACCM', 'NorESM2-MM'] #\n",
    "folder='preprocessed'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "ds_dict = {}\n",
    "\n",
    "for period in experiment_id:\n",
    "    # Create dictionary using a dictionary comprehension and Dask\n",
    "    ds_dict[period] = dask.compute({model: open_and_merge_datasets(folder, model, period, variables) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc4e0d-185d-4308-af3a-4f368827826e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= Have a look into the data ==============\n",
    "print(ds_dict.keys())\n",
    "#ds_dict[list(ds_dict.keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8d859-6e4a-44f2-8bb4-e89ea99c0e9e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50535a9e-df11-4941-96a8-b20b619794fe",
   "metadata": {},
   "source": [
    "#### Merge time series to a consecutive one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5289caef-ae18-444f-bca5-260fc46a37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict = create_consecutive_ts(ds_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12466a47-ee3c-4321-83b4-575b90a77b38",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute ensemble mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85004b45-6361-445c-b706-358694e30388",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict = compute_ens_metric(ds_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2a94e-258c-41f4-8955-c16010c374b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ensmean = {'Ensemble_mean': ds_dict['Ensemble_mean']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb98df-cf89-487f-a117-3837344a7ca2",
   "metadata": {},
   "source": [
    "#### Standardize data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1109c9e-7727-47f6-99c6-d8373e073f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ensmean_stand = standardize(ds_dict_ensmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a92b8-c493-4561-b59e-d4cd250a95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ensmean = ds_dict_ensmean_stand['Ensemble_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed8c5f-5f92-4a8b-a150-f0dc44eed6ba",
   "metadata": {},
   "source": [
    "#### Flatten and remove nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e7fd8-f750-448d-bf38-a2daade2050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten data for NN\n",
    "flattened_data_df = flatten_data(ds_ensmean, variables)\n",
    "\n",
    "# Drop NaN values\n",
    "flattened_data_df = flattened_data_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12561748-f7de-4175-ac37-b1c162ff694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0645b-bbfa-4c4f-9364-17f6054e3142",
   "metadata": {},
   "source": [
    "### Set up NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea9e0e-041c-4fe3-a145-1fcd4483f478",
   "metadata": {},
   "source": [
    "#### Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d63cea-4806-4a55-8685-466481903deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = 'tran'\n",
    "\n",
    "# Flatten your data and prepare your features and targets\n",
    "X = flattened_data_df.drop(columns=[target_var])\n",
    "y = flattened_data_df[target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ee152-6049-470d-b7d4-c9597c070d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the data is ordered chronologically\n",
    "train_size = int(len(flattened_data_df) * 0.6)\n",
    "\n",
    "# Train features and targets\n",
    "X_train = X.iloc[:train_size]\n",
    "y_train = y.iloc[:train_size]\n",
    "\n",
    "# Test features and targets\n",
    "X_test = X.iloc[train_size:]\n",
    "y_test = y.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093428f2-44f2-4883-bd30-9d32632b1a41",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b011458-f564-43ac-8c04-078b25c990bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc468ff9-f6b0-49fc-aedf-4933e28b571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network with 3 hidden layer and sigmoid activation in the last layer\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 50) # 50 units\n",
    "        self.fc2 = nn.Linear(50, 30)\n",
    "        self.fc3 = nn.Linear(30, 10)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))  # using sigmoid in the last layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48f7b0-f1e7-48fa-96af-1140d4e4c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f30ad8-5ca6-4a61-b283-78c92ec61acd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define your loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbd0d7-5a7a-43ba-b002-ff6ff76dc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01) # optimizer with L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d3e75-0d0a-4cee-9a02-f8d8a224e85d",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5a737-b2f7-448a-985e-dd03bc484176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float).view(-1, 1) # reshape the test target data to match the output size of model\n",
    "\n",
    "for epoch in range(150):  # loop over the dataset multiple times\n",
    "    optimizer.zero_grad()  # zero the gradient buffers\n",
    "    outputs = model(X_train_tensor)  # forward pass\n",
    "    loss = criterion(outputs, y_train_tensor)  # compute loss\n",
    "    loss.backward()  # backward pass\n",
    "    optimizer.step()  # update weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31f78a-73b4-48ce-93b2-5da2e68b120d",
   "metadata": {},
   "source": [
    "#### Evaluate your model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755d024-8daa-4f0b-8b41-80a238505a7d",
   "metadata": {},
   "source": [
    "##### Get MSE and MAE and compare it to a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509e27a-2149-484a-84e6-f1ad871c8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float).view(-1, 1) # reshape the test target data to match the output size of model\n",
    "\n",
    "model.eval()  # set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "mse = criterion(predictions, y_test_tensor)\n",
    "print(f\"The Mean Squared Error of our forecasts is {mse.item()}\")\n",
    "mae_criterion = torch.nn.L1Loss()\n",
    "mae = mae_criterion(predictions, y_test_tensor)\n",
    "print(f\"The Mean Absolute Error of our forecasts is {mae.item()}\")\n",
    "\n",
    "# Calculate the mean of the training data\n",
    "mean_train = y_train_tensor.mean()\n",
    "\n",
    "# Create a tensor of the same shape as y_test_tensor, filled with the mean of the training data\n",
    "mean_preds = torch.full_like(y_test_tensor, fill_value=mean_train)\n",
    "\n",
    "# Calculate the MSE of the mean predictions\n",
    "mse_baseline = criterion(mean_preds, y_test_tensor)\n",
    "print(f\"The Mean Squared Error of the baseline model is {mse_baseline.item()}\")\n",
    "# If your model's MSE is lower than the baseline's MSE, that suggests your model is learning something useful from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18507390-1735-4cac-87c7-159badff0be3",
   "metadata": {},
   "source": [
    "#### Evaluate multivarite dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f703743c-9316-44aa-862a-521469a9a71b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Compute Permutation Importance\n",
    "- get a general sense of feature importance\n",
    "- might not provide clear results when features are correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca047c-23c3-4a61-8d43-b00728b11557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def permutation_importance(model, X, y, loss_fn):\n",
    "    X = X.clone().detach().requires_grad_(True)\n",
    "    y = y.clone().detach()\n",
    "    \n",
    "    output = model(X)\n",
    "    original_loss = loss_fn(output, y).item()\n",
    "    \n",
    "    importances = []\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        X_perm = X.clone()\n",
    "        X_perm[:, i] = torch.rand(X.shape[0])\n",
    "        output_perm = model(X_perm)\n",
    "        perm_loss = loss_fn(output_perm, y).item()\n",
    "        \n",
    "        importances.append(perm_loss - original_loss)\n",
    "    \n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208450a-da5b-4d99-881a-78f700d6819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a loss function (like MSE) stored in variable 'criterion'\n",
    "importances = permutation_importance(model, X_test_tensor, y_test_tensor, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87d677-5ee4-4a44-82fe-af302572f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or plot importances as desired\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613cf39-3ff3-42e5-85d4-239a456c5815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of feature names\n",
    "feature_names = X_test.columns\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, importances, align='center')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c192e8-b6b0-450a-ae28-77b4b235da37",
   "metadata": {},
   "source": [
    "##### Partial Dependence Plots (PDPs)\n",
    "- visualize the effect of certain features on the model output, given that all other features remain constant\n",
    "- get a sense of how different values of a feature affect the output of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067fd44d-c442-4e65-94b3-da76e9fadf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808f64d-386a-4103-90f5-c62a960edd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an IntegratedGradients object\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# Compute the attribution scores\n",
    "attributions = ig.attribute(X_train_tensor, target=0, n_steps=10)\n",
    "\n",
    "# Convert tensor to numpy for plotting\n",
    "attributions = attributions.detach().numpy()\n",
    "\n",
    "# Sum up the attributions for each feature across all data points (you might also consider taking the mean)\n",
    "attributions_sum = attributions.sum(axis=0)\n",
    "\n",
    "# Plotting\n",
    "plt.barh(X_train.columns, attributions_sum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a34dc-37be-4a3f-8695-796b9ec61f87",
   "metadata": {},
   "source": [
    "##### SHAP Values\n",
    "- how much does each feature contributed, positively or negatively, to each individual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154d3e0-4e7f-4e2d-8993-804bbd068dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Get a batch of your training data to serve as a representative dataset\n",
    "background_data = X_train_tensor[:100]\n",
    "\n",
    "# Create an explainer object\n",
    "from shap.explainers._deep import PyTorchDeep\n",
    "explainer = PyTorchDeep(model, background_data)\n",
    "\n",
    "# Calculate SHAP values for a sample\n",
    "shap_values = explainer.shap_values(X_test_tensor[:10])\n",
    "\n",
    "# Convert the test set to numpy array for the plot\n",
    "X_test_array = X_test[:10].values if isinstance(X_test, pd.DataFrame) else X_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505aee3-d87f-43be-acd7-6d40330d2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature names\n",
    "feature_names = X_test.columns\n",
    "\n",
    "# plot\n",
    "shap.summary_plot(shap_values, X_test_array, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d1d33-66aa-4904-863a-2da19bd66d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "mypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
