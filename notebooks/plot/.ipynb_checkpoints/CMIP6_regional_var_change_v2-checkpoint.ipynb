{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7716ffb9-a564-44c7-87d4-2d4caeecdb1c",
   "metadata": {},
   "source": [
    "# CMIP6 Regional variable change\n",
    "\n",
    "**Following steps are included in this script:**\n",
    "\n",
    "1. Load netCDF files\n",
    "2. Compute regional variable change\n",
    "3. Plot change in reional parallel coordinate plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5a4f3-40bb-459a-88f0-f180c8b3ee8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854af429-43a1-489d-8513-dca8c1466663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== Packages ==========\n",
    "import glob\n",
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import copy\n",
    "\n",
    "#import numpy as np\n",
    "#import copy\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#import cartopy.crs as ccrs\n",
    "\n",
    "#import matplotlib.cm\n",
    "#from matplotlib import rcParams\n",
    "#import math\n",
    "#import multiprocessing as mp\n",
    "#from cftime import DatetimeNoLeap\n",
    "\n",
    "\n",
    "#from matplotlib.lines import Line2D\n",
    "#from sklearn.metrics import r2_score\n",
    "#import matplotlib.colors as colors\n",
    "#from matplotlib.patches import Patch\n",
    "#import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "#rcParams[\"mathtext.default\"] = 'regular'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5daa16-4811-4773-b98d-6b69626ff6cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb57b43-bbbe-414b-8075-8cda5d1ea4ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90f06d-bac2-43fe-af9c-c274a49d664e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_dataset(filename):\n",
    "    \"\"\"Opens and returns an xarray Dataset from a given filename.\"\"\"\n",
    "    return xr.open_dataset(filename)\n",
    "\n",
    "def open_and_merge_datasets(folder, model, experiment_id, temp_res, variables):\n",
    "    \"\"\"Opens and merges datasets for a given model and experiment ID, across specified variables.\"\"\"\n",
    "    merged_ds = None\n",
    "    for var in variables:\n",
    "        # Build the file path pattern\n",
    "        path_pattern = os.path.join('../../data/CMIP6', experiment_id, folder, temp_res, var, \n",
    "                                    f'CMIP.{model}.{experiment_id}.{var}.regridded.nc')\n",
    "        # Find all files that match the pattern\n",
    "        matching_files = glob.glob(path_pattern)\n",
    "        if not matching_files:\n",
    "            print(f\"No file found for variable '{var}' in model '{model}'. Searched pattern: {path_pattern}\")\n",
    "            continue\n",
    "\n",
    "        # Assuming only one match per variable, which seems to be the expectation\n",
    "        file_path = matching_files[0]\n",
    "        ds = open_dataset(file_path)\n",
    "\n",
    "        if merged_ds is None:\n",
    "            merged_ds = ds\n",
    "        else:\n",
    "            merged_ds = xr.merge([merged_ds, ds])\n",
    "\n",
    "    if merged_ds is None:\n",
    "        raise ValueError(f\"No datasets found for model '{model}' and experiment '{experiment_id}'. Please check the inputs.\")\n",
    "    \n",
    "    return merged_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe4af3b-1dc9-48ef-8d26-1b2eca9b8419",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0bead-6e19-4463-a553-3eaca7079bc2",
   "metadata": {},
   "source": [
    "##### Select period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e98cea-e180-4c12-b3c6-e401f2c6435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_period(ds_dict, start_year=None, end_year=None, period=None, yearly_sum=False):\n",
    "    '''\n",
    "    Helper function to select periods and optionally compute yearly sums.\n",
    "    \n",
    "    Parameters:\n",
    "    ds_dict (dict): Dictionary with xarray datasets.\n",
    "    start_year (int): The start year of the period.\n",
    "    end_year (int): The end year of the period.\n",
    "    period (int, list, str, None): Single month (int), list of months (list), multiple seasons (str) to select,\n",
    "                                   or None to not select any specific period.\n",
    "    yearly_sum (bool): If True, compute the yearly sum over the selected period.\n",
    "    '''\n",
    "    \n",
    "    # Create a deep copy of the original ds_dict to avoid modifying it directly\n",
    "    ds_dict_copy = copy.deepcopy(ds_dict)\n",
    "\n",
    "    # Define season to month mapping for northern hemisphere\n",
    "    seasons_to_months = {\n",
    "        'winter': [12, 1, 2],\n",
    "        'spring': [3, 4, 5],\n",
    "        'summer': [6, 7, 8],\n",
    "        'fall': [9, 10, 11]\n",
    "    }\n",
    "    \n",
    "    # Define month name mapping\n",
    "    month_names = {\n",
    "        1: 'J', 2: 'F', 3: 'M', 4: 'A', 5: 'M', 6: 'J',\n",
    "        7: 'J', 8: 'A', 9: 'S', 10: 'O', 11: 'N', 12: 'D'\n",
    "    }\n",
    "\n",
    "    # Define number of days per month (assuming 28 days for February)\n",
    "    days_per_month = {\n",
    "        1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30,\n",
    "        7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31\n",
    "    }\n",
    "\n",
    "    months = []\n",
    "\n",
    "    # If no specific period is selected, all data will be used.\n",
    "    if period is None:\n",
    "        period_name = 'whole_year'\n",
    "        months = list(range(1, 13))  # All months\n",
    "    elif isinstance(period, int):\n",
    "        period_name = month_names[period]\n",
    "        months = [period]\n",
    "    elif isinstance(period, str):\n",
    "        # Check if the input is a single season or multiple seasons\n",
    "        if 'and' in period:\n",
    "            seasons = period.lower().split('and')\n",
    "            period_name = ''\n",
    "            for season in seasons:\n",
    "                season = season.strip()\n",
    "                months.extend(seasons_to_months.get(season, []))\n",
    "                period_name += ''.join(month_names[m] for m in seasons_to_months.get(season, []))\n",
    "        else:\n",
    "            months = seasons_to_months.get(period.lower(), [])\n",
    "            period_name = ''.join(month_names[m] for m in months)\n",
    "    elif isinstance(period, list):\n",
    "        period_name = ''.join(month_names[m] for m in period if m in month_names)\n",
    "        months = period\n",
    "    else:\n",
    "        raise ValueError(\"Period must be None, an integer, a string representing a single season, \"\n",
    "                         \"a string with multiple seasons separated by 'and', or a list of integers.\")\n",
    "\n",
    "    for k, ds in ds_dict_copy.items():\n",
    "        if start_year and end_year:\n",
    "            start_date = f'{start_year}-01-01'\n",
    "            end_date = f'{end_year}-12-31'\n",
    "            ds = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # If months are specified, select those months\n",
    "        if months:\n",
    "            month_mask = ds['time.month'].isin(months)\n",
    "            ds = ds.where(month_mask, drop=True)\n",
    "\n",
    "        # Store the original attributes of each variable\n",
    "        original_attrs = {var: ds[var].attrs for var in ds.data_vars}\n",
    "\n",
    "        # If yearly_sum is True, sum over 'time' dimension to get yearly sum\n",
    "        if yearly_sum: # does only make sense for accumulative variables e.g. pr or tran\n",
    "            attrs = ds.attrs\n",
    "            # Multiply each value by the number of days in the respective month\n",
    "            days = ds['time'].dt.days_in_month\n",
    "            ds = (ds * days).resample(time='AS').sum(dim='time')\n",
    "            sum_type = 'yearly_sum'\n",
    "            ds.attrs = attrs\n",
    "        else:\n",
    "            sum_type = 'monthly_mean'\n",
    "\n",
    "        # Reassign the original attributes back to each variable\n",
    "        for var in ds.data_vars:\n",
    "            ds[var].attrs = original_attrs[var]\n",
    "\n",
    "        ds_dict_copy[k] = ds\n",
    "        ds_dict_copy[k].attrs['months'] = period_name\n",
    "        ds_dict_copy[k].attrs['yearly_sum'] = sum_type\n",
    "\n",
    "    return ds_dict_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25428a98-ad22-4ec2-adc0-704fc758dacb",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Compute period mean/median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadad33e-375e-4f6f-94b1-918dce7e2f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic(ds_dict, period_statistic, dimension, yearly_mean=True):\n",
    "    \"\"\"\n",
    "    Computes the specified statistic for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        period_statistic (str): The statistic to compute, which can be one of 'mean', 'std', 'min', 'var', or 'median'.\n",
    "        dimension (str): The dimension to compute over, which can be 'time' or 'space'.\n",
    "        yearly_mean (boolean): Defines the time interval for statistic computations over space. yearly_mean is True so 30 values for 30 year period.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with computed statistic for each dataset.\n",
    "    \"\"\"\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if period_statistic not in [\"mean\", \"std\", \"min\", \"max\", \"var\", \"median\"]:\n",
    "        raise ValueError(f\"Invalid statistic '{period_statistic}' specified.\")\n",
    "    if dimension not in [\"time\", \"space\"]:\n",
    "        raise ValueError(f\"Invalid dimension '{dimension}' specified.\")  \n",
    "        \n",
    "    # Use multiprocessing to compute the statistic for each dataset in parallel\n",
    "    with mp.Pool() as pool:\n",
    "        results = pool.starmap(compute_statistic_single, [(ds, period_statistic, dimension, yearly_mean) for ds in ds_dict.values()])\n",
    "\n",
    "    return dict(zip(ds_dict.keys(), results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766cd9d-393f-4db9-b0a3-744388ad88c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic_single(ds, period_statistic, dimension, yearly_mean=True):\n",
    "    if dimension == \"time\":\n",
    "        stat_ds = getattr(ds, period_statistic)(\"time\", keep_attrs=True, skipna=True)\n",
    "        stat_ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "    if dimension == \"space\":\n",
    "        # Assign the period attribute before grouping by year\n",
    "        ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "        if yearly_mean:\n",
    "            ds = ds.groupby('time.year').mean('time', keep_attrs=True, skipna=True)\n",
    "            ds.attrs['mean'] = 'yearly mean'\n",
    "            \n",
    "        \n",
    "        #get the weights, apply on data, and compute statistic\n",
    "        weights = np.cos(np.deg2rad(ds.lat))\n",
    "        weights.name = \"weights\"\n",
    "        ds_weighted = ds.weighted(weights)\n",
    "        stat_ds = getattr(ds_weighted, period_statistic)((\"lon\", \"lat\"), keep_attrs=True, skipna=True)\n",
    "    \n",
    "    stat_ds.attrs['statistic'] = period_statistic\n",
    "    stat_ds.attrs['statistic_dimension'] = dimension\n",
    "\n",
    "    return stat_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9148cef-f033-40ac-aeef-dec5ccdb277d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0755d44-865c-439e-b702-f2ccb30e1010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_var(ds_dict, var):\n",
    "    for name, ds in ds_dict.items():\n",
    "        ds_dict[name] = ds.drop(var)\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba91dd1-a3c7-4ef4-9b93-4116470f0d84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_period(ds_dict, start_year=None, end_year=None, period=None, yearly_sum=False):\n",
    "    '''\n",
    "    Helper function to select periods and optionally compute yearly sums.\n",
    "    \n",
    "    Parameters:\n",
    "    ds_dict (dict): Dictionary with xarray datasets.\n",
    "    start_year (int): The start year of the period.\n",
    "    end_year (int): The end year of the period.\n",
    "    period (int, list, str, None): Single month (int), list of months (list), multiple seasons (str) to select,\n",
    "                                   or None to not select any specific period.\n",
    "    yearly_sum (bool): If True, compute the yearly sum over the selected period.\n",
    "    '''\n",
    "    \n",
    "    # Create a deep copy of the original ds_dict to avoid modifying it directly\n",
    "    ds_dict_copy = copy.deepcopy(ds_dict)\n",
    "\n",
    "    # Define season to month mapping for northern hemisphere\n",
    "    seasons_to_months = {\n",
    "        'nh_winter': [12, 1, 2],\n",
    "        'nh_spring': [3, 4, 5],\n",
    "        'nh_summer': [6, 7, 8],\n",
    "        'nh_fall': [9, 10, 11]\n",
    "    }\n",
    "    \n",
    "    # Define month name mapping\n",
    "    month_names = {\n",
    "        1: 'J', 2: 'F', 3: 'M', 4: 'A', 5: 'M', 6: 'J',\n",
    "        7: 'J', 8: 'A', 9: 'S', 10: 'O', 11: 'N', 12: 'D'\n",
    "    }\n",
    "\n",
    "    # Define number of days per month (assuming 28 days for February)\n",
    "    days_per_month = {\n",
    "        1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30,\n",
    "        7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31\n",
    "    }\n",
    "\n",
    "    months = []\n",
    "\n",
    "    # If no specific period is selected, all data will be used.\n",
    "    if period is None:\n",
    "        period_name = 'whole_year'\n",
    "        months = list(range(1, 13))  # All months\n",
    "    elif isinstance(period, int):\n",
    "        period_name = month_names[period]\n",
    "        months = [period]\n",
    "    elif isinstance(period, str):\n",
    "        # Check if the input is a single season or multiple seasons\n",
    "        if 'and' in period:\n",
    "            seasons = period.lower().split('and')\n",
    "            period_name = ''\n",
    "            for season in seasons:\n",
    "                season = season.strip()\n",
    "                months.extend(seasons_to_months.get(season, []))\n",
    "                period_name += ''.join(month_names[m] for m in seasons_to_months.get(season, []))\n",
    "        else:\n",
    "            months = seasons_to_months.get(period.lower(), [])\n",
    "            period_name = ''.join(month_names[m] for m in months)\n",
    "    elif isinstance(period, list):\n",
    "        period_name = ''.join(month_names[m] for m in period if m in month_names)\n",
    "        months = period\n",
    "    else:\n",
    "        raise ValueError(\"Period must be None, an integer, a string representing a single season, \"\n",
    "                         \"a string with multiple seasons separated by 'and', or a list of integers.\")\n",
    "\n",
    "    for k, ds in ds_dict_copy.items():\n",
    "        if start_year and end_year:\n",
    "            start_date = f'{start_year}-01-01'\n",
    "            end_date = f'{end_year}-12-31'\n",
    "            ds = ds.sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # If months are specified, select those months\n",
    "        if months:\n",
    "            month_mask = ds['time.month'].isin(months)\n",
    "            ds = ds.where(month_mask, drop=True)\n",
    "\n",
    "        # Store the original attributes of each variable\n",
    "        original_attrs = {var: ds[var].attrs for var in ds.data_vars}\n",
    "\n",
    "        # If yearly_sum is True, sum over 'time' dimension to get yearly sum\n",
    "        if yearly_sum: # does only make sense for accumulative variables e.g. pr or tran\n",
    "            attrs = ds.attrs\n",
    "            # Multiply each value by the number of days in the respective month\n",
    "            days = ds['time'].dt.days_in_month\n",
    "            ds = (ds * days).resample(time='AS').sum(dim='time')\n",
    "            sum_type = 'yearly_sum'\n",
    "            ds.attrs = attrs\n",
    "        else:\n",
    "            sum_type = 'monthly_mean'\n",
    "\n",
    "        # Reassign the original attributes back to each variable\n",
    "        for var in ds.data_vars:\n",
    "            ds[var].attrs = original_attrs[var]\n",
    "\n",
    "        ds_dict_copy[k] = ds\n",
    "        ds_dict_copy[k].attrs['months'] = period_name\n",
    "        ds_dict_copy[k].attrs['yearly_sum'] = sum_type\n",
    "\n",
    "    return ds_dict_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a690d-ac2f-4875-96dc-66e8424a4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Standardize ========\n",
    "def standardize(ds_dict):\n",
    "    '''\n",
    "    Helper function to standardize datasets of a dictionary\n",
    "    '''\n",
    "    ds_dict_stand = {}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        ds_stand = (ds - ds.mean()) / ds.std()\n",
    "\n",
    "        # Preserve variable attributes from the original dataset\n",
    "        for var in ds.variables:\n",
    "            if var in ds_stand.variables:\n",
    "                ds_stand[var].attrs = ds[var].attrs\n",
    "\n",
    "        ds_stand.attrs = attrs\n",
    "        ds_dict_stand[name] = ds_stand\n",
    "        \n",
    "    return ds_dict_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090c127-204b-4d69-9e33-5769b28e4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_args_and_get_info(ds_dict, variable):\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if not isinstance(variable, str):\n",
    "        raise TypeError('variable must be a string.')\n",
    "        \n",
    "    # Dictionary to store plot titles for each statistic\n",
    "    titles = {\"mean\": \"Mean\", \"std\": \"Standard deviation of yearly means\", \"min\": \"Minimum\", \"max\": \"Maximum\", \"median\": \"Median\", \"time\": \"Time\", \"space\": \"Space\"}\n",
    "    freq = {\"mon\": \"Monthly\"}\n",
    "    \n",
    "    long_name = {\n",
    "        'Precipitation': 'Precipitation',\n",
    "        'Total Runoff': 'Total Runoff',\n",
    "        'Vapor Pressure Deficit': 'Vapor Pressure Deficit',\n",
    "        'Evaporation Including Sublimation and Transpiration': 'Evapotranspiration',\n",
    "        'Transpiration': 'Transpiration',\n",
    "        'Leaf Area Index': 'Leaf Area Index',\n",
    "        'Carbon Mass Flux out of Atmosphere Due to Gross Primary Production on Land [kgC m-2 s-1]': 'Gross Primary Production',\n",
    "        'Total Liquid Soil Moisture Content of 1 m Column': '1 m Soil Moisture',\n",
    "        'Total Liquid Soil Moisture Content of 2 m Column': '2 m Soil Moisture',\n",
    "        'Runoff - Precipitation': 'Runoff - Precipitation',\n",
    "        'Transpiration - Precipitation': 'Transpiration - Precipitation',\n",
    "        '(Runoff + Transpiration) - Precipitation':  '(Runoff + Transpiration) - Precipitation',\n",
    "        'ET - Precipitation':  'ET - Precipitation', \n",
    "        'Negative Runoff': 'Negative Runoff',\n",
    "    }\n",
    "   \n",
    "    # Data information\n",
    "    var_long_name = ds_dict[list(ds_dict.keys())[0]][variable].long_name\n",
    "    period = f\"{ds_dict[list(ds_dict.keys())[0]].attrs['period'][0]}-{ds_dict[list(ds_dict.keys())[0]].attrs['period'][1]}\"\n",
    "    experiment_id =  ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    unit = ds_dict[list(ds_dict.keys())[0]][variable].units\n",
    "    statistic_dim = ds_dict[list(ds_dict.keys())[0]].statistic_dimension\n",
    "    statistic = ds_dict[list(ds_dict.keys())[0]].attrs['statistic']\n",
    "    frequency = freq[ds_dict[list(ds_dict.keys())[0]].frequency]\n",
    "\n",
    "    return var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a73aa-e566-4d11-8dd0-c6de7e0cc432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_ensemble(ds_dict_change):\n",
    "    for key in ['Ensemble mean', 'Ensemble median']:\n",
    "        if key in ds_dict_change:\n",
    "            ds_dict_change.pop(key)\n",
    "\n",
    "    # Drop 'member_id' coordinate if it exists in any of the datasets\n",
    "    for ds_key in ds_dict_change:\n",
    "        if 'member_id' in ds_dict_change[ds_key].coords:\n",
    "            ds_dict_change[ds_key] = ds_dict_change[ds_key].drop('member_id')\n",
    "\n",
    "    combined = xr.concat(ds_dict_change.values(), dim='ensemble')\n",
    "    ds_dict_change['Ensemble median'] = getattr(combined, 'median')(dim='ensemble')\n",
    "    \n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e4851-532d-4dd6-af51-15e12b7761f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regionmask\n",
    "\n",
    "def apply_region_mask(ds_dict):\n",
    "    \"\"\"\n",
    "    Applies the AR6 land region mask to datasets in the provided dictionary and adds a region dimension.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary where keys are the same as in the input dictionary,\n",
    "              and each value is an xarray Dataset with a region dimension added to each variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    land_regions = regionmask.defined_regions.ar6.land\n",
    "    ds_masked_dict = {}\n",
    "    \n",
    "    for ds_name, ds in ds_dict.items():\n",
    "        ds_out = xr.Dataset()  # Initiate an empty Dataset for the masked data\n",
    "        \n",
    "        # Get attributes\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        for var in ds:\n",
    "            # Get the binary mask\n",
    "            mask = land_regions.mask_3D(ds[var])\n",
    "            \n",
    "            var_attrs = ds[var].attrs\n",
    "\n",
    "            # Multiply the original data with the mask to get the masked data\n",
    "            masked_var = ds[var] * mask\n",
    "\n",
    "            # Replace 0s with NaNs, if desired\n",
    "            masked_var = masked_var.where(masked_var != 0)\n",
    "\n",
    "            # Add the masked variable to the output Dataset\n",
    "            ds_out[var] = masked_var\n",
    "            \n",
    "            ds_out[var].attrs = var_attrs\n",
    "            \n",
    "        # Add the attributes\n",
    "        ds_out.attrs = attrs\n",
    "\n",
    "        # Add the Dataset to the output dictionary\n",
    "        ds_masked_dict[ds_name] = ds_out\n",
    "\n",
    "    return ds_masked_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fedf68-0e05-43a4-a705-ba2e9b163a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_numeric(data):\n",
    "    try:\n",
    "        _ = data.astype(float)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def compute_change(ds_dict_hist, ds_dict_fut, var_rel_change=None):\n",
    "    ds_dict_change = {}\n",
    "\n",
    "    for name, ds_hist in ds_dict_hist.items():\n",
    "        if name in ds_dict_fut:\n",
    "            ds_future = ds_dict_fut[name]\n",
    "            common_vars = set(ds_hist.data_vars).intersection(ds_future.data_vars)\n",
    "\n",
    "            ds_change = ds_hist.copy(deep=True)\n",
    "            \n",
    "            if var_rel_change == 'all':\n",
    "                var_rel_change = common_vars\n",
    "                \n",
    "            for var in common_vars:\n",
    "                if is_numeric(ds_hist[var].data) and is_numeric(ds_future[var].data):\n",
    "                    # Always compute percentage change for 'mrso' as models have different depths\n",
    "                    if var == 'mrso':\n",
    "                        rel_change = (ds_future[var] - ds_hist[var]) / ds_hist[var].where(ds_hist[var] != 0) * 100\n",
    "                        ds_change[var].data = rel_change.data\n",
    "                        ds_change[var].attrs['units'] = '%'\n",
    "                    elif var_rel_change is not None and var in var_rel_change:\n",
    "                        # Compute relative change where ds_hist is not zero for specified variables\n",
    "                        rel_change = (ds_future[var] - ds_hist[var]) / ds_hist[var].where(ds_hist[var] != 0) * 100\n",
    "                        ds_change[var].data = rel_change.data\n",
    "                        ds_change[var].attrs['units'] = '%'\n",
    "                    else:\n",
    "                        # Compute absolute change for other variables\n",
    "                        abs_change = ds_future[var] - ds_hist[var]\n",
    "                        ds_change[var].data = abs_change.data\n",
    "\n",
    "            ds_change.attrs = ds_future.attrs\n",
    "            ds_dict_change[name] = ds_change\n",
    "\n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a2736-eb0f-466e-b547-4d293e8a5af5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2f45a-7c54-4e4b-a62d-c43186adb78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_spatial_mean(ds_dict):\n",
    "    ds_dict_mean = {}\n",
    "    for key, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        for var in list(ds.data_vars.keys()):\n",
    "            var_attrs = ds[var].attrs\n",
    "            \n",
    "            ds_dict_mean[key][var] = ds.mean(['lon', 'lat'])\n",
    "            ds_dict_mean[key][var].attrs = var_attrs\n",
    "        \n",
    "        ds_dict_mean[key].attrs = attrs\n",
    "        \n",
    "    return ds_dict_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d176a36-95a5-4a18-b46f-a311e49243f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def precompute_metrics(ds_dict, variables, metrics=['pearson']):\n",
    "    # Initialize the results dictionary\n",
    "    results_dict = {metric: {} for metric in metrics}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        # Create a DataFrame with all the variables\n",
    "        df = pd.DataFrame({var: ds[var].values.flatten() for var in variables})\n",
    "        \n",
    "        # Define all pairs of variables\n",
    "        pairs = list(permutations(variables, 2))  # <-- Change here\n",
    "        args = [(df, var1, var2, metrics) for var1, var2 in pairs]\n",
    "\n",
    "        # Use a multiprocessing pool to compute the metrics for all pairs\n",
    "        with Pool() as p:\n",
    "            results = p.map(compute_metrics_for_pair, args)\n",
    "        \n",
    "        # Store the results in the results_dict\n",
    "        for var1, var2, metric_dict in results:\n",
    "            for metric, value in metric_dict.items():\n",
    "                # Ensure the keys exist in the dictionary\n",
    "                results_dict[metric].setdefault(name, {}).setdefault(f'{var1}_{var2}', value)\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9381cb-6944-4552-b6c7-aac1b6a3c6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_stats(ds_dict):\n",
    "    \"\"\"\n",
    "    Compute yearly mean of each variable in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds_dict (dict): The input dictionary of xarray.Dataset.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are the dataset names and the values are another dictionary.\n",
    "          This inner dictionary has keys as variable names and values as DataArray of yearly means.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    for model, ds in ds_dict.items():\n",
    "        # Compute the yearly mean\n",
    "        yearly_ds = ds.resample(time='1Y').mean()\n",
    "\n",
    "        stats[model] = {}\n",
    "        for var in yearly_ds.data_vars:\n",
    "            # Compute the spatial mean\n",
    "            spatial_mean = yearly_ds[var].mean(dim=['lat', 'lon'])\n",
    "            \n",
    "            # Store the yearly mean values\n",
    "            stats[model][var] = spatial_mean\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43015f97-e54e-4f20-aa0c-3e10f0c8d748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_yearly_means(ds_dict):\n",
    "    yearly_means_dict = {}\n",
    "\n",
    "    # For each dataset, compute the yearly mean over the 'time', 'lat', and 'lon' dimensions\n",
    "    for name, ds in ds_dict.items():  \n",
    "        ds_yearly = ds.groupby('time.year').mean('time')    \n",
    "        \n",
    "        yearly_means_dict[name] = ds_yearly\n",
    "\n",
    "    return yearly_means_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbbc45-b47c-461b-99db-d20d9fa0625d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_yearly_regional_means(ds_dict_region):\n",
    "    yearly_means_dict = {}\n",
    "\n",
    "    # For each dataset, compute the yearly mean over the 'time', 'lat', and 'lon' dimensions\n",
    "    for region, ds_dict in ds_dict_region.items():\n",
    "        yearly_means_dict[region] = {}\n",
    "        for ds_name, ds in ds_dict.items():\n",
    "            # Compute the yearly mean\n",
    "            ds_yearly = ds.groupby('time.year').mean('time')\n",
    "            \n",
    "            # Create weights\n",
    "            weights = np.cos(np.deg2rad(ds.lat))\n",
    "            # Apply the weights and calculate the spatial mean\n",
    "            ds_weighted = ds_yearly.weighted(weights)\n",
    "            yearly_means_dict[region][ds_name] = ds_weighted.mean(('lat', 'lon'))\n",
    "\n",
    "    return yearly_means_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b71e4e-b89b-4169-bac4-97f010c2f2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_spatial_mean(ds_dict):\n",
    "    ds_dict_mean = {}\n",
    "    \n",
    "    for key, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        \n",
    "        # Initialize a new Dataset for this key\n",
    "        ds_dict_mean[key] = xr.Dataset()\n",
    "        \n",
    "        for var in list(ds.data_vars.keys()):\n",
    "            var_attrs = ds[var].attrs\n",
    "            \n",
    "            ds_dict_mean[key][var] = ds[var].mean(['lon', 'lat'])\n",
    "            ds_dict_mean[key][var].attrs = var_attrs\n",
    "        \n",
    "        ds_dict_mean[key].attrs = attrs\n",
    "        \n",
    "    return ds_dict_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2f88f-9004-49ad-97dd-f942347313de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354754c-b13c-4650-b31b-7ae9e9e424ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_bgws(ds_dict):\n",
    "\n",
    "    for model, ds in ds_dict.items():\n",
    "        bgws = (ds['mrro']-ds['tran'])/ds['pr']\n",
    "\n",
    "        # Replace infinite values with NaN\n",
    "        bgws = xr.where(np.isinf(bgws), float('nan'), bgws)\n",
    "\n",
    "        # Set all values above 2 and below -2 to NaN\n",
    "        bgws = xr.where(bgws > 2, float('nan'), bgws)\n",
    "        bgws = xr.where(bgws < -2, float('nan'), bgws)\n",
    "\n",
    "        ds_dict[model]['bgws'] = bgws\n",
    "        ds_dict[model]['bgws'].attrs = {'long_name': 'Blue Green Water Share',\n",
    "                             'units': ''}\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b5d86-b10a-4946-ba0d-976ff6238f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wue(ds_dict):\n",
    "\n",
    "    for model, ds in ds_dict.items():\n",
    "        if 'gpp' in ds.variables:\n",
    "            wue = ds['gpp']/ds['tran']\n",
    "\n",
    "            # Replace infinite values with NaN\n",
    "            wue = xr.where(np.isinf(wue), float('nan'), wue)\n",
    "\n",
    "            # Set all values above 4 and below -4 to ±5\n",
    "            wue = xr.where(wue > 4, 5, wue)\n",
    "            wue = xr.where(wue < -4, -5, wue)\n",
    "\n",
    "            ds_dict[model]['wue'] = wue\n",
    "            ds_dict[model]['wue'].attrs = {'long_name': 'Water Use Efficiency',\n",
    "                                 'units': ''}\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9cb0f6-a971-4d9d-9d9f-a8c66c41d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rgtr(ds_dict):\n",
    "\n",
    "    for model, ds in ds_dict.items():\n",
    "        if 'gpp' in ds.variables:\n",
    "            \n",
    "            gpp_standardized = ds['gpp'] / ds['gpp'].max()\n",
    "            tas_standardized = (ds['tas'] + 273.15) / (ds['tas'] + 273.15).max()\n",
    "\n",
    "            # Compute Relative GPP-Temperature Response\n",
    "            rgtr = gpp_standardized / tas_standardized\n",
    "\n",
    "            # Replace infinite values with NaN\n",
    "            rgtr = xr.where(np.isinf(rgtr), float('nan'), rgtr)\n",
    "            \n",
    "            # Set all values below 0 to 0\n",
    "            rgtr = xr.where(rgtr < 0, 0, rgtr)\n",
    "\n",
    "            ds_dict[model]['rgtr'] = rgtr\n",
    "            ds_dict[model]['rgtr'].attrs = {'long_name': 'Relative GPP-Temperature Response',\n",
    "                                 'units': ''}\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84ac6a-bcf6-4262-b155-d16d5a254e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_etp(ds_dict):\n",
    "    \"\"\"\n",
    "    Computes the partitioning of evapotranspiration into evaporation and transpiration components,\n",
    "    scaled between -100 and 100 to represent the share of evaporation to transpiration in the total\n",
    "    evapotranspiration (ET), where -100 indicates total evaporation and 100 indicates total transpiration.\n",
    "\n",
    "    Parameters:\n",
    "    - ds_dict (dict): A dictionary of datasets, where each dataset contains variables\n",
    "                      for evaporation (E), transpiration (T), and evapotranspiration (ET).\n",
    "\n",
    "    Returns:\n",
    "    - dict: The same dictionary with an added variable for each model representing the\n",
    "            Evapotranspiration Partitioning metric.\n",
    "    \"\"\"\n",
    "    \n",
    "    for model, ds in ds_dict.items():\n",
    "        if {'evapo', 'tran', 'evspsbl'}.issubset(ds.variables):\n",
    "            # Ensure ET is consistent with E + T if necessary or use directly as provided\n",
    "            total_ET = ds['evspsbl']\n",
    "            \n",
    "            # Calculate the partitioning of ET into E and T, scaled between -100 and 100\n",
    "            et_partitioning = ((ds['evapo'] - ds['tran']) / total_ET) * 100\n",
    "            \n",
    "            # Add the new metric to the dataset\n",
    "            ds_dict[model]['et_partitioning'] = et_partitioning\n",
    "            ds_dict[model]['et_partitioning'].attrs = {\n",
    "                'long_name': 'Evapotranspiration Partitioning',\n",
    "                'units': '%',\n",
    "                'description': 'Indicates the share of Evaporation to Transpiration in ET, scaling between -100 (total evaporation) and 100 (total transpiration)'\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Model {model} lacks necessary variables ('evapo', 'tran', 'evspsbl'). Skipping.\")\n",
    "            \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4e92a-fcc9-4aeb-83e9-b2e326eef75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_mean(ds, variable):\n",
    "    \"\"\"\n",
    "    Calculate the mean for each month across all years for a given variable in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The input dataset.\n",
    "    variable (str): The name of the variable to calculate the monthly mean for.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: A dataset containing the monthly mean for the specified variable.\n",
    "    \"\"\"\n",
    "    # Calculate the monthly mean\n",
    "    monthly_means = ds[variable].groupby('time.month').mean('time')\n",
    "\n",
    "    # Create a new dataset for the monthly means\n",
    "    monthly_mean_ds = xr.Dataset({variable: monthly_means})\n",
    "\n",
    "    # Copy attributes from the original dataset and variable\n",
    "    monthly_mean_ds.attrs = ds.attrs\n",
    "    monthly_mean_ds[variable].attrs = ds[variable].attrs\n",
    "    monthly_mean_ds[variable].attrs['description'] = f'Monthly mean of {variable}'\n",
    "\n",
    "    return monthly_mean_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff949400-bf13-4ff6-8ac8-4fbaffe1b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_growing_season_length(ds_monthly_mean):\n",
    "    lai = ds_monthly_mean['lai']\n",
    "\n",
    "    # Create a DataArray for storing the growing season length\n",
    "    growing_season_length = xr.DataArray(\n",
    "        np.nan, \n",
    "        dims=('lat', 'lon'), \n",
    "        coords={'lat': ds_monthly_mean.lat, 'lon': ds_monthly_mean.lon}\n",
    "    )\n",
    "\n",
    "    for lat in ds_monthly_mean.lat.values:\n",
    "        for lon in ds_monthly_mean.lon.values:\n",
    "            lai_ts = lai.sel(lat=lat, lon=lon)\n",
    "\n",
    "            # Check if the LAI time series is entirely NaN (ocean cell)\n",
    "            if lai_ts.isnull().all():\n",
    "                continue  # Skip this cell, it remains NaN in growing_season_length\n",
    "\n",
    "            monthly_pct_change = calculate_monthly_pct_change(lai_ts)\n",
    "            starts, ends = detect_season_starts_and_ends(monthly_pct_change)\n",
    "            length = calculate_season_length(starts, ends)\n",
    "            growing_season_length.loc[dict(lat=lat, lon=lon)] = length\n",
    "\n",
    "    # Create a new dataset for the growing season length\n",
    "    ds_growing_season_length = xr.Dataset()\n",
    "    ds_growing_season_length['growing_season_length'] = growing_season_length\n",
    "    ds_growing_season_length['growing_season_length'].attrs = {\n",
    "        'description': 'Length of the growing season in months',\n",
    "        'calculated_from': 'LAI'\n",
    "    }\n",
    "\n",
    "    return ds_growing_season_length\n",
    "\n",
    "\n",
    "def calculate_monthly_pct_change(lai_ts):\n",
    "    monthly_pct_change = ((lai_ts - lai_ts.roll(month=1)) / lai_ts.roll(month=1)) * 100\n",
    "    monthly_pct_change[0] = ((lai_ts[0] - lai_ts[-1]) / lai_ts[-1]) * 100\n",
    "    return monthly_pct_change\n",
    "\n",
    "def detect_season_starts_and_ends(monthly_pct_change):\n",
    "    starts, ends = [], []\n",
    "    \n",
    "    # Check if December starts or ends a growing season \n",
    "    # STARTS\n",
    "    # Wenn November kleiner gleich 0 und December größer 0 beginnt die GS\n",
    "    if monthly_pct_change[-2] <= 0 and monthly_pct_change[-1] > 0:\n",
    "        starts.append(12) \n",
    "    # ENDS\n",
    "    # Wenn November größer 0 und December kleiner gleich 0 endet die GS\n",
    "    if monthly_pct_change[-2] > 0 and monthly_pct_change[-1] <= 0:\n",
    "        ends.append(12)\n",
    "    \n",
    "    #Check if January stats or ends a growing season\n",
    "    # STARTS\n",
    "    # Wenn Dezember kleiner gleich 0 und Januar größer Null beginnt die GS\n",
    "    if monthly_pct_change[-1] <= 0 and monthly_pct_change[0] > 0:\n",
    "        starts.append(1) \n",
    "    # ENDS\n",
    "    # Wenn Dezember größer 0 und Januar kleiner gleich 0\n",
    "    if monthly_pct_change[-1] > 0 and monthly_pct_change[0] <= 0:\n",
    "        ends.append(1)\n",
    "        \n",
    "    # Now handle February to November for starts and ends\n",
    "    for i in range(1, len(monthly_pct_change) - 1):\n",
    "        if monthly_pct_change[i-1] <= 0 and monthly_pct_change[i] > 0:\n",
    "            starts.append(i + 1)\n",
    "        elif monthly_pct_change[i-1] > 0 and monthly_pct_change[i] <= 0:\n",
    "            ends.append(i + 1)\n",
    "            \n",
    "    return starts, ends\n",
    "\n",
    "def calculate_season_length(starts, ends):\n",
    "    growing_season_length = 0\n",
    "    \n",
    "    # Calculate the growing season length(s)\n",
    "    if len(starts) > 1:\n",
    "        if any(end > starts[0] for end in ends):\n",
    "            closest_end = min(filter(lambda end: end > starts[0], ends), key=lambda end: end - starts[0], default=None)\n",
    "            growing_season_length = closest_end - starts[0]\n",
    "        else:\n",
    "            growing_season_length = min(ends) + (12 - starts[0])\n",
    "\n",
    "        if any(end > starts[1] for end in ends):\n",
    "            growing_season_length = growing_season_length + max(ends) - starts[1]\n",
    "        else:\n",
    "            growing_season_length = growing_season_length + min(ends) + (12 - starts[1])\n",
    "    elif len(starts) == 1:\n",
    "        if starts[0] < ends[0]:\n",
    "            growing_season_length = ends[0] - starts[0]\n",
    "        elif starts[0] > ends[0]:\n",
    "            growing_season_length = (12 - starts[0]) + ends[0]\n",
    "    \n",
    "    return growing_season_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173177e7-1327-46be-9952-4ba5d9d45155",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6300c893-d2ca-4261-8da4-05f787f98f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e7d206-0c7c-4cbd-b49e-01c815e991de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12770258-06cf-40eb-b84e-21274152a45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ba4bc-bc65-465b-b86b-7b987f17a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(var='all', scenarios='all', period=None, yearly_sum=False, period_statistic='mean'):\n",
    "    # Load the datasets for all specified scenarios\n",
    "    ds_dict = load_data(var=var, scenarios=scenarios)\n",
    "\n",
    "    # Define scenario-specific start and end years\n",
    "    scenario_years = {\n",
    "        'historical': (1985, 2014),\n",
    "        'ssp370': (2071, 2100)\n",
    "    }\n",
    "\n",
    "    # Loop over each loaded scenario\n",
    "    for scenario in ds_dict:\n",
    "        # Get the start and end year for the current scenario\n",
    "        start_year, end_year = scenario_years.get(scenario, (None, None))\n",
    "\n",
    "        # If the scenario is not recognized, skip further processing\n",
    "        if start_year is None or end_year is None:\n",
    "            print(f\"Warning: Start and end years not defined for scenario '{scenario}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Apply period selection and yearly sum computation\n",
    "        ds_dict[scenario] = select_period(ds_dict[scenario], start_year=start_year, end_year=end_year, \n",
    "                                          period=period, yearly_sum=yearly_sum)\n",
    "\n",
    "        # Compute the specified statistic\n",
    "        ds_dict[scenario] = compute_statistic(ds_dict[scenario], period_statistic=period_statistic, dimension='time')\n",
    "\n",
    "    return ds_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9991c-cf67-439b-9318-8c4109297034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(var='all', scenarios='all', models='all'):\n",
    "    folder = 'preprocessed'\n",
    "    temp_res = 'month'\n",
    "\n",
    "    # Define default variables if 'all' is selected\n",
    "    default_variables = ['tas', 'pr', 'vpd', 'mrro', 'mrso', 'tran', 'lai', 'gpp', 'evspsbl', 'evapo']\n",
    "    variables = var if var != 'all' else default_variables\n",
    "\n",
    "    # Define default experiment IDs if 'all' is selected\n",
    "    default_experiment_ids = ['historical', 'ssp370']\n",
    "    experiment_ids = scenarios if scenarios != 'all' else default_experiment_ids\n",
    "\n",
    "    # Define the source IDs (models)\n",
    "    source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM', \n",
    "                 'CNRM-CM6-1', 'CNRM-ESM2-1', 'GFDL-ESM4', 'GISS-E2-1-G', 'MIROC-ES2L', \n",
    "                 'MPI-ESM1-2-LR', 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL']\n",
    "\n",
    "    # Initialize the dictionary to store datasets\n",
    "    ds_dict = {}\n",
    "\n",
    "    # Loop over each experiment ID and source ID to load and merge datasets\n",
    "    for experiment_id in experiment_ids:\n",
    "        ds_dict[experiment_id] = {}\n",
    "        for model in source_id:\n",
    "            try:\n",
    "                ds = open_and_merge_datasets(folder, model, experiment_id, temp_res, variables)\n",
    "                ds_dict[experiment_id][model] = ds\n",
    "            except ValueError as e:\n",
    "                print(f\"Failed to load data for model {model} in scenario {experiment_id}: {e}\")\n",
    "\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e9d37-d9b7-4f78-8f9b-15be6927eac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c086404-a2d7-4c7b-ab8b-8a38fa897b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_variables=categorize_variables(['tas', 'pr', 'vpd', 'mrro', 'mrso', 'tran', 'lai', 'gpp', 'evspsbl', 'evapo', 'RX5day', 'growing_season_length'], period=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd53d3-4542-42bf-a069-22fd024a59c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorized_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0ff62-e0b9-4b67-abec-b8d71b84d0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(categorized_variables.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b3cf2-7b2b-4dbc-84a1-f187ea22b214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def categorize_variables(vars_selected, period):\n",
    "    # Define categories based on temporal resolution\n",
    "    monthly_variables = ['tas', 'pr', 'vpd', 'mrro', 'mrso', 'tran', 'lai', 'gpp', 'evspsbl', 'evapo']\n",
    "    yearly_variables = ['RX5day']\n",
    "    \n",
    "    # Adjust the handling of period\n",
    "    if period is not None:\n",
    "        period_variables = [f'growing_season_length_{period}']  # Use the specific period if provided\n",
    "        yearly = period  # The default yearly category name\n",
    "    else:\n",
    "        period_variables = ['growing_season_length_period']  # Use a general name if no specific period is provided\n",
    "        yearly = 'year'  # The default yearly category name\n",
    "        period = 'period'  # Use 'period' as the category name if no specific period is provided\n",
    "    \n",
    "    # Initialize categories\n",
    "    categories = {\n",
    "        'month': [],\n",
    "        yearly: [],\n",
    "        period: []\n",
    "    }\n",
    "\n",
    "    # Check the selection and categorize\n",
    "    if vars_selected == 'all':\n",
    "        categories['month'].extend(monthly_variables)\n",
    "        categories[yearly].extend(yearly_variables)\n",
    "        categories[period].extend(period_variables)\n",
    "    else:\n",
    "        for var in vars_selected:\n",
    "            if var in monthly_variables:\n",
    "                categories['month'].append(var)\n",
    "            elif var in yearly_variables:\n",
    "                categories[yearly].append(var)\n",
    "            elif var == 'growing_season_length':  # Use a general check for the variable name\n",
    "                categories[period].append(var if period is None else f'growing_season_length_{period}')\n",
    "            else:\n",
    "                print(f\"Warning: Variable '{var}' not recognized.\")\n",
    "                \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af9196-eb89-434f-a7b7-49268ade08b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(scenario, source_id, folder, variables, temp_res):\n",
    "    \"\"\"\n",
    "    Generalized function to load and merge datasets across different temporal resolutions.\n",
    "    \"\"\"\n",
    "    ds_dict = {}\n",
    "    \n",
    "    for model in source_id:\n",
    "        ds = open_and_merge_datasets(folder, model, scenario, temp_res, variables)\n",
    "        ds_dict[model] = ds\n",
    "\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd10e64-b285-4920-bc14-e6663a307a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(vars='all', scenarios='all', models='all', period=None, yearly_sum=False, period_statistic='mean'):\n",
    "    # Categorize variables based on temporal resolution\n",
    "    categorized_variables = categorize_variables(vars, period)\n",
    "    \n",
    "    # Initialize dictionaries for processed datasets\n",
    "    ds_dict = {}\n",
    "    \n",
    "    # Define default scenarios if 'all' is selected\n",
    "    experiment_ids = ['historical', 'ssp126', 'ssp370', 'ssp585'] if scenarios == 'all' else scenarios\n",
    "    \n",
    "    print(experiment_ids)\n",
    "    \n",
    "    # Source models (IDs)\n",
    "    source_ids = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM', 'CNRM-CM6-1', \n",
    "                 'CNRM-ESM2-1', 'GFDL-ESM4', 'GISS-E2-1-G', 'MIROC-ES2L', 'MPI-ESM1-2-LR', \n",
    "                 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL'] if models == 'all' else models\n",
    "    \n",
    "    # Less models are available for daily pr data\n",
    "    source_ids_rx5day = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5', 'CESM2-WACCM', 'CNRM-CM6-1', \n",
    "                 'CNRM-ESM2-1', 'GFDL-ESM4','MIROC-ES2L', 'MPI-ESM1-2-LR', \n",
    "                 'NorESM2-MM', 'UKESM1-0-LL'] if models == 'all' else models\n",
    "\n",
    "    # Loop through each scenario\n",
    "    for scenario in experiment_ids:\n",
    "        print(scenario)\n",
    "        ds_dict_temp = {}\n",
    "        ds_dict[scenario] = {}\n",
    "        for temp_res, vars_in_res in categorized_variables.items():\n",
    "            \n",
    "            print(temp_res, vars_in_res)\n",
    "            \n",
    "            if vars_in_res == ['RX5day']:\n",
    "                ds_dict_temp[f'{temp_res}'] = load_data(scenario, source_ids_rx5day, 'preprocessed', vars_in_res, temp_res)\n",
    "            else:\n",
    "                ds_dict_temp[f'{temp_res}'] = load_data(scenario, source_ids, 'preprocessed', vars_in_res, temp_res)\n",
    "                \n",
    "            if temp_res == 'month':\n",
    "                ds_dict_temp[temp_res] = select_period(ds_dict_temp[temp_res], start_year=1985 if scenario == 'historical' else 2071, end_year=2014 if scenario == 'historical' else 2100, period=period, yearly_sum=yearly_sum)\n",
    "                ds_dict_temp[temp_res] = compute_statistic(ds_dict_temp[temp_res], period_statistic=period_statistic, dimension='time')\n",
    "                print(f'Period mean of {temp_res} data computed')\n",
    "            elif temp_res == 'year':\n",
    "                #ds_dict_temp[temp_res] = select_period(ds_dict_temp[temp_res], start_year=1985 if scenario == 'historical' else 2071, end_year=2014 if scenario == 'historical' else 2100, period=period, yearly_sum=yearly_sum)\n",
    "                ds_dict_temp[temp_res] = compute_statistic(ds_dict_temp[temp_res], period_statistic=period_statistic, dimension='time')\n",
    "                print(f'Period mean of {temp_res} data computed')\n",
    "                \n",
    "            print(ds_dict_temp.keys())\n",
    "        \n",
    "        for model in source_ids_rx5day:\n",
    "            ds_dict[scenario][model] = xr.merge([ds_dict_temp['month'][model], ds_dict_temp['year'][model], ds_dict_temp['period'][model]])\n",
    "\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8631b-4338-437a-8758-2f69b5ac5788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess(vars='all', scenarios='all', models='all', period=None, yearly_sum=False, period_statistic='mean'):\n",
    "    # Categorize variables based on temporal resolution\n",
    "    categorized_variables = categorize_variables(vars, period)\n",
    "    \n",
    "    # Initialize dictionaries for processed datasets\n",
    "    ds_dict = {}\n",
    "    \n",
    "    # Define default scenarios if 'all' is selected\n",
    "    experiment_ids = ['historical', 'ssp126', 'ssp370', 'ssp585'] if scenarios == 'all' else scenarios\n",
    "    \n",
    "    print(experiment_ids)\n",
    "    \n",
    "    # Source models (IDs)\n",
    "    source_ids = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM', 'CNRM-CM6-1', \n",
    "                 'CNRM-ESM2-1', 'GFDL-ESM4', 'GISS-E2-1-G', 'MIROC-ES2L', 'MPI-ESM1-2-LR', \n",
    "                 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL'] if models == 'all' else models\n",
    "    \n",
    "    # Less models are available for daily pr data\n",
    "    source_ids_rx5day = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5', 'CESM2-WACCM', 'CNRM-CM6-1', \n",
    "                 'CNRM-ESM2-1', 'GFDL-ESM4','MIROC-ES2L', 'MPI-ESM1-2-LR', \n",
    "                 'NorESM2-MM', 'UKESM1-0-LL'] if models == 'all' else models\n",
    "    \n",
    "    # Loop through each scenario\n",
    "    for scenario in experiment_ids:\n",
    "        print(scenario)\n",
    "        ds_dict_temp = {}\n",
    "        ds_dict[scenario] = {}\n",
    "        for temp_res, vars_in_res in categorized_variables.items():\n",
    "            \n",
    "            print(temp_res, vars_in_res)\n",
    "            \n",
    "            if vars_in_res == ['RX5day']:\n",
    "                ds_dict_temp[f'{temp_res}'] = load_data(scenario, source_ids_rx5day, 'preprocessed', vars_in_res, temp_res)\n",
    "            else:\n",
    "                ds_dict_temp[f'{temp_res}'] = load_data(scenario, source_ids, 'preprocessed', vars_in_res, temp_res)\n",
    "                \n",
    "            if temp_res == 'month':\n",
    "                ds_dict_temp[temp_res] = select_period(ds_dict_temp[temp_res], start_year=1985 if scenario == 'historical' else 2071, end_year=2014 if scenario == 'historical' else 2100, period=period, yearly_sum=yearly_sum)\n",
    "                ds_dict_temp[temp_res] = compute_statistic(ds_dict_temp[temp_res], period_statistic=period_statistic, dimension='time')\n",
    "                print(f'Period mean of {temp_res} data computed')\n",
    "            elif temp_res == 'year':\n",
    "                #ds_dict_temp[temp_res] = select_period(ds_dict_temp[temp_res], start_year=1985 if scenario == 'historical' else 2071, end_year=2014 if scenario == 'historical' else 2100, period=period, yearly_sum=yearly_sum)\n",
    "                ds_dict_temp[temp_res] = compute_statistic(ds_dict_temp[temp_res], period_statistic=period_statistic, dimension='time')\n",
    "                print(f'Period mean of {temp_res} data computed')\n",
    "                \n",
    "            print(ds_dict_temp.keys())\n",
    "            \n",
    "    # Merging logic starts here\n",
    "        # First, merge for models that include RX5day data or have data for all temporal resolutions\n",
    "        for model in source_ids_rx5day:\n",
    "            datasets_to_merge = []\n",
    "            for temp_res in ['month', 'year', 'period']:\n",
    "                if temp_res in ds_dict_temp and model in ds_dict_temp[temp_res]:\n",
    "                    datasets_to_merge.append(ds_dict_temp[temp_res][model])\n",
    "            if datasets_to_merge:\n",
    "                ds_dict[scenario][model] = xr.merge(datasets_to_merge)\n",
    "        \n",
    "        # Then, include models not in source_ids_rx5day, ensuring they're merged even if they lack 'year' data\n",
    "        models_excluding_rx5day = set(source_ids) - set(source_ids_rx5day)\n",
    "        for model in models_excluding_rx5day:\n",
    "            datasets_to_merge = []\n",
    "            # Exclude 'year' from temp_res as these models don't have RX5day data\n",
    "            for temp_res in ['month', 'period']:\n",
    "                if temp_res in ds_dict_temp and model in ds_dict_temp[temp_res]:\n",
    "                    datasets_to_merge.append(ds_dict_temp[temp_res][model])\n",
    "            if datasets_to_merge:\n",
    "                # Ensure there's a dictionary entry for the model\n",
    "                if model not in ds_dict[scenario]:\n",
    "                    ds_dict[scenario][model] = xr.merge(datasets_to_merge)\n",
    "                else:\n",
    "                    # If the model already has data (from previous steps), merge new data with existing\n",
    "                    ds_dict[scenario][model] = xr.merge([ds_dict[scenario][model]] + datasets_to_merge)\n",
    "\n",
    "    return ds_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94aed4-c53c-4c17-9c6e-777d2eb732e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a85ca-1466-45e5-a01b-9bcbc250c5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_default_settings(models='all'):\n",
    "    source_ids = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM', 'CNRM-CM6-1', \n",
    "                  'CNRM-ESM2-1', 'GFDL-ESM4', 'GISS-E2-1-G', 'MIROC-ES2L', 'MPI-ESM1-2-LR', \n",
    "                  'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL'] if models == 'all' else models\n",
    "    source_ids_rx5day = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5', 'CESM2-WACCM', 'CNRM-CM6-1', \n",
    "                         'CNRM-ESM2-1', 'GFDL-ESM4', 'MIROC-ES2L', 'MPI-ESM1-2-LR', \n",
    "                         'NorESM2-MM', 'UKESM1-0-LL'] if models == 'all' else models\n",
    "    return source_ids, source_ids_rx5day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b191b-9caa-4d8e-a86c-d7d4f77a12af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data_for_temp_res(scenario, temp_res, vars_in_res, period, yearly_sum, period_statistic, source_ids, source_ids_rx5day):\n",
    "    if vars_in_res == ['RX5day']:\n",
    "        current_source_ids = source_ids_rx5day\n",
    "    else:\n",
    "        current_source_ids = source_ids\n",
    "    \n",
    "    loaded_data = load_data(scenario, current_source_ids, 'preprocessed', vars_in_res, temp_res)\n",
    "    processed_data = select_period(loaded_data, start_year=1985 if scenario == 'historical' else 2071, end_year=2014 if scenario == 'historical' else 2100, period=period, yearly_sum=yearly_sum)\n",
    "    processed_data = compute_statistic(processed_data, period_statistic=period_statistic, dimension='time')\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e3cef-ddd4-4bde-931c-9bbd43d39b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_datasets_for_model(scenario, ds_dict_temp, source_ids, source_ids_rx5day):\n",
    "    merged_data = {}\n",
    "    models_excluding_rx5day = set(source_ids) - set(source_ids_rx5day)\n",
    "    \n",
    "    for model in source_ids:\n",
    "        datasets_to_merge = []\n",
    "        for temp_res in ['month', 'year', 'period']:\n",
    "            if temp_res in ds_dict_temp and model in ds_dict_temp[temp_res]:\n",
    "                datasets_to_merge.append(ds_dict_temp[temp_res][model])\n",
    "        if datasets_to_merge:\n",
    "            merged_data[model] = xr.merge(datasets_to_merge)\n",
    "    \n",
    "    for model in models_excluding_rx5day:\n",
    "        if model not in merged_data:\n",
    "            datasets_to_merge = [ds_dict_temp[res][model] for res in ['month', 'period'] if model in ds_dict_temp[res]]\n",
    "            if datasets_to_merge:\n",
    "                merged_data[model] = xr.merge(datasets_to_merge)\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5c2fa-af89-4ad7-ada9-eebdd17f71a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess(vars='all', scenarios='all', models='all', period=None, yearly_sum=False, period_statistic='mean'):\n",
    "    source_ids, source_ids_rx5day = get_default_settings(models)\n",
    "    categorized_variables = categorize_variables(vars, period)\n",
    "    ds_dict = {}\n",
    "    experiment_ids = ['historical', 'ssp126', 'ssp370', 'ssp585'] if scenarios == 'all' else scenarios\n",
    "    \n",
    "    for scenario in experiment_ids:\n",
    "        ds_dict_temp = {'month': {}, 'year': {}, 'period': {}}\n",
    "        ds_dict[scenario] = {}\n",
    "        for temp_res, vars_in_res in categorized_variables.items():\n",
    "            ds_dict_temp[temp_res] = load_and_preprocess_data_for_temp_res(scenario, temp_res, vars_in_res, period, yearly_sum, period_statistic, source_ids, source_ids_rx5day)\n",
    "        \n",
    "        ds_dict[scenario] = merge_datasets_for_model(scenario, ds_dict_temp, source_ids, source_ids_rx5day)\n",
    "    \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d413e5f-4c82-4776-b0a8-39dfe8752d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict=load_and_preprocess(vars='all', scenarios=['historical', 'ssp370'], models='all', period='winter', yearly_sum=False, period_statistic='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ed0e1-168e-44bb-b60e-6c725fcdc3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict['historical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f17f5f-192d-423c-81b6-8a4ecaffb115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess(var='all', scenarios='all', period=None, yearly_sum=False, period_statistic='mean'):\n",
    "    # Initial variable and scenario setup\n",
    "    monthly_variables = ['tas', 'pr', 'vpd', 'mrro', 'mrso', 'tran', 'lai', 'gpp', 'evspsbl', 'evapo']\n",
    "    if var != 'all':\n",
    "        variables = var\n",
    "    else:\n",
    "        monthly_variables = ['tas', 'pr', 'vpd', 'mrro', 'mrso', 'tran', 'lai', 'gpp', 'evspsbl', 'evapo']\n",
    "        yearly_variables = ['RX5day']\n",
    "        period_variables = ['growing_season_length']\n",
    "        \n",
    "    variables = var if var != 'all' else monthly_variables\n",
    "    default_scenarios = ['historical', 'ssp370']\n",
    "    scenarios = scenarios if scenarios != 'all' else default_scenarios\n",
    "    \n",
    "    # Source ID configuration remains the same\n",
    "    source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5', 'CESM2-WACCM', 'CNRM-CM6-1', 'CNRM-ESM2-1', 'GFDL-ESM4', 'MIROC-ES2L', 'MPI-ESM1-2-LR', 'NorESM2-MM', 'UKESM1-0-LL']\n",
    "    \n",
    "    # Load monthly resolved data\n",
    "    ds_dict_monthly = load_data_general(scenarios, source_id, 'preprocessed', variables, 'month')\n",
    "\n",
    "    # Process additional data with different temporal resolutions\n",
    "    variables_period_mean = ['growing_season_length']\n",
    "    ds_dict_period_mean = load_data_general(scenarios, source_id, 'preprocessed', variables_period_mean, 'period_mean')\n",
    "\n",
    "    variables_year = ['RX5day']\n",
    "    ds_dict_year = load_data_general(scenarios, source_id, 'preprocessed', variables_year, 'year')\n",
    "\n",
    "    # Merge additional data into the main dataset dictionary\n",
    "    for scenario in scenarios:\n",
    "        ds_dict_monthly[scenario] = select_period(ds_dict_monthly[scenario], start_year=1985 if scenario == 'historical' else 2071, end_year=2014 if scenario == 'historical' else 2100, period=period, yearly_sum=yearly_sum)\n",
    "        ds_dict_monthly[scenario] = compute_statistic(ds_dict_monthly[scenario], period_statistic=period_statistic, dimension='time')\n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "            \n",
    "        for model in source_id:\n",
    "            if model in ds_dict_monthly[scenario]:\n",
    "                # Correctly apply `select_period`\n",
    "                selected_ds = select_period(ds_dict_monthly[scenario][model], start_year=1985 if scenario == 'historical' else 2071, end_year=2014 if scenario == 'historical' else 2100, period=period, yearly_sum=yearly_sum)\n",
    "\n",
    "                # Correctly apply `compute_statistic`\n",
    "                computed_ds = compute_statistic(selected_ds, period_statistic=period_statistic, dimension='time')\n",
    "\n",
    "                # Integrate additional data (assuming these functions return a Dataset)\n",
    "                if model in ds_dict_period_mean[scenario]:\n",
    "                    computed_ds['growing_season_length'] = ds_dict_period_mean[scenario][model]['growing_season_length']\n",
    "                if model in ds_dict_year[scenario]:\n",
    "                    computed_ds['RX5day'] = ds_dict_year[scenario][model]['RX5day']\n",
    "\n",
    "                # Store the updated Dataset back into the dictionary\n",
    "                ds_dict_monthly[scenario][model] = computed_ds\n",
    "\n",
    "    return ds_dict_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c864437-e305-4d2b-8c9b-e74e7107833a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess(vars='all', scenarios='all', models='all', period=None, yearly_sum=False, period_statistic='mean'):\n",
    "    categorized_variables = categorize_variables(vars, period)\n",
    "    \n",
    "    ds_dict = {}\n",
    "    \n",
    "    experiment_ids = ['historical', 'ssp126', 'ssp370', 'ssp585'] if scenarios == 'all' else scenarios\n",
    "    \n",
    "    initial_source_ids = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM', 'CNRM-CM6-1', \n",
    "                          'CNRM-ESM2-1', 'GFDL-ESM4', 'GISS-E2-1-G', 'MIROC-ES2L', 'MPI-ESM1-2-LR', \n",
    "                          'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL'] if models == 'all' else models\n",
    "\n",
    "    for scenario in experiment_ids:\n",
    "        ds_dict[scenario] = {}\n",
    "        for model_name in initial_source_ids:\n",
    "            ds_dict[scenario][model_name] = []\n",
    "\n",
    "        for temp_res, vars_in_res in categorized_variables.items():\n",
    "            # Adjust source_ids dynamically based on variable requirements\n",
    "            temp_source_ids = initial_source_ids.copy()\n",
    "            if 'RX5day' in vars_in_res:\n",
    "                temp_source_ids = [src for src in temp_source_ids if src in ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5', 'CESM2-WACCM', 'CNRM-CM6-1', \n",
    "                                                                             'CNRM-ESM2-1', 'GFDL-ESM4', 'MIROC-ES2L', 'MPI-ESM1-2-LR', \n",
    "                                                                             'NorESM2-MM', 'UKESM1-0-LL']]\n",
    "\n",
    "            for model_name in temp_source_ids:\n",
    "                # Load data for the specific model and variable\n",
    "                data = load_data(scenario, [model_name], 'preprocessed', vars_in_res, temp_res)\n",
    "                # Select period and compute statistics as needed\n",
    "                data = select_period(data, start_year=1985 if scenario == 'historical' else 2071, end_year=2014 if scenario == 'historical' else 2100, period=period, yearly_sum=yearly_sum)\n",
    "                data = compute_statistic(data, period_statistic=period_statistic, dimension='time')\n",
    "                ds_dict[scenario][model_name].append(data)\n",
    "\n",
    "        # After loading and processing all variables for a scenario\n",
    "        for model_name in ds_dict[scenario]:\n",
    "            # Combine all variable datasets for each model\n",
    "            if ds_dict[scenario][model_name]:  # Check if there are datasets to combine\n",
    "                ds_dict[scenario][model_name] = xr.combine_by_coords(ds_dict[scenario][model_name], combine_attrs='override')\n",
    "\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae561f-3530-4369-8561-f3f384feb832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict = load_and_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6913aa-47ea-4074-a86d-63fb28109da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286515c4-4edd-4458-aaf0-678e70346908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343c372-5fab-4276-9c97-171ecae7340b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "#variables=['tas', 'pr', 'vpd', 'evspsbl', 'mrro', 'lmrso_1m', 'lmrso_2m', 'tran', 'lai', 'gpp', 'EI', 'wue']\n",
    "variables=['tas', 'pr', 'vpd', 'mrro', 'mrso', 'tran', 'lai', 'gpp', 'evspsbl', 'evapo']\n",
    "folder='preprocessed'\n",
    "temp_res = 'month'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# Create dictionary using a dictionary comprehension and Dask\n",
    "experiment_id = 'historical'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','GISS-E2-1-G','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','TaiESM1','UKESM1-0-LL']\n",
    "ds_dict_hist = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]\n",
    "\n",
    "experiment_id = 'ssp370'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5-CanOE', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','GISS-E2-1-G','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','TaiESM1','UKESM1-0-LL']\n",
    "ds_dict_ssp370 = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ae740-bb1e-443a-8b28-e569a06de618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "variables=['growing_season_length']\n",
    "folder='preprocessed'\n",
    "temp_res = 'period_mean'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# Create dictionary using a dictionary comprehension and Dask\n",
    "experiment_id = 'historical'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0','CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','UKESM1-0-LL']\n",
    "ds_dict_hist_period_mean = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]\n",
    "\n",
    "experiment_id = 'ssp370'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','UKESM1-0-LL']\n",
    "ds_dict_ssp370_period_mean = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee00e31-a8ca-4756-bf10-7dcd86807784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "variables=['RX5day']\n",
    "folder='preprocessed'\n",
    "temp_res = 'year'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# Create dictionary using a dictionary comprehension and Dask\n",
    "experiment_id = 'historical'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0','CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','UKESM1-0-LL']\n",
    "ds_dict_hist_idx = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]\n",
    "\n",
    "experiment_id = 'ssp370'\n",
    "source_id = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CanESM5', 'CESM2-WACCM','CNRM-CM6-1', 'CNRM-ESM2-1','GFDL-ESM4','MIROC-ES2L', 'MPI-ESM1-2-LR','NorESM2-MM','UKESM1-0-LL']\n",
    "ds_dict_ssp370_idx = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, temp_res, variables) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7a34f-fa9f-4544-b614-e8d0d550fd01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370[list(ds_dict_hist.keys())[0]].lai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f749cba-efb8-4532-9e4b-a7b880530fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= Have a look into the data ==============\n",
    "#print(ds_dict_ssp126.keys())\n",
    "ds_dict_ssp370_period[list(ds_dict_hist.keys())[1]].evspsbl.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f417b3-2b33-4081-b1d8-6d3a1c21d269",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a9eff-675e-464e-9236-d82dc7a38d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'nh_winter': [12, 1, 2],'nh_spring': [3, 4, 5],'nh_summer': [6, 7, 8], 'nh_fall': [9, 10, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29262fe-a12e-44ff-894e-6d851abac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_period = select_period(ds_dict_hist, start_year=1985, end_year=2014, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938cc065-ebae-4275-9032-c60801fbe35f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ds_dict_ssp126_period = select_period(ds_dict_ssp126, start_year=2071, end_year=2100, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a12260-1df3-4232-bf91-196fb8144269",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period = select_period(ds_dict_ssp370, start_year=2071, end_year=2100, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b5ac7-d295-4756-be5a-dab0f661373e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ds_dict_ssp585_period = select_period(ds_dict_ssp585, start_year=2071, end_year=2100, period=None, yearly_sum=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de9ead-0d36-4f06-9430-9efeae7cc9bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b6390-f4b2-41d0-951d-c6bae78ce192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_hist_period_metric = compute_statistic(ds_dict_hist_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984dd44-1f18-4f2c-aceb-465d8473c124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ds_dict_ssp126_period_metric = compute_statistic(ds_dict_ssp126_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a9c21-8094-4700-b972-36e3f720d629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_ssp370_period_metric = compute_statistic(ds_dict_ssp370_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f412cbe-95ef-4315-b9b5-ea06ffe0506f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ds_dict_ssp585_period_metric = compute_statistic(ds_dict_ssp585_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65cb01-c3ae-463d-ba97-bcc8a8503523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_idx_period_metric = compute_statistic(ds_dict_hist_idx, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72548857-1952-46b4-b0f2-14cb5d382f00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, ds in ds_dict_hist_idx_period_metric.items():\n",
    "    ds_dict_hist_period_metric[name]['RX5day'] = ds.RX5day\n",
    "    ds_dict_hist_period_metric[name]['growing_season_length'] = ds_dict_hist_period_mean[name]['growing_season_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9792ec-5646-4acd-9e76-ecfede4a5d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_idx_period_metric = compute_statistic(ds_dict_ssp370_idx, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3f697-c366-48a1-821b-57404589785d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, ds in ds_dict_ssp370_idx_period_metric.items():\n",
    "    ds_dict_ssp370_period_metric[name]['RX5day'] = ds.RX5day\n",
    "    ds_dict_ssp370_period_metric[name]['growing_season_length'] = ds_dict_ssp370_period_mean[name]['growing_season_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccdd62e-d3a5-49c3-9828-ca514ad84f83",
   "metadata": {},
   "source": [
    "### Compute variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6bc0a-0837-4359-bd61-a6ad7408853e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute BGWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e99e6-709a-410c-abaa-c9a087e0346f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric = compute_bgws(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eeca7e-4c4b-4b89-9d47-047ab44e6d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ds_dict_ssp126_period_metric = compute_bgws(ds_dict_ssp126_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d56759-5317-4815-936e-9b352c10589a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric = compute_bgws(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b5860-687e-4325-a8d7-fe3aff147d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ds_dict_ssp585_period_metric = compute_bgws(ds_dict_ssp585_period_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdab93a-92b0-44b3-ba3c-3ff9dafbb837",
   "metadata": {},
   "source": [
    "#### Compute WUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc290487-54aa-48bb-abf7-1a09d3f1f499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric = compute_wue(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa75f8c-64c6-402c-811d-8788e7c0e2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric = compute_wue(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c944b9-6a1c-43d6-9fad-eabe5a2c7427",
   "metadata": {},
   "source": [
    "#### Compute ET Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49854bb0-1ecd-47f2-97ff-290a36a1c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric = compute_etp(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2fed6-efb9-4da9-849c-187ccf083355",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric = compute_etp(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913e646-e441-4989-b975-a12023b0baeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef8139-5a2c-43ce-902a-c19a7112355b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric_regions = {}\n",
    "ds_dict_hist_period_metric_regions = apply_region_mask(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b41f3-524b-40f6-93bf-16298c93737c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ds_dict_ssp126_period_metric_regions = {}\n",
    "#ds_dict_ssp126_period_metric_regions = apply_region_mask(ds_dict_ssp126_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbb35e-e9ae-4879-b94d-98d345b56487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period_metric_regions = {}\n",
    "ds_dict_ssp370_period_metric_regions = apply_region_mask(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61624353-4e9b-405b-b2c3-464c9158764e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ds_dict_ssp585_period_metric_regions = {}\n",
    "#ds_dict_ssp585_period_metric_regions = apply_region_mask(ds_dict_ssp585_period_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e5240-ef4c-4649-a512-499bf54b78fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute regional mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798fb9c8-eb90-420a-aac7-2d952df8bf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute spatial mean of regional data\n",
    "ds_dict_hist_period_metric_regional_mean = {}\n",
    "ds_dict_hist_period_metric_regional_mean = calculate_spatial_mean(ds_dict_hist_period_metric_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527e66f-0aa1-4e2e-a34f-126e0d650b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute spatial mean of regional data\n",
    "#ds_dict_ssp126_period_metric_regional_mean = {}\n",
    "#ds_dict_ssp126_period_metric_regional_mean = calculate_spatial_mean(ds_dict_ssp126_period_metric_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f73e8-df66-4029-beea-f682896e98b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute spatial mean of regional data\n",
    "ds_dict_ssp370_period_metric_regional_mean = {}\n",
    "ds_dict_ssp370_period_metric_regional_mean = calculate_spatial_mean(ds_dict_ssp370_period_metric_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8ce6a-3188-442b-8584-b6f723e2d8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute spatial mean of regional data\n",
    "#ds_dict_ssp585_period_metric_regional_mean = {}\n",
    "#ds_dict_ssp585_period_metric_regional_mean = calculate_spatial_mean(ds_dict_ssp585_period_metric_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dff0a4-5120-4ab6-b5d7-30d5bc890203",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605c15d-e959-4553-a1ed-a1a38d773958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rel\n",
    "ds_dict_region_change_ssp126 = compute_change(ds_dict_hist_period_metric_regional_mean, ds_dict_ssp126_period_metric_regional_mean, var_rel_change=['pr', 'vpd', 'evspsbl', 'mrro', 'mrso', 'tran', 'lai', 'gpp'])\n",
    "for name,ds in ds_dict_region_change_ssp126.items():\n",
    "    if 'member_id' in ds:\n",
    "        ds_dict_region_change_ssp126[name] = ds_dict_region_change_ssp126[name].drop('member_id')\n",
    "#abs\n",
    "#ds_dict_region_change_ssp126 = compute_change(ds_dict_hist_period_metric_regional_mean, ds_dict_ssp126_period_metric_regional_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b69340-5d6b-4407-b725-6bf3c71cfdab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rel\n",
    "#ds_dict_region_change_ssp370 = compute_change(ds_dict_hist_period_metric_regional_mean, ds_dict_ssp370_period_metric_regional_mean)#, var_rel_change=['pr', 'vpd', 'evspsbl', 'mrro', 'mrso', 'tran', 'lai', 'gpp'])\n",
    "\n",
    "#abs\n",
    "ds_dict_region_change_ssp370 = compute_change(ds_dict_hist_period_metric_regional_mean, ds_dict_ssp370_period_metric_regional_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f3039-69a2-42fb-9ba9-b1413889fd37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rel\n",
    "ds_dict_region_change_ssp585 = compute_change(ds_dict_hist_period_metric_regional_mean, ds_dict_ssp585_period_metric_regional_mean, var_rel_change=['pr', 'vpd', 'evspsbl', 'mrro', 'mrso', 'tran', 'lai', 'gpp'])\n",
    "\n",
    "#abs\n",
    "#ds_dict_region_change_ssp585 = compute_change(ds_dict_hist_period_metric_regional_mean, ds_dict_ssp585_period_metric_regional_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e57e1c-ecb3-4e8e-8a2f-125cb6977482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for name, ds in ds_dict_region_change_ssp585.items():\n",
    "#    # Drop variables only if they are present in the dataset\n",
    "#    variables_to_drop_present = [var for var in variables_to_drop if var in ds.variables]\n",
    "#    ds_dict_region_change_ssp585[name] = ds.drop_vars(variables_to_drop_present)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b6ca4-55eb-4275-95f1-b88e51b6d370",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Select important regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e915c-c7e1-4b9c-81b2-759ff05196dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_regions = ['W.North-America',\n",
    " 'Mediterranean',\n",
    " 'S.E.Asia',\n",
    " 'N.South-America',\n",
    " 'S.E.South-America',\n",
    " 'Central-Africa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316cc17-d4bd-40b3-9f20-3edaeb327d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_indices = [next(i for i, name in enumerate(ds_dict_hist_period_metric_regional_mean[list(ds_dict_hist.keys())[0]].names.values) if name == region) for region in selected_regions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6231ec2-c750-44a5-8d6f-97f8b2f67766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbabecc0-4852-4f3a-b6c2-e0b43432dcb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot regional var change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfd1d3-d787-4484-aa0e-fbedcd496cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_ensemble(ds_dict_change):\n",
    "    for key in ['Ensemble mean', 'Ensemble median']:\n",
    "        if key in ds_dict_change:\n",
    "            ds_dict_change.pop(key)\n",
    "    \n",
    "    combined = xr.concat(ds_dict_change.values(), dim='ensemble')\n",
    "    ds_dict_change['Ensemble mean'] = getattr(combined, 'mean')(dim='ensemble')\n",
    "    \n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacecae-67af-418b-acbf-dd95c1cec334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_ticks(min_val, max_val, base=20):\n",
    "    \"\"\"Calculate tick positions for the y-axis.\"\"\"\n",
    "    # Calculate the 10% of the min and max values\n",
    "    offset_min = abs(min_val) * 0.1\n",
    "    offset_max = max_val * 0.1\n",
    "\n",
    "    # Find the next \"round\" number beyond min and max values based on the base\n",
    "    lower_tick = base * np.floor(min_val / base)\n",
    "    upper_tick = base * np.ceil(max_val / base)\n",
    "\n",
    "    # Ensure lower_tick is not more than 10% lower than min_val\n",
    "    if min_val - lower_tick > offset_min:\n",
    "        lower_tick += base\n",
    "\n",
    "    # Ensure upper_tick is not more than 10% higher than max_val\n",
    "    if upper_tick - max_val > offset_max:\n",
    "        upper_tick -= base\n",
    "\n",
    "    # If the lower_tick or upper_tick is equal to the base, divide the base by 2\n",
    "    if lower_tick >= -base or upper_tick <= base:\n",
    "        base = base / 2\n",
    "        # Recalculate the ticks with the new base\n",
    "        lower_tick = base * np.floor(min_val / base)\n",
    "        upper_tick = base * np.ceil(max_val / base)\n",
    "\n",
    "    # Include zero and extend to the next round number beyond the data's min and max\n",
    "    ticks = [lower_tick] if lower_tick < 0 else []\n",
    "    ticks += [0]  # Always include zero\n",
    "    ticks += [upper_tick] if upper_tick > 0 else []\n",
    "\n",
    "    # Generate intermediate ticks between the round numbers\n",
    "    intermediate_ticks = np.arange(lower_tick + base, upper_tick, base)\n",
    "    ticks.extend(intermediate_ticks)\n",
    "\n",
    "    return sorted(ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab421365-c368-4fae-8744-643cefae1fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "\n",
    "# Define start and end colors for both gradients\n",
    "deep_blue = (20/255, 110/255, 180/255)  \n",
    "light_blue = (180/255, 215/255, 255/255)\n",
    "deep_green = (14/255, 119/255, 14/255) \n",
    "light_green = (160/255, 220/255, 140/255)\n",
    "\n",
    "# Create custom colormaps\n",
    "blue_cmap = LinearSegmentedColormap.from_list(\"blue_cmap\", [light_blue, deep_blue], N=4)\n",
    "green_cmap = LinearSegmentedColormap.from_list(\"green_cmap\", [deep_green, light_green], N=4)\n",
    "\n",
    "# Sample colors from colormaps\n",
    "blue_colors = [blue_cmap(i) for i in np.linspace(0, 1, 4)]\n",
    "green_colors = [green_cmap(i) for i in np.linspace(0, 1, 4)]\n",
    "\n",
    "# Combine both gradients\n",
    "combined_grad = green_colors + blue_colors \n",
    "\n",
    "# Define boundaries\n",
    "boundaries = [-0.1, -0.75, -0.05, -0.25, 0, 0.25, 0.05, 0.75, 0.1]\n",
    "norm = BoundaryNorm(boundaries, len(combined_grad), clip=True)\n",
    "\n",
    "cmap_name = 'BGWS colormap'\n",
    "bgws_cm = LinearSegmentedColormap.from_list(cmap_name, combined_grad, N=len(combined_grad))\n",
    "\n",
    "# To test and display the colormap\n",
    "fig, ax = plt.subplots(figsize=(6, 1))\n",
    "ax.set_title(cmap_name)\n",
    "plt.imshow(np.linspace(0, 1, 256).reshape(1, -1), aspect='auto', cmap=bgws_cm)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94199d-decc-4c7e-ba6a-2788d5628ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_region_change(ds_dict_change, selected_indices=\"ALL\", selected_vars=None, save_fig=True):\n",
    "    \n",
    "    # Compute ensemble data\n",
    "    ds_dict_change = compute_ensemble(ds_dict_change)\n",
    "\n",
    "    # Determine the type of change: relative or absolute\n",
    "    change = 'rel_change' if ds_dict_change[list(ds_dict_change.keys())[0]]['pr'].units == '%' else 'abs_change'\n",
    "\n",
    "    # Extract models and variables information\n",
    "    models = list(ds_dict_change.keys())\n",
    "    ensemble = ds_dict_change['Ensemble mean']\n",
    "    if selected_vars is None:\n",
    "        variables = [var for var in ensemble.data_vars.keys() if var != 'bgws']\n",
    "    else:\n",
    "        variables = [var for var in selected_vars if var in ensemble.data_vars.keys()]\n",
    "    experiment_id = ds_dict_change[list(ds_dict_change.keys())[0]].experiment_id\n",
    "    description = ds_dict_change[list(ds_dict_change.keys())[0]].description\n",
    "    # Check for 'period' and 'yearly_sum' attributes, set defaults if not found\n",
    "    months = ds_dict_change[list(ds_dict_change.keys())[0]].months\n",
    "    yearly_sum = ds_dict_change[list(ds_dict_change.keys())[0]].yearly_sum\n",
    "\n",
    "\n",
    "    # Create a map for variable display names\n",
    "    var_map = {\n",
    "        'tas': 'T', 'vpd': 'VPD', 'gpp': 'GPP', 'pr': 'P', 'mrro': 'R',\n",
    "        'evspsbl': 'ET', 'tran': 'T', 'evapo': 'E', 'lai': 'Lai', 'mrso': 'SM', \n",
    "        'rgtr':'P/T', 'et_partitioning': 'EP', 'growing_season_length': 'GS', 'RX5day': 'P5'\n",
    "    }\n",
    "    display_vars = [\n",
    "        f\"$\\Delta\\, \\mathrm{{\\it{{{var_map[var]}}}}}$\" if var in var_map else var for var in variables\n",
    "    ]\n",
    "\n",
    "    # Handle selection of indices\n",
    "    selected_indices = (ensemble.region.values.tolist()\n",
    "                        if selected_indices == \"ALL\" else selected_indices)\n",
    "\n",
    "    # Setup plot grid\n",
    "    #nregions = len(selected_indices)\n",
    "    #ncols = 9\n",
    "    #nrows = (nregions + ncols - 1) // ncols \n",
    "    \n",
    "    #fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8 * ncols, 7 * nrows))\n",
    "\n",
    "    # Setup plot grid\n",
    "    nrows = len(selected_indices) // 9 + (len(selected_indices) % 9 > 0)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=9, figsize=(8 * 9, 7 * nrows), squeeze=False)\n",
    "    \n",
    "    # bgws_colormap\n",
    "    bgws_vals = np.array([ds_dict_change[model]['bgws'].values for model in models])\n",
    "    bgws_min, bgws_max = np.nanmin(bgws_vals), np.nanmax(bgws_vals)\n",
    "    norm = plt.Normalize(vmin=-0.1, vmax=0.1)\n",
    "    \n",
    "    # Preliminary settings\n",
    "    threshold = 100  # Set the threshold for y-axis limits\n",
    "    capped_value = 105  # The value to assign to capped data points\n",
    "\n",
    "    if selected_indices == \"ALL\":\n",
    "        selected_indices = ds_dict_change['Ensemble mean'].region.values.tolist()\n",
    "        \n",
    "    # Iterate over each region to plot data\n",
    "    for ridx, region_idx in enumerate(selected_indices):\n",
    "        ax = axes.flatten()[ridx]\n",
    "        max_change = 0  # Track the maximum change for y-axis limits   \n",
    "        min_change = 0 \n",
    "        \n",
    "        # Collect all y-values to determine if any exceed the threshold\n",
    "        all_y_vals = []\n",
    "        \n",
    "        for idx, model in enumerate(models):\n",
    "            model_data = ds_dict_change[model]\n",
    "            y = [\n",
    "                model_data[var].sel(region=region_idx).values\n",
    "                if var in ds_dict_change[model].data_vars else float('nan')\n",
    "                for var in variables\n",
    "            ]\n",
    "            all_y_vals.extend(y)\n",
    "            \n",
    "            # Ensure y is a NumPy array for element-wise comparison\n",
    "            y_array = np.array(y)\n",
    "            \n",
    "            # Cap the values at the threshold\n",
    "            y_capped = np.clip(y_array, -threshold, threshold)\n",
    "            # Set values that were above or below the threshold\n",
    "            y_capped = np.where(y_array > threshold, capped_value, y_capped)  # For values above the threshold\n",
    "            y_capped = np.where(y_array < -threshold, -capped_value, y_capped)  # For values below the threshold\n",
    "            \n",
    "            # Update max_change and min_change\n",
    "            max_change = max(max_change, np.nanmax(y))  \n",
    "            min_change = min(min_change, np.nanmin(y)) \n",
    "            \n",
    "            # Set plotting of bgws\n",
    "            x = range(len(display_vars))\n",
    "            bgws_val = model_data['bgws'].sel(region=region_idx).values if 'bgws' in model_data.data_vars else None\n",
    "            linestyle = '-' if bgws_val is not None and bgws_val >= 0 else '--'\n",
    "            color = bgws_cm(norm(bgws_val))\n",
    "            \n",
    "            # Plot all models\n",
    "            if model != \"Ensemble mean\":\n",
    "                ax.plot(x, y_capped, linestyle=linestyle, color=color, linewidth=1.0, alpha=0.6, zorder=1)\n",
    "                for xi, yi in zip(x, y_capped):\n",
    "                    if not np.isnan(yi):\n",
    "                        ax.text(xi, yi, str(idx + 1), ha='center', va='center', fontsize=16, color=color, fontweight='bold', zorder=2)\n",
    "            else: # Plot Ensemble \n",
    "                ax.scatter(x, y_capped, marker='D', edgecolors='red', s=150, label=model, zorder=3, facecolors='none', lw=2)\n",
    "        \n",
    "        \n",
    "        # Set plot titles and labels\n",
    "        ax.set_title(ensemble.names.sel(region=region_idx).values, fontsize=20)\n",
    "        ax.set_xticks(range(len(display_vars)))\n",
    "        ax.set_xticklabels(display_vars, rotation=45, fontsize=18, ha='right')\n",
    "        y_label = 'End of century response [%]' if change == 'rel_change' else 'End of century response'\n",
    "        ax.set_ylabel(y_label, fontsize=18)\n",
    "        \n",
    "        # Set y-axis limits dynamically based on data spread\n",
    "        lower_limit = max(-threshold, min_change) \n",
    "        upper_limit = min(threshold, max_change) \n",
    "\n",
    "        # Add 10 on y-axis if change exceeds threshold\n",
    "        extra_space = 10  # 10 percent extra space\n",
    "        \n",
    "        # Plot dashed line, extent and add arrows to y-axis if change > threshold\n",
    "        # Max Change\n",
    "        if max_change > threshold:\n",
    "            ax.axhline(threshold, color='grey', linestyle='--', linewidth=0.5, zorder=1)\n",
    "            ax.plot(0, threshold+10, \"^k\", transform=ax.get_yaxis_transform(), clip_on=False)\n",
    "\n",
    "        # Min Change    \n",
    "        #if min_change < -threshold:\n",
    "            ax.axhline(-threshold, color='grey', linestyle='--', linewidth=0.5, zorder=1)\n",
    "            ax.plot(0, -threshold-10, \"vk\", transform=ax.get_yaxis_transform(), clip_on=False)\n",
    "            \n",
    "        # Set y-limits \n",
    "        ax.set_ylim(lower_limit - extra_space, upper_limit + extra_space)\n",
    "        ax.set_ylim(lower_limit * 1.1, upper_limit * 1.1)\n",
    "        \n",
    "        # Set a buffer for visual clarity\n",
    "        buffer = 1\n",
    "        \n",
    "        # Adjust the y-axis limits based on your data range\n",
    "        #ax.set_ylim(-5, 5)\n",
    "\n",
    "\n",
    "        # Define the tick positions, ensuring they do not go beyond the max/min values or thresholds\n",
    "        # Calculate and set y-axis ticks\n",
    "        y_ticks = calculate_ticks(lower_limit, upper_limit)\n",
    "        ax.set_yticks(y_ticks)\n",
    "        \n",
    "        ax.axhline(0, color='grey', linewidth=0.5)\n",
    "        ax.tick_params(axis='x', length=0)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(True)\n",
    "        ax.yaxis.set_tick_params(labelsize=14)\n",
    "        \n",
    "        for var_index in range(len(display_vars)):\n",
    "            ax.axvline(x=var_index, color='gray', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Caption and figure saving\n",
    "    caption = f\"{description} (2071-2100) - Historical (1985-2014)\"\n",
    "    fig.text(0.52, 1.04, caption, ha='center', va='top', fontsize=45, wrap=True, weight='bold')\n",
    "    \n",
    "    # Layout adjustments and legend\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.7)\n",
    "    \n",
    "    # Remove the empty plot in the last row and third column\n",
    "    axes.flatten()[-1].remove()\n",
    "    \n",
    "     # Define positions for the legend and colorbar\n",
    "    legend_position = [0.8, 0.04, 0.3, 0.15]  # [x0, y0, width, height]\n",
    "    colorbar_position = [0.905, 0.055, 0.09, 0.015]\n",
    "\n",
    "    # Add legend directly to the figure\n",
    "    legend_ax = fig.add_axes(legend_position, frame_on=False)\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='D', markeredgecolor='red', markerfacecolor='none', label='Ensemble mean', markersize=10, linestyle='None', lw=2)]\n",
    "    for idx, model in enumerate(models):\n",
    "        if model != \"Ensemble mean\":\n",
    "            legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='w', label=f\"{idx + 1}: {model}\", markersize=10))\n",
    "    \n",
    "    legend = legend_ax.legend(handles=legend_elements, fontsize=14, ncol=2, loc='center')\n",
    "    legend_ax.axis('off')\n",
    "\n",
    "    # Add the colorbar below the legend\n",
    "    cbar_ax = fig.add_axes(colorbar_position)\n",
    "    cbar = fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=bgws_cm), cax=cbar_ax, orientation='horizontal', extend='both')\n",
    "    cbar.set_label(\"$\\Delta$ Blue-Green Water Share\", fontsize=16, weight='bold')\n",
    "\n",
    "    # Define tick locations\n",
    "    tick_locs = [-0.1, -0.075, -0.05, -0.025, 0, 0.025, 0.05, 0.075, 0.1]\n",
    "\n",
    "    # Define tick labels\n",
    "    tick_labels = [\"-0.1\", \"-0.075\", \"-0.05\", \"-0.025\", \"0\", \"0.025\", \"0.05\", \"0.075\", \"0.1\"]\n",
    "\n",
    "    # Set the ticks and tick labels\n",
    "    cbar.set_ticks(tick_locs)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        savepath = os.path.join('../..', 'results', 'CMIP6', 'comparison', 'regional_var_change')\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        filename = f'regional_var_{change}_{experiment_id}_{months}_{yearly_sum}.pdf'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        fig.savefig(filepath, dpi=600, bbox_inches='tight', format='pdf')\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afbc28-a06c-4835-a1b6-e6a5cb42f8be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_custom_region_change(ds_dict_change, selected_indices=\"ALL\", selected_vars=None, y_axis_limits=(-3, 3), save_fig=True):\n",
    "    \n",
    "    # Compute ensemble\n",
    "    ds_dict_change = compute_ensemble(ds_dict_change)\n",
    "    \n",
    "    # Determine the type of change: relative or absolute\n",
    "    change = 'rel_change' if ds_dict_change[list(ds_dict_change.keys())[0]]['pr'].units == '%' else 'abs_change'\n",
    "\n",
    "    # Extract models and variables information\n",
    "    models = list(ds_dict_change.keys())\n",
    "    ensemble = ds_dict_change['Ensemble mean']\n",
    "    variables = selected_vars if selected_vars else [var for var in ensemble.data_vars.keys() if var not in ['bgws']]\n",
    "    selected_indices = ensemble.region.values.tolist() if selected_indices == \"ALL\" else selected_indices\n",
    "    experiment_id = ds_dict_change[list(ds_dict_change.keys())[0]].experiment_id\n",
    "    description = ds_dict_change[list(ds_dict_change.keys())[0]].description\n",
    "    # Check for 'period' and 'yearly_sum' attributes, set defaults if not found\n",
    "    months = ds_dict_change[list(ds_dict_change.keys())[0]].months\n",
    "    yearly_sum = ds_dict_change[list(ds_dict_change.keys())[0]].yearly_sum\n",
    "    \n",
    "    # Create a map for variable display names\n",
    "    var_map = {\n",
    "        'tas': 'T', 'vpd': 'VPD', 'gpp': 'GPP', 'pr': 'P', 'mrro': 'R',\n",
    "        'evspsbl': 'ET', 'tran': 'T', 'evapo': 'E', 'lai': 'Lai', 'mrso': 'SM', \n",
    "        'rgtr':'P/T', 'et_partitioning': 'EP', 'growing_season_length': 'GS', 'RX5day': 'P5'\n",
    "    }\n",
    "    display_vars = [\n",
    "        f\"$\\Delta\\, \\mathrm{{\\it{{{var_map[var]}}}}}$\" if var in var_map else var for var in variables\n",
    "    ]\n",
    "\n",
    "    # Setup plot grid\n",
    "    nregions = len(selected_indices)\n",
    "    ncols = 9\n",
    "    nrows = (nregions + ncols - 1) // ncols \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8 * ncols, 7 * nrows))\n",
    "       \n",
    "    # bgws_colormap\n",
    "    bgws_vals = np.array([ds_dict_change[model]['bgws'].values for model in models])\n",
    "    bgws_min, bgws_max = np.nanmin(bgws_vals), np.nanmax(bgws_vals)\n",
    "    norm = plt.Normalize(vmin=-0.1, vmax=0.1)\n",
    "        \n",
    "    # Iterate over each region to plot data\n",
    "    for ridx, region_idx in enumerate(selected_indices):\n",
    "        ax = axes.flatten()[ridx]\n",
    "        \n",
    "        # Collect all y-values to determine if any exceed the threshold\n",
    "        all_y_vals = []\n",
    "        \n",
    "        for idx, model in enumerate(models):\n",
    "            model_data = ds_dict_change[model]\n",
    "            y = [\n",
    "                model_data[var].sel(region=region_idx).values\n",
    "                if var in ds_dict_change[model].data_vars else float('nan')\n",
    "                for var in variables\n",
    "            ]\n",
    "            all_y_vals.extend(y)\n",
    "            \n",
    "            # Ensure y is a NumPy array for element-wise comparison\n",
    "            y_array = np.array(y)\n",
    "            \n",
    "            # Set plotting of bgws\n",
    "            x = range(len(display_vars))\n",
    "            bgws_val = model_data['bgws'].sel(region=region_idx).values if 'bgws' in model_data.data_vars else None\n",
    "            linestyle = '-' if bgws_val is not None and bgws_val >= 0 else '--'\n",
    "            color = bgws_cm(norm(bgws_val))\n",
    "            \n",
    "            # Plot all models\n",
    "            if model != \"Ensemble mean\":\n",
    "                ax.plot(x, y, linestyle=linestyle, color=color, linewidth=1.0, alpha=0.6, zorder=1)\n",
    "                for xi, yi in zip(x, y):\n",
    "                    if not np.isnan(yi):\n",
    "                        ax.text(xi, yi, str(idx + 1), ha='center', va='center', fontsize=16, color=color, fontweight='bold', zorder=2)\n",
    "            else: # Plot Ensemble \n",
    "                ax.scatter(x, y, marker='D', edgecolors='red', s=150, label=model, zorder=3, facecolors='none', lw=2)\n",
    "        \n",
    "        \n",
    "        # Set plot titles and labels\n",
    "        ax.set_title(ensemble.names.sel(region=region_idx).values, fontsize=20)\n",
    "        ax.set_xticks(range(len(display_vars)))\n",
    "        ax.set_xticklabels(display_vars, rotation=45, fontsize=18, ha='right')\n",
    "        y_label = 'End of century response [%]' if change == 'rel_change' else 'End of century response'\n",
    "        ax.set_ylabel(y_label, fontsize=18)\n",
    "        \n",
    "        # Adjust the y-axis limits based on your data range\n",
    "        #ax.set_ylim(y_axis_limits)\n",
    "\n",
    "        # Define the tick positions, ensuring they do not go beyond the max/min values or thresholds\n",
    "        # Calculate and set y-axis ticks\n",
    "        #y_ticks = calculate_ticks(y_axis_limits)\n",
    "        #ax.set_yticks(y_ticks)\n",
    "        \n",
    "        ax.axhline(0, color='grey', linewidth=0.5)\n",
    "        ax.tick_params(axis='x', length=0)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(True)\n",
    "        ax.yaxis.set_tick_params(labelsize=14)\n",
    "        \n",
    "        for var_index in range(len(display_vars)):\n",
    "            ax.axvline(x=var_index, color='gray', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Caption and figure saving\n",
    "    caption = f\"{description} (2071-2100) - Historical (1985-2014)\"\n",
    "    fig.text(0.52, 1.04, caption, ha='center', va='top', fontsize=45, wrap=True, weight='bold')\n",
    "    \n",
    "    # Layout adjustments and legend\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.7)\n",
    "    \n",
    "    # Remove the empty plot in the last row and third column\n",
    "    axes.flatten()[-1].remove()\n",
    "    \n",
    "     # Define positions for the legend and colorbar\n",
    "    legend_position = [0.8, 0.04, 0.3, 0.15]  # [x0, y0, width, height]\n",
    "    colorbar_position = [0.905, 0.055, 0.09, 0.015]\n",
    "\n",
    "    # Add legend directly to the figure\n",
    "    legend_ax = fig.add_axes(legend_position, frame_on=False)\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='D', markeredgecolor='red', markerfacecolor='none', label='Ensemble mean', markersize=10, linestyle='None', lw=2)]\n",
    "    for idx, model in enumerate(models):\n",
    "        if model != \"Ensemble mean\":\n",
    "            legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='w', label=f\"{idx + 1}: {model}\", markersize=10))\n",
    "    \n",
    "    legend = legend_ax.legend(handles=legend_elements, fontsize=14, ncol=2, loc='center')\n",
    "    legend_ax.axis('off')\n",
    "\n",
    "    # Add the colorbar below the legend\n",
    "    cbar_ax = fig.add_axes(colorbar_position)\n",
    "    cbar = fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=bgws_cm), cax=cbar_ax, orientation='horizontal', extend='both')\n",
    "    cbar.set_label(\"$\\Delta$ Blue-Green Water Share\", fontsize=16, weight='bold')\n",
    "\n",
    "    # Define tick locations\n",
    "    tick_locs = [-0.1, -0.075, -0.05, -0.025, 0, 0.025, 0.05, 0.075, 0.1]\n",
    "\n",
    "    # Define tick labels\n",
    "    tick_labels = [\"-0.1\", \"-0.075\", \"-0.05\", \"-0.025\", \"0\", \"0.025\", \"0.05\", \"0.075\", \"0.1\"]\n",
    "\n",
    "    # Set the ticks and tick labels\n",
    "    cbar.set_ticks(tick_locs)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        savepath = os.path.join('../..', 'results', 'CMIP6', 'comparison', 'regional_var_change')\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        filename = f'regional_var_{change}_{experiment_id}_{months}_{yearly_sum}.pdf'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        fig.savefig(filepath, dpi=600, bbox_inches='tight', format='pdf')\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45feda64-9daa-4585-bcf8-f6277fec2d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot_region_change(ds_dict_region_change_ssp126, selected_indices=\"ALL\", save_fig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edcf3dc-3e89-4251-b4ed-59f79e3353f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming ds_dict_region_change_ssp370 is your dictionary of datasets\n",
    "\n",
    "# Create an empty dictionary to store the selected region data for each model\n",
    "selected_region_data = {}\n",
    "\n",
    "# Loop through each model in the original dictionary\n",
    "for model, ds in ds_dict_region_change_ssp370.items():\n",
    "    # Select only data for region 0\n",
    "    selected_region_data[model] = ds.isel(region=slice(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614a2c5-85d4-48df-a586-83098af8e443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_region_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95017df9-5525-4d0e-91a2-cadf37560b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_region_data['BCC-CSM2-MR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee61ab6-b5dc-408a-a804-2ec46489ffcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_vars = ['tas', 'vpd', 'RX5day', 'et_partitioning', 'pr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d208ae-b88b-414d-8c2b-6493027a9ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aca297-0d4f-45ae-914a-62bbdad02a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47aa156-2460-46fe-8efe-1d9fd6f6ca25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033016e8-fc05-4b4b-85b0-d766912c488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_change_type(ds_dict):\n",
    "    return 'rel_change' if ds_dict[list(ds_dict.keys())[0]]['pr'].units == '%' else 'abs_change'\n",
    "\n",
    "def extract_variables(ds_dict, selected_vars):\n",
    "    ensemble = ds_dict['Ensemble mean']\n",
    "    variables = [var for var in ensemble.data_vars.keys() if var not in ['bgws', 'region', 'abbrevs', 'names', 'member_id']] if selected_vars is None else [var for var in selected_vars if var in ensemble.data_vars.keys()]\n",
    "    experiment_id = ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    description = ds_dict[list(ds_dict.keys())[0]].description\n",
    "    months = ds_dict[list(ds_dict.keys())[0]].months\n",
    "    yearly_sum = ds_dict[list(ds_dict.keys())[0]].yearly_sum\n",
    "    return variables, experiment_id, description, months, yearly_sum\n",
    "\n",
    "def prepare_display_variables(variables):\n",
    "    var_map = {\n",
    "        'tas': 'T', 'vpd': 'VPD', 'gpp': 'GPP', 'pr': 'P', 'mrro': 'R',\n",
    "        'evspsbl': 'ET', 'tran': 'T', 'evapo': 'E', 'lai': 'Lai', 'mrso': 'SM', \n",
    "        'rgtr':'P/T', 'et_partitioning': 'EP', 'growing_season_length': 'GS', 'RX5day': 'P5'\n",
    "    }\n",
    "    return [f\"$\\Delta\\, \\mathrm{{\\it{{{var_map[var]}}}}}$\" if var in var_map else var for var in variables]\n",
    "\n",
    "\n",
    "\n",
    "def setup_plot_grid(selected_indices):\n",
    "    nrows = len(selected_indices) // 9 + (len(selected_indices) % 9 > 0)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=9, figsize=(8 * 9, 7 * nrows), squeeze=False)\n",
    "    return fig, axes\n",
    "\n",
    "def plot_models(axes, ds_dict, models, display_vars, selected_indices, threshold, capped_value):\n",
    "    # Your existing logic for plotting the models\n",
    "    pass\n",
    "\n",
    "def format_axes(axes, ensemble, region_idx, display_vars, change, threshold, max_change, min_change):\n",
    "    # Your existing logic for setting titles, labels, limits, ticks\n",
    "    pass\n",
    "\n",
    "def add_legend_and_colorbar(fig, models):\n",
    "    # Your existing logic for adding legends and colorbars\n",
    "    pass\n",
    "\n",
    "def save_figure(fig, change, experiment_id, months, yearly_sum, save_fig):\n",
    "    if save_fig:\n",
    "        savepath = os.path.join('results', 'CMIP6', 'comparison', 'regional_var_change')\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        filename = f'regional_var_{change}_{experiment_id}_{months}_{yearly_sum}.pdf'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        fig.savefig(filepath, dpi=600, bbox_inches='tight', format='pdf')\n",
    "        return filepath\n",
    "    else:\n",
    "        return 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "\n",
    "def plot_region_change(ds_dict_change, selected_indices=\"ALL\", selected_vars=None, save_fig=True):\n",
    "    ds_dict_change = compute_ensemble(ds_dict_change)\n",
    "    change = determine_change_type(ds_dict_change)\n",
    "    models = list(ds_dict_change.keys())\n",
    "    \n",
    "    variables, experiment_id, description, months, yearly_sum = extract_variables(ds_dict_change, selected_vars)\n",
    "    display_vars = prepare_display_variables(variables)\n",
    "    selected_indices = ds_dict_change['Ensemble mean'].region.values.tolist() if selected_indices == \"ALL\" else selected_indices\n",
    "    fig, axes = setup_plot_grid(selected_indices)\n",
    "    \n",
    "    # Plotting logic...\n",
    "    plot_models(axes, ds_dict_change, models, display_vars, selected_indices, 100, 105)\n",
    "    \n",
    "    # Formatting axes...\n",
    "    # You would loop over your axes and apply formatting using the format_axes function\n",
    "    \n",
    "    # Add legend and colorbar\n",
    "    add_legend_and_colorbar(fig, models)\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    filepath = save_figure(fig, change, experiment_id, months, yearly_sum, save_fig)\n",
    "    return filepath\n",
    "\n",
    "# Call the main function with your dataset dictionary\n",
    "# plot_region_change(ds_dict_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa84bb8-36c0-499f-ac2a-a3c1adea7505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd76287e-6e14-472b-b77c-d69aba3a11b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980041b-a7ca-4885-b4ad-1666b3875bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ab7fb-a07d-4826-acb9-fa31e1f3c471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c21fa8-7bcc-42d3-8e18-dcf51bbd5fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee6a80-153e-4104-a07e-2199809dc36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbe52b-0f0e-4576-8454-8dac253c8ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b35c6ad-a599-4352-b87b-cbf85bede0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b78c1f-1e2d-4ce1-aa57-8bd0308ae96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad32adc-8248-48f2-8087-2033e8bc3574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391edfd-070e-45d9-9c21-0689ec4595a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_vars = ['tas', 'vpd', 'RX5day', 'et_partitioning', 'pr', 'mrro', 'mrso', 'tran', 'lai', 'gpp', 'evspsbl', 'evapo', 'growing_season_length', 'wue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502703a8-7dd8-4bd3-bfc9-a5b15f31627c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_vars = ['tas', 'vpd', 'RX5day', 'et_partitioning']#['pr', 'mrro', 'mrso', 'tran', 'lai', 'gpp', 'evspsbl', 'evapo', 'growing_season_length', 'wue', 'rgtr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3bbca-1676-4c37-b3b8-8a19e9edb401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_region_change(ds_dict_region_change_ssp370, selected_indices='ALL', selected_vars=selected_vars, save_fig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795c4e4-544d-49b1-a603-5232d4e29827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_region_change(ds_dict_region_change_ssp585, selected_indices=\"ALL\", save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da520c0a-945b-46ec-9340-b01b16325f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustern und bgws nicht den change plotten, da ich den plot sonst sehr schwer zu \n",
    "# verstehen finde. Grün kann nämich bedeuten, dass sehr positive (runoff dominiert) bgws regionen etwas\n",
    "# negativer werden. Eventuell doch absolute veränderung plotten. Gleich größe relative Abnahme von runoff \n",
    "# und Transpiration können bedeuten, dass runoff viel mehr abnimmt in absoluten Zahlen und deswegen\n",
    "# BGWS abnimmt (grün)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f7fec-e2f0-430e-8334-d9307ae78b05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cluster Regions based on var change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c8866-b7d3-48fe-9ad9-0caad3b62e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20487f0-429c-43d6-8744-b2b8c93d2af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_distance(point1, point2):\n",
    "    \"\"\"\n",
    "    Custom distance metric that gives more weight to the sign.\n",
    "    Compares the sign of each dimension and penalizes the distance if the signs are different.\n",
    "    \"\"\"\n",
    "    sign_difference = np.sign(point1) != np.sign(point2)  # Array of sign differences for each dimension\n",
    "    sign_weight = 2 if np.any(sign_difference) else 1    # If any sign difference, weight is 2, else 1\n",
    "    return np.linalg.norm(point1 - point2) * sign_weight\n",
    "\n",
    "def custom_kmeans(data, n_clusters, max_iter=100):\n",
    "    # Randomly initialize cluster centers\n",
    "    centers = data[np.random.choice(data.shape[0], n_clusters, replace=False)]\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Assign points to nearest cluster\n",
    "        clusters = np.array([np.argmin([custom_distance(x, center) for center in centers]) for x in data])\n",
    "\n",
    "        # Recalculate centers\n",
    "        new_centers = np.array([data[clusters == k].mean(axis=0) for k in range(n_clusters)])\n",
    "\n",
    "        # Break if centers do not change\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "\n",
    "    return clusters, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a52fdce-0fd4-4285-babc-365340a3e2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_regions(ds_dict, max_clusters=40):\n",
    "    \"\"\"\n",
    "    This function clusters regions based on the sign of their numeric variables across multiple models,\n",
    "    and returns a DataFrame with region names and their corresponding cluster numbers.\n",
    "\n",
    "    :param ds_dict: A dictionary of xarray.Dataset objects, each representing a different model.\n",
    "    :param max_clusters: The maximum number of clusters to test for the elbow method.\n",
    "    :return: A pandas DataFrame with columns 'Region' and 'Cluster'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the xarray Datasets to pandas DataFrames and concatenate them\n",
    "    dfs = []\n",
    "    for model, ds in ds_dict.items():\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        df = df.set_index('region')\n",
    "        # Select only numeric columns and convert values to -1 (negative) or 1 (positive)\n",
    "        numeric_cols = df.select_dtypes(include=[float, int]).columns\n",
    "        df = df[numeric_cols].apply(np.sign)\n",
    "        df.columns = [f\"{var}_{model}\" for var in df.columns]  # Rename columns to include model name\n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # Drop columns with NaN values\n",
    "    combined_df = combined_df.dropna(axis=1)\n",
    "\n",
    "    # Find the optimal number of clusters using the elbow method\n",
    "    sum_of_squared_distances = []\n",
    "    K = range(1, max_clusters + 1)\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        km = km.fit(combined_df)\n",
    "        sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "    # Plot the elbow curve\n",
    "    plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum of squared distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    # Ask user for the optimal number of clusters\n",
    "    n_clusters = int(input(\"Enter the optimal number of clusters: \"))\n",
    "    \n",
    "    # Use the KMeans algorithm to find clusters with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    kmeans.fit(combined_df)\n",
    "\n",
    "    # Extract region names from one of the datasets (assuming all datasets have the same region names)\n",
    "    region_names = list(ds_dict.values())[0].names.values\n",
    "\n",
    "    # Create a DataFrame with region names and their assigned clusters\n",
    "    clustered_df = pd.DataFrame({\n",
    "        'Region': region_names,\n",
    "        'Cluster': kmeans.labels_\n",
    "    }).sort_values('Cluster')\n",
    "\n",
    "    return clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae54c6a-0a88-49b0-b573-27c298d9bd69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def cluster_regions(ds_dict, max_clusters=40):\n",
    "    \"\"\"\n",
    "    This function clusters regions based on their numeric variables across multiple models,\n",
    "    standardizes the data, and returns a DataFrame with region names and their corresponding cluster numbers.\n",
    "\n",
    "    :param ds_dict: A dictionary of xarray.Dataset objects, each representing a different model.\n",
    "    :param max_clusters: The maximum number of clusters to test for the elbow method.\n",
    "    :return: A pandas DataFrame with columns 'Region' and 'Cluster'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the xarray Datasets to pandas DataFrames and concatenate them\n",
    "    dfs = []\n",
    "    for model, ds in ds_dict.items():\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        df = df.set_index('region')\n",
    "        # Select only numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[float, int]).columns\n",
    "        df = df[numeric_cols]\n",
    "        df.columns = [f\"{var}_{model}\" for var in df.columns]  # Rename columns to include model name\n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # Drop columns with NaN values\n",
    "    combined_df = combined_df.dropna(axis=1)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(combined_df)\n",
    "\n",
    "    # Find the optimal number of clusters using the elbow method\n",
    "    sum_of_squared_distances = []\n",
    "    K = range(1, max_clusters + 1)\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k,  n_init=10, random_state=42)\n",
    "        km = km.fit(scaled_data)\n",
    "        sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "    # Plot the elbow curve\n",
    "    plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum of squared distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    # Ask user for the optimal number of clusters\n",
    "    n_clusters = int(input(\"Enter the optimal number of clusters: \"))\n",
    "    \n",
    "    # Use the KMeans algorithm to find clusters with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "\n",
    "    # Extract region names from one of the datasets (assuming all datasets have the same region names)\n",
    "    region_names = list(ds_dict.values())[0].names.values\n",
    "\n",
    "    # Create a DataFrame with region names and their assigned clusters\n",
    "    clustered_df = pd.DataFrame({\n",
    "        'Region': region_names,\n",
    "        'Cluster': kmeans.labels_\n",
    "    }).sort_values('Cluster')\n",
    "\n",
    "    return clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ddeee-7aa3-44a9-b9d2-7acd737dda69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def cluster_regions_gmm(ds_dict, max_clusters=40):\n",
    "    \"\"\"\n",
    "    Clusters regions using a Gaussian Mixture Model based on their numeric variables across multiple models.\n",
    "    Returns a DataFrame with region names and their corresponding cluster numbers.\n",
    "\n",
    "    :param ds_dict: A dictionary of xarray.Dataset objects, each representing a different model.\n",
    "    :param max_clusters: The maximum number of clusters to test for the elbow method.\n",
    "    :return: A pandas DataFrame with columns 'Region' and 'Cluster'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the xarray Datasets to pandas DataFrames and concatenate them\n",
    "    dfs = []\n",
    "    for model, ds in ds_dict.items():\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        df = df.set_index('region')\n",
    "        # Select only numeric columns and convert values to -1 (negative) or 1 (positive)\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df = df[numeric_cols].apply(np.sign)\n",
    "        df.columns = [f\"{var}_{model}\" for var in df.columns]  # Rename columns to include model name\n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # Drop columns with NaN values\n",
    "    combined_df = combined_df.dropna(axis=1)\n",
    "\n",
    "    # Find the optimal number of clusters using Gaussian Mixture Model\n",
    "    bic_scores = []\n",
    "    K = range(1, max_clusters + 1)\n",
    "    for k in K:\n",
    "        gmm = GaussianMixture(n_components=k, n_init=10, random_state=42)\n",
    "        gmm = gmm.fit(combined_df)\n",
    "        bic_scores.append(gmm.bic(combined_df))\n",
    "\n",
    "    # Plot the BIC scores\n",
    "    plt.plot(K, bic_scores, 'bx-')\n",
    "    plt.xlabel('k (number of components)')\n",
    "    plt.ylabel('BIC score')\n",
    "    plt.title('BIC Scoring for Gaussian Mixture Model')\n",
    "    plt.show()\n",
    "\n",
    "    # Ask user for the optimal number of clusters\n",
    "    n_clusters = int(input(\"Enter the optimal number of clusters: \"))\n",
    "\n",
    "    # Use the Gaussian Mixture Model to find clusters with the optimal number of clusters\n",
    "    gmm = GaussianMixture(n_components=n_clusters, n_init=10, random_state=42)\n",
    "    gmm = gmm.fit(combined_df)\n",
    "\n",
    "    # Predict the cluster for each region\n",
    "    cluster_labels = gmm.predict(combined_df)\n",
    "\n",
    "    # Extract region names from one of the datasets (assuming all datasets have the same region names)\n",
    "    region_names = list(ds_dict.values())[0].names.values\n",
    "\n",
    "    # Create a DataFrame with region names and their assigned clusters\n",
    "    clustered_df = pd.DataFrame({\n",
    "        'Region': region_names,\n",
    "        'Cluster': cluster_labels\n",
    "    }).sort_values('Cluster')\n",
    "\n",
    "    return clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fced4-0891-4090-aa41-2cb0d04a1f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def cluster_regions(ds_dict, max_clusters=40):\n",
    "    \"\"\"\n",
    "    This function clusters regions based on their numeric variables across multiple models,\n",
    "    standardizes the data, and returns a DataFrame with region names and their corresponding cluster numbers.\n",
    "\n",
    "    :param ds_dict: A dictionary of xarray.Dataset objects, each representing a different model.\n",
    "    :param max_clusters: The maximum number of clusters to test for the elbow method.\n",
    "    :return: A pandas DataFrame with columns 'Region' and 'Cluster'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the xarray Datasets to pandas DataFrames and concatenate them\n",
    "    dfs = []\n",
    "    for model, ds in ds_dict.items():\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        df = df.set_index('region')\n",
    "        # Select only numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[float, int]).columns\n",
    "        df = df[numeric_cols]\n",
    "        df.columns = [f\"{var}_{model}\" for var in df.columns]  # Rename columns to include model name\n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs, axis=1)\n",
    "\n",
    "    # Drop columns with NaN values\n",
    "    combined_df = combined_df.dropna(axis=1)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(combined_df)\n",
    "\n",
    "    # Find the optimal number of clusters using the elbow method\n",
    "    sum_of_squared_distances = []\n",
    "    K = range(1, max_clusters + 1)\n",
    "    for k in K:\n",
    "        _, centers = custom_kmeans(scaled_data, n_clusters=k)\n",
    "        distances = [min([np.linalg.norm(data_point - center) for center in centers]) for data_point in scaled_data]\n",
    "        sum_of_squared_distances.append(np.sum(np.square(distances)))\n",
    "\n",
    "    # Plot the elbow curve\n",
    "    plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum of squared distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    # Ask user for the optimal number of clusters\n",
    "    n_clusters = int(input(\"Enter the optimal number of clusters: \"))\n",
    "    \n",
    "    # Use the custom KMeans algorithm to find clusters with the optimal number of clusters\n",
    "    kmeans_labels, _ = custom_kmeans(scaled_data, n_clusters=n_clusters)\n",
    "\n",
    "    # Extract region names from one of the datasets (assuming all datasets have the same region names)\n",
    "    region_names = list(ds_dict.values())[0].names.values\n",
    "\n",
    "    # Create a DataFrame with region names and their assigned clusters\n",
    "    clustered_df = pd.DataFrame({\n",
    "        'Region': region_names,\n",
    "        'Cluster': kmeans_labels\n",
    "    }).sort_values('Cluster')\n",
    "\n",
    "    return clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f73d7-b87b-4007-80c3-2bc56df4601b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_df = cluster_regions_gmm(ds_dict_region_change_ssp126, max_clusters=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28acc261-2846-4584-8ef3-ea17d4b330ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_df = cluster_regions(ds_dict_region_change_ssp126, max_clusters=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddaf4f2-80c9-40f4-b019-556f518725aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f5d03-f367-4e6f-8edf-c10f88ec754a",
   "metadata": {},
   "source": [
    "### Compute global median change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab319a-3613-4b15-a98f-6b6d0b5055dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Change\n",
    "ds_dict_change = compute_change(ds_dict_hist_period_metric, ds_dict_ssp126_period_metric, var_rel_change=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2016863-0724-4f44-a814-d1a97a3e6b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute ensemble median for the change\n",
    "ds_dict_change_ensemble = compute_ensemble(ds_dict_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8488c-115a-4344-a0fa-e855fc5ef005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only use ensemble median for further analysis\n",
    "ds_ensmed_glob = ds_dict_change_ensemble['Ensemble median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78296b-272c-499c-9ad3-972f9d3cc4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_ensmed_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ac0d7-b920-4aab-9987-c1d3d330caf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_region_change_ssp126_ensemble = compute_ensemble(ds_dict_region_change_ssp126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15879a-0013-4b06-aac9-c6ffd8e8a02b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds_dict_region_change_ssp126_ensemble['Ensemble median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf65bc1-3c7e-410a-98f1-dac85e9ca91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded64b21-332d-464c-ba2a-68327bfc1c94",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot clusters on map¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c115f4e-131f-4bb3-865f-9773c0e6f58e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import ShapelyFeature\n",
    "import regionmask\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6258ed-b2ba-465b-8543-87c5b8255f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "def split_polygon(polygon, meridian=180):\n",
    "    \"\"\"\n",
    "    Splits a Shapely polygon into two polygons at a specified meridian\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    if maxx > meridian and minx < -meridian:\n",
    "        # Polygon crosses the antimeridian\n",
    "        left_poly = []\n",
    "        right_poly = []\n",
    "        for x, y in polygon.exterior.coords:\n",
    "            if x >= meridian:\n",
    "                right_poly.append((x - 360, y))  # Wraparound for the right side\n",
    "            else:\n",
    "                left_poly.append((x, y))\n",
    "        return [Polygon(left_poly), Polygon(right_poly)]\n",
    "    else:\n",
    "        return [polygon]  # Wrap the single polygon in a list for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbe31d-a20f-4a58-b15e-148334bb5dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_clusters_and_regions(ds_ensmed_glob, ds, clustered_df):\n",
    "    # Initialize the plot with a cartopy projection\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "    ax_main = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "    \n",
    "    # Plot the 'bgws' variable from the dataset\n",
    "    ds_ensmed_glob['bgws'].plot(ax=ax_main, vmin=-0.2, vmax=0.2, cmap=bgws_cm, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "    \n",
    "    # Add coastlines and gridlines\n",
    "    ax_main.coastlines()\n",
    "    ax_main.tick_params(axis='both', which='major', labelsize=20)\n",
    "    gridlines = ax_main.gridlines(draw_labels=True, color='black', alpha=0.2, linestyle='--')\n",
    "    gridlines.top_labels = gridlines.right_labels = False\n",
    "    gridlines.xlabel_style = {'size': 18}\n",
    "    gridlines.ylabel_style = {'size': 18}\n",
    "    \n",
    "    # Get region bounds using regionmask\n",
    "    land_regions = regionmask.defined_regions.ar6.land\n",
    "    \n",
    "    # Create a mapping from region names to region numbers\n",
    "    region_name_to_number = dict(zip(ds.names.values, ds.region.values))\n",
    "    \n",
    "    # Map the region names to the same order as ds.names.values\n",
    "    region_to_cluster_map = dict(zip(clustered_df['Region'], clustered_df['Cluster']))\n",
    "\n",
    "    # Now create a new column in ds that maps the region names to cluster numbers\n",
    "    # This assumes ds.names.values has the same region names as in clustered_df['Region']\n",
    "    ds['Cluster'] = [region_to_cluster_map[name] for name in ds.names.values]\n",
    "\n",
    "    # Convert the 'Cluster' DataArray to a NumPy array and get unique values\n",
    "    unique_clusters = np.unique(ds['Cluster'].values)\n",
    "\n",
    "    # Prepare colors for clusters - this assumes a finite number of clusters\n",
    "    cluster_colors = plt.cm.tab20b(np.linspace(0, 1, len(unique_clusters)))\n",
    "\n",
    "\n",
    "    # Loop over the regions and plot the cluster numbers with abbreviations\n",
    "    for region_name, cluster_number in zip(ds.names.values, ds['Cluster']):\n",
    "        reg_num = region_name_to_number[region_name]\n",
    "        region_polygons = land_regions[reg_num].polygon\n",
    "        \n",
    "        region_abbr = ds.abbrevs.values[ds.region.values == reg_num][0]  # Assuming this gives us the correct abbreviation\n",
    "        cluster_color = cluster_colors[cluster_number]  # Get the color for the cluster\n",
    "        \n",
    "        # Fetch the polygon or polygons for this region\n",
    "        region_obj = land_regions[reg_num]\n",
    "        if hasattr(region_obj, 'polygons'):\n",
    "            # If the attribute is 'polygons', we assume it's iterable (e.g., a list of Polygon objects)\n",
    "            region_polygons = region_obj.polygons\n",
    "        elif hasattr(region_obj, 'polygon'):\n",
    "            # If there's only one Polygon, we wrap it in a list to make it iterable\n",
    "            region_polygons = [region_obj.polygon]\n",
    "        else:\n",
    "            raise AttributeError(f\"The region object does not have 'polygons' or 'polygon' attribute.\")\n",
    "\n",
    "        for region_polygon in region_polygons:\n",
    "            # If the polygon crosses the antimeridian, split it\n",
    "            split_polys = split_polygon(region_polygon)\n",
    "\n",
    "            # Handle both Polygon and MultiPolygon types after splitting\n",
    "            for poly in split_polys:\n",
    "                if isinstance(poly, Polygon):\n",
    "                    features_to_plot = [poly]\n",
    "                elif isinstance(poly, MultiPolygon):\n",
    "                    features_to_plot = list(poly.geoms)\n",
    "                else:\n",
    "                    raise TypeError(f\"Unhandled geometry type: {type(poly)}\")\n",
    "\n",
    "                for feature_poly in features_to_plot:\n",
    "                    feature = ShapelyFeature([feature_poly], ccrs.PlateCarree(), edgecolor=cluster_color, facecolor='none', linewidth=2)\n",
    "                    ax_main.add_feature(feature)\n",
    "\n",
    "            # Calculate the centroid for text placement using features_to_plot\n",
    "            centroids = [feature_poly.centroid for feature_poly in features_to_plot]\n",
    "    \n",
    "            text_lon, text_lat = max(centroids, key=lambda c: c.x).coords[0]  # Use the easternmost centroid\n",
    "        \n",
    "            # Ensure cluster_number is a plain integer if it's a single-item array or DataArray\n",
    "            if isinstance(cluster_number, np.ndarray) and cluster_number.size == 1:\n",
    "                cluster_number = cluster_number.item()  # Converts a one-element array to a scalar\n",
    "            elif isinstance(cluster_number, xr.DataArray) and cluster_number.ndim == 0:\n",
    "                cluster_number = cluster_number.values.item()  # Gets the scalar value from a 0-dim DataArray\n",
    "\n",
    "            # Annotate the cluster number for each region\n",
    "            ax_main.text(text_lon, text_lat, f\"{region_abbr}\\n{cluster_number}\",\n",
    "                         horizontalalignment='center', verticalalignment='center', transform=ccrs.PlateCarree(),\n",
    "                         fontsize=20, bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9eeaf4-9075-4d1f-bf71-cbf978c00ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_clusters_and_regions(ds_ensmed_glob, ds, clustered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769075e-bdb5-436c-b452-e123f4c51c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_ensemble_average_per_cluster(clustered_df, ds_dict):\n",
    "    \"\"\"\n",
    "    Calculate the ensemble average for each variable across all models, grouped by cluster,\n",
    "    and append the region names corresponding to each cluster.\n",
    "\n",
    "    :param clustered_df: DataFrame with regions and their corresponding cluster assignments.\n",
    "    :param ds_dict: Dictionary of xarray.Dataset objects with percentage changes for each model.\n",
    "    :return: A DataFrame with the ensemble mean for each variable for each cluster, along with region names.\n",
    "    \"\"\"\n",
    "    # Aggregate data across all models\n",
    "    ensemble_data = []\n",
    "\n",
    "    # Extract all variables from the first dataset as a reference\n",
    "    reference_ds = list(ds_dict.values())[0]\n",
    "    variables = list(reference_ds.data_vars)\n",
    "\n",
    "    for var in variables:\n",
    "        # Check if the variable exists in all models\n",
    "        if all(var in ds.data_vars for ds in ds_dict.values()):\n",
    "            var_all_models = [ds[var].to_dataframe().reset_index()[['region', var]] for ds in ds_dict.values()]\n",
    "            var_combined = pd.concat(var_all_models, axis=0)\n",
    "            ensemble_mean = var_combined.groupby('region').mean()\n",
    "            ensemble_data.append(ensemble_mean)\n",
    "\n",
    "    # Combine the ensemble means for all variables\n",
    "    combined_ensemble = pd.concat(ensemble_data, axis=1)\n",
    "\n",
    "    # Merge with cluster assignments using the region index\n",
    "    combined_ensemble.reset_index(inplace=True)\n",
    "    combined_ensemble = combined_ensemble.merge(clustered_df, left_on='region', right_index=True)\n",
    "\n",
    "    # Group by cluster and calculate the mean for each variable, excluding 'region'\n",
    "    final_stats_df = combined_ensemble.drop(columns=['region']).groupby('Cluster').mean(numeric_only=True)\n",
    "\n",
    "    # Append region names to each cluster\n",
    "    region_names = reference_ds.abbrevs.values\n",
    "    clustered_regions = clustered_df.reset_index().groupby('Cluster')['index'].apply(\n",
    "        lambda x: [region_names[i] for i in x])\n",
    "    final_stats_df['Regions'] = clustered_regions\n",
    "    \n",
    "    return final_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e894c-9520-4497-a6f5-bd632a614585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_changes_per_cluster = calculate_ensemble_average_per_cluster(clustered_df, ds_dict_region_change_ssp126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5316a3-dcb2-4e4b-a120-d6fcc6d0aefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_changes_per_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0448b-8de9-4efe-94c9-d77686a69ed1",
   "metadata": {},
   "source": [
    "### Plot Cluster Spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0c10c-8172-4f89-b4ac-296d3e669f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6aa65b-2daf-44dc-8769-bcd3f3f61094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_cluster_distributions(clustered_df, ds_dict):\n",
    "    combined_data = []\n",
    "    for model, ds in ds_dict.items():\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        df['model'] = model  # Add a column to identify the model\n",
    "        combined_data.append(df)\n",
    "    combined_df = pd.concat(combined_data)\n",
    "    \n",
    "    # Create a mapping from region indices to names\n",
    "    region_index_to_name = list(ds_dict_region_change_ssp126.values())[0].names.to_dict()\n",
    "\n",
    "    # Melt the dataframe to long-form\n",
    "    long_df = combined_df.melt(id_vars=['region', 'abbrevs', 'names', 'model'],\n",
    "                               var_name='Variable', value_name='Value')\n",
    "    # Merge with cluster assignments\n",
    "    long_df = long_df.merge(clustered_df, left_on='names', right_on='Region')\n",
    "\n",
    "    # Number of unique clusters\n",
    "    num_clusters = clustered_df['Cluster'].nunique()\n",
    "\n",
    "    # Calculate the layout for subplots (3 columns)\n",
    "    num_cols = 3\n",
    "    num_rows = np.ceil(num_clusters / num_cols).astype(int)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Iterate over each cluster to create a box plot\n",
    "    for cluster_num in range(num_clusters):\n",
    "        ax = axes[cluster_num]\n",
    "        cluster_data = long_df[long_df['Cluster'] == cluster_num]\n",
    "        sns.boxplot(data=cluster_data, x='Variable', y='Value', ax=ax)\n",
    "        # Use region abbreviations\n",
    "        region_abbrevs = cluster_data['abbrevs'].unique()\n",
    "        ax.set_title(f'Cluster {cluster_num}: {\", \".join(region_abbrevs)}')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_clusters, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772fcee-0eb9-45ca-953b-9874af054641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "plot_cluster_distributions(clustered_df, ds_dict_region_change_ssp126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ffd62-d910-4b4d-a452-cfb720f2fb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def plot_cluster_distributions(clustered_df, ensemble_ds):\n",
    "    # Convert the xarray Dataset to a pandas DataFrame\n",
    "    ensemble_df = ensemble_ds.to_dataframe().reset_index()\n",
    "\n",
    "    # Multiply 'bgws' by 100\n",
    "    ensemble_df['bgws'] *= 100\n",
    "\n",
    "    # Melt the DataFrame to long-form for plotting\n",
    "    long_df = ensemble_df.melt(id_vars=['region', 'abbrevs', 'names'],\n",
    "                               var_name='Variable', value_name='Value')\n",
    "\n",
    "    # Merge with cluster assignments\n",
    "    long_df = long_df.merge(clustered_df, left_on='names', right_on='Region')\n",
    "\n",
    "    # Number of unique clusters\n",
    "    num_clusters = clustered_df['Cluster'].nunique()\n",
    "\n",
    "    # Calculate the layout for subplots (3 columns)\n",
    "    num_cols = 3\n",
    "    num_rows = np.ceil(num_clusters / num_cols).astype(int)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Define markers for regions\n",
    "    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'H', 'X']  # All filled markers\n",
    "    unique_abbrevs = long_df['abbrevs'].unique()\n",
    "    marker_dict = {abbrev: markers[i % len(markers)] for i, abbrev in enumerate(unique_abbrevs)}\n",
    "\n",
    "    # Iterate over each cluster to create a scatter plot\n",
    "    for cluster_num in range(num_clusters):\n",
    "        ax = axes[cluster_num]\n",
    "        cluster_data = long_df[long_df['Cluster'] == cluster_num]\n",
    "        sns.scatterplot(data=cluster_data, x='Variable', y='Value', ax=ax, hue='abbrevs', \n",
    "                        style='abbrevs', markers=marker_dict, legend='brief')\n",
    "        # Use region abbreviations\n",
    "        region_abbrevs = cluster_data['abbrevs'].unique()\n",
    "        ax.set_title(f'Cluster {cluster_num}: {\", \".join(region_abbrevs)}')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Setting the legend outside the plot\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_clusters, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b47be-ac47-46a2-8c3f-c5201befb4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_cluster_distributions(clustered_df, ensemble_ds):\n",
    "    # Convert the xarray Dataset to a pandas DataFrame\n",
    "    ensemble_df = ensemble_ds.to_dataframe().reset_index()\n",
    "\n",
    "    # Multiply 'bgws' by 100\n",
    "    ensemble_df['bgws'] *= 100\n",
    "\n",
    "    # Exclude 'Cluster' from the variables if present\n",
    "    ensemble_df = ensemble_df.drop(columns=['Cluster'], errors='ignore')\n",
    "\n",
    "    # Melt the DataFrame to long-form for plotting\n",
    "    long_df = ensemble_df.melt(id_vars=['region', 'abbrevs', 'names'],\n",
    "                               var_name='Variable', value_name='Value')\n",
    "\n",
    "    # Merge with cluster assignments\n",
    "    long_df = long_df.merge(clustered_df, left_on='names', right_on='Region')\n",
    "\n",
    "    # Number of unique clusters\n",
    "    num_clusters = clustered_df['Cluster'].nunique()\n",
    "\n",
    "    # Calculate the layout for subplots (3 columns)\n",
    "    num_cols = 3\n",
    "    num_rows = np.ceil(num_clusters / num_cols).astype(int)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Define markers for regions\n",
    "    markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'H', 'X']  # All filled markers\n",
    "    unique_abbrevs = long_df['abbrevs'].unique()\n",
    "    marker_dict = {abbrev: markers[i % len(markers)] for i, abbrev in enumerate(unique_abbrevs)}\n",
    "\n",
    "    # Iterate over each cluster to create a scatter plot\n",
    "    for cluster_num in range(num_clusters):\n",
    "        ax = axes[cluster_num]\n",
    "        cluster_data = long_df[long_df['Cluster'] == cluster_num]\n",
    "        sns.scatterplot(data=cluster_data, x='Variable', y='Value', ax=ax, hue='abbrevs', \n",
    "                        style='abbrevs', markers=marker_dict, legend='brief')\n",
    "        # Use region abbreviations\n",
    "        region_abbrevs = cluster_data['abbrevs'].unique()\n",
    "        ax.set_title(f'Cluster {cluster_num}: {\", \".join(region_abbrevs)}')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Setting the legend outside the plot\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for i in range(num_clusters, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b234e-1455-4b2c-a974-5688a01d088e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_cluster_distributions(clustered_df, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0171b487-9048-4fc5-8676-c32f79f7e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop regions that cluster alone:\n",
    "RAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05543f05-8d9c-49a4-8b5c-0751786caf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "mypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
