{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7716ffb9-a564-44c7-87d4-2d4caeecdb1c",
   "metadata": {},
   "source": [
    "# CMIP6 Statistics and Plots\n",
    "\n",
    "**Following steps are included in this script:**\n",
    "\n",
    "1. Load netCDF files\n",
    "2. Compute statistics\n",
    "3. Plot statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5a4f3-40bb-459a-88f0-f180c8b3ee8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854af429-43a1-489d-8513-dca8c1466663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Packages ==========\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import dask\n",
    "import matplotlib.cm\n",
    "from matplotlib import rcParams\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from cftime import DatetimeNoLeap\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams[\"mathtext.default\"] = 'regular'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5daa16-4811-4773-b98d-6b69626ff6cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb57b43-bbbe-414b-8075-8cda5d1ea4ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9326b-048c-48c2-84e0-5af87d06a120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Helper function to open the dataset ========\n",
    "def open_dataset(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ca849-c1f4-431c-ae0f-e6a3fb6ba92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to open and merge datasets\n",
    "def open_and_merge_datasets(folder, model, experiment_id, variables):\n",
    "    filepaths = []\n",
    "    for var in variables:\n",
    "        path = f'../../data/CMIP6/{experiment_id}/{folder}/{var}'\n",
    "        fp = glob.glob(os.path.join(path, f'CMIP.{model}.{experiment_id}.{var}_regridded.nc'))\n",
    "        if fp:\n",
    "            filepaths.append(fp[0])\n",
    "        else:\n",
    "            #print(f\"No file found for variable '{var}' in model '{model}'.\")\n",
    "            print(fp)\n",
    "\n",
    "    datasets = [xr.open_dataset(fp) for fp in filepaths]\n",
    "    ds = xr.merge(datasets)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9148cef-f033-40ac-aeef-dec5ccdb277d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79176a9d-1bde-438d-a226-9c749fc98042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_period(ds_dict, start_year=None, end_year=None):\n",
    "    '''\n",
    "    Helper function to select periods.\n",
    "    \n",
    "    Parameters:\n",
    "    ds_dict (dict): Dictionary with xarray datasets.\n",
    "    start_year (int): The start year of the period.\n",
    "    end_year (int): The end year of the period.\n",
    "    \"\"\"\n",
    "    '''\n",
    "    start_year = DatetimeNoLeap(start_year, 1, 16, 12, 0, 0, 0,has_year_zero=True) # 16th of January of start year\n",
    "    end_year = DatetimeNoLeap(end_year, 12, 16, 12, 0, 0, 0, has_year_zero=True) # 16th of December of end year\n",
    "    ds_dict = {k: v.sel(time=slice(start_year, end_year)) for k, v in ds_dict.items()}\n",
    "    \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a690d-ac2f-4875-96dc-66e8424a4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Standardize ========\n",
    "def standardize(ds_dict):\n",
    "    '''\n",
    "    Helper function to standardize datasets of a dictionary\n",
    "    '''\n",
    "    ds_dict_stand = {}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        ds_stand = (ds - ds.mean()) / ds.std()\n",
    "\n",
    "        # Preserve variable attributes from the original dataset\n",
    "        for var in ds.variables:\n",
    "            if var in ds_stand.variables:\n",
    "                ds_stand[var].attrs = ds[var].attrs\n",
    "\n",
    "        ds_stand.attrs = attrs\n",
    "        ds_dict_stand[name] = ds_stand\n",
    "        \n",
    "    return ds_dict_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb19561-1cb9-4d0d-8565-3fa7c45988e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_args_and_get_info(ds_dict, variable):\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if not isinstance(variable, str):\n",
    "        raise TypeError('variable must be a string.')\n",
    "        \n",
    "    # Dictionary to store plot titles for each statistic\n",
    "    titles = {\"mean\": \"Mean\", \"std\": \"Standard deviation of yearly means\", \"min\": \"Minimum\", \"max\": \"Maximum\", \"median\": \"Median\", \"time\": \"Time\", \"space\": \"Space\"}\n",
    "    \n",
    "    long_name = {\n",
    "        'Precipitation': 'Precipitation',\n",
    "        'Total Runoff': 'Total Runoff',\n",
    "        'Vapor Pressure Deficit': 'Vapor Pressure Deficit',\n",
    "        'Evaporation Including Sublimation and Transpiration': 'Evapotranspiration',\n",
    "        'Transpiration': 'Transpiration',\n",
    "        'Leaf Area Index': 'Leaf Area Index',\n",
    "        'Carbon Mass Flux out of Atmosphere Due to Gross Primary Production on Land [kgC m-2 s-1]': 'Gross Primary Production',\n",
    "        'Total Liquid Soil Moisture Content of 1 m Column': '1 m Soil Moisture',\n",
    "        'Total Liquid Soil Moisture Content of 2 m Column': '2 m Soil Moisture',\n",
    "        'Runoff - Precipitation': 'Runoff - Precipitation',\n",
    "        'Transpiration - Precipitation': 'Transpiration - Precipitation',\n",
    "        '(Runoff + Transpiration) - Precipitation':  '(Runoff + Transpiration) - Precipitation'\n",
    "    }\n",
    "    \n",
    "    # Data information\n",
    "    var_long_name = long_name[ds_dict[list(ds_dict.keys())[0]][variable].long_name]\n",
    "    period = f\"{ds_dict[list(ds_dict.keys())[0]].attrs['period']}\"\n",
    "    experiment_id =  ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    unit = ds_dict[list(ds_dict.keys())[0]][variable].units\n",
    "    statistic_dim = ds_dict[list(ds_dict.keys())[0]].statistic_dimension\n",
    "    statistic = ds_dict[list(ds_dict.keys())[0]].attrs['statistic']\n",
    "\n",
    "    return var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090c127-204b-4d69-9e33-5769b28e4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_args_and_get_info(ds_dict, variable):\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if not isinstance(variable, str):\n",
    "        raise TypeError('variable must be a string.')\n",
    "        \n",
    "    # Dictionary to store plot titles for each statistic\n",
    "    titles = {\"mean\": \"Mean\", \"std\": \"Standard deviation of yearly means\", \"min\": \"Minimum\", \"max\": \"Maximum\", \"median\": \"Median\", \"time\": \"Time\", \"space\": \"Space\"}\n",
    "    freq = {\"mon\": \"Monthly\"}\n",
    "    \n",
    "    long_name = {\n",
    "        'Precipitation': 'Precipitation',\n",
    "        'Total Runoff': 'Total Runoff',\n",
    "        'Vapor Pressure Deficit': 'Vapor Pressure Deficit',\n",
    "        'Evaporation Including Sublimation and Transpiration': 'Evapotranspiration',\n",
    "        'Transpiration': 'Transpiration',\n",
    "        'Leaf Area Index': 'Leaf Area Index',\n",
    "        'Carbon Mass Flux out of Atmosphere Due to Gross Primary Production on Land [kgC m-2 s-1]': 'Gross Primary Production',\n",
    "        'Total Liquid Soil Moisture Content of 1 m Column': '1 m Soil Moisture',\n",
    "        'Total Liquid Soil Moisture Content of 2 m Column': '2 m Soil Moisture',\n",
    "        'Runoff - Precipitation': 'Runoff - Precipitation',\n",
    "        'Transpiration - Precipitation': 'Transpiration - Precipitation',\n",
    "        '(Runoff + Transpiration) - Precipitation':  '(Runoff + Transpiration) - Precipitation',\n",
    "        'ET - Precipitation':  'ET - Precipitation', \n",
    "        'Negative Runoff': 'Negative Runoff',\n",
    "    }\n",
    "   \n",
    "    # Data information\n",
    "    var_long_name = ds_dict[list(ds_dict.keys())[0]][variable].long_name\n",
    "    period = f\"{ds_dict[list(ds_dict.keys())[0]].attrs['period'][0]}-{ds_dict[list(ds_dict.keys())[0]].attrs['period'][1]}\"\n",
    "    experiment_id =  ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    unit = ds_dict[list(ds_dict.keys())[0]][variable].units\n",
    "    statistic_dim = ds_dict[list(ds_dict.keys())[0]].statistic_dimension\n",
    "    statistic = ds_dict[list(ds_dict.keys())[0]].attrs['statistic']\n",
    "    frequency = freq[ds_dict[list(ds_dict.keys())[0]].frequency]\n",
    "\n",
    "    return var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54437cf1-8268-4619-b224-e6be3684894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_to_regions(ds_dict, regions):\n",
    "\n",
    "    ds_dict_region = {region: {} for region in regions.keys()}\n",
    "\n",
    "    # For each dataset, slice to each region and save in new dict\n",
    "    for ds_name, ds in ds_dict.items():\n",
    "        for region, bounds in regions.items():\n",
    "            ds_dict_region[region][ds_name] = ds.sel(lat=bounds['lat'], lon=bounds['lon'])\n",
    "            \n",
    "    return ds_dict_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a73aa-e566-4d11-8dd0-c6de7e0cc432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_ensemble(ds_dict):\n",
    "    # Combine all datasets into one larger dataset\n",
    "    combined = xr.concat(ds_dict.values(), dim='ensemble')\n",
    "    # Compute the ensemble metric\n",
    "    ds_dict['Ensemble mean'] = getattr(combined, 'mean')(dim='ensemble')#, skipna=True) # use getattr to call method by string name\n",
    "    \n",
    "    # Preserve variable attributes from the original dataset\n",
    "    for var in ds_dict['Ensemble mean'].variables:\n",
    "        ds_dict['Ensemble mean'][var].attrs = ds_dict[list(ds_dict.keys())[0]][var].attrs\n",
    "    \n",
    "    ds_dict['Ensemble mean'].attrs = {\"period\" : ds_dict[list(ds_dict.keys())[0]].attrs['period'],\n",
    "                           \"statistic\" : \"mean\", # use variable metric here\n",
    "                           \"statistic_dimension\" : \"time\",\n",
    "                           \"experiment_id\": \"ssp370-historical\", \n",
    "                           \"source_id\" : f\"Ensemble mean\",\n",
    "                           \"frequency\":  ds_dict[list(ds_dict.keys())[0]].attrs['frequency']} \n",
    "    \n",
    "    ds_dict['Ensemble median'] = getattr(combined, 'median')(dim='ensemble')#, skipna=True) # use getattr to call method by string name\n",
    "    \n",
    "    ds_dict['Ensemble median'].attrs = {\"period\" : ds_dict[list(ds_dict.keys())[0]].attrs['period'],\n",
    "                           \"statistic\" : \"median\", # use variable metric here\n",
    "                           \"statistic_dimension\" : \"time\",\n",
    "                           \"experiment_id\": \"ssp370-historical\", \n",
    "                           \"source_id\": f\"Ensemble median\",\n",
    "                           \"frequency\":  ds_dict[list(ds_dict.keys())[0]].attrs['frequency']} \n",
    "    \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a2736-eb0f-466e-b547-4d293e8a5af5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766cd9d-393f-4db9-b0a3-744388ad88c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic_single(ds, statistic, dimension, yearly_mean=True):\n",
    "    if dimension == \"time\":\n",
    "        stat_ds = getattr(ds, statistic)(\"time\", keep_attrs=True, skipna=True)\n",
    "        stat_ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "    if dimension == \"space\":\n",
    "        # Assign the period attribute before grouping by year\n",
    "        ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "        if yearly_mean:\n",
    "            ds = ds.groupby('time.year').mean('time', keep_attrs=True, skipna=True)\n",
    "            ds.attrs['mean'] = 'yearly mean'\n",
    "            \n",
    "        \n",
    "        #get the weights, apply on data, and compute statistic\n",
    "        weights = np.cos(np.deg2rad(ds.lat))\n",
    "        weights.name = \"weights\"\n",
    "        ds_weighted = ds.weighted(weights)\n",
    "        stat_ds = getattr(ds_weighted, statistic)((\"lon\", \"lat\"), keep_attrs=True, skipna=True)\n",
    "    \n",
    "    stat_ds.attrs['statistic'] = statistic\n",
    "    stat_ds.attrs['statistic_dimension'] = dimension\n",
    "\n",
    "    return stat_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadad33e-375e-4f6f-94b1-918dce7e2f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic(ds_dict, statistic, dimension, start_year=None, end_year=None, yearly_mean=True):\n",
    "    \"\"\"\n",
    "    Computes the specified statistic for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        statistic (str): The statistic to compute, which can be one of 'mean', 'std', 'min', 'var', or 'median'.\n",
    "        dimension (str): The dimension to compute over, which can be 'time' or 'space'.\n",
    "        start_year (str, optional): The start year of the period to compute the statistic over.\n",
    "        end_year (str, optional): The end year of the period to compute the statistic over.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with computed statistic for each dataset.\n",
    "    \"\"\"\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if statistic not in [\"mean\", \"std\", \"min\", \"max\", \"var\", \"median\"]:\n",
    "        raise ValueError(f\"Invalid statistic '{statistic}' specified.\")\n",
    "    if dimension not in [\"time\", \"space\"]:\n",
    "        raise ValueError(f\"Invalid dimension '{dimension}' specified.\")\n",
    "\n",
    "    # Select period\n",
    "    if start_year is not None and end_year is not None:\n",
    "        select_period(ds_dict, start_year=start_year, end_year=end_year)\n",
    "        \n",
    "        \n",
    "    # Use multiprocessing to compute the statistic for each dataset in parallel\n",
    "    with mp.Pool() as pool:\n",
    "        results = pool.starmap(compute_statistic_single, [(ds, statistic, dimension, yearly_mean) for ds in ds_dict.values()])\n",
    "\n",
    "    return dict(zip(ds_dict.keys(), results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55955a2d-2e0a-41ba-bdd8-d0d8b8211506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from itertools import combinations\n",
    "\n",
    "# Define a function to compute the metrics\n",
    "def compute_metrics_for_pair(args):\n",
    "    df, var1, var2, metrics = args\n",
    "    valid_values = np.logical_and(np.isfinite(df[var1]), np.isfinite(df[var2]))\n",
    "\n",
    "    # Compute metrics\n",
    "    X = df[var1][valid_values].values.reshape(-1, 1)\n",
    "    y = df[var2][valid_values].values\n",
    "    metric_dict = {}\n",
    "\n",
    "    if 'rmse_rf' in metrics:\n",
    "        rf = RandomForestRegressor()\n",
    "        rf.fit(X, y)\n",
    "        y_pred_rf = rf.predict(X)\n",
    "        rmse_rf = np.sqrt(mean_squared_error(y, y_pred_rf))\n",
    "        metric_dict['rmse_rf'] = rmse_rf\n",
    "    \n",
    "    if 'rmse_lr' or 'r2_lr' in metrics:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X, y)\n",
    "        y_pred_lr = lr.predict(X)\n",
    "        if 'rmse_lr' in metrics:\n",
    "            rmse_lr = np.sqrt(mean_squared_error(y, y_pred_lr))\n",
    "            metric_dict['rmse_lr'] = rmse_lr\n",
    "        if 'r2_lr' in metrics:\n",
    "            r2_lr = r2_score(y, y_pred_lr) # compute the R^2 (coefficient of determination)\n",
    "            metric_dict['r2_lr'] = r2_lr\n",
    "\n",
    "    if 'pearson' in metrics:\n",
    "        r_pearson = pearsonr(X.flatten(), y)[0]\n",
    "        metric_dict['pearson'] = r_pearson\n",
    "\n",
    "    if 'spearman' in metrics:\n",
    "        r_spearman = spearmanr(X.flatten(), y)[0]\n",
    "        metric_dict['spearman'] = r_spearman\n",
    "        \n",
    "    if 'kendalltau' in metrics:\n",
    "        tau_kendall = kendalltau(X.flatten(), y)[0]\n",
    "        metric_dict['kendalltau'] = tau_kendall\n",
    "\n",
    "    return (var1, var2, metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d176a36-95a5-4a18-b46f-a311e49243f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def precompute_metrics(ds_dict, variables, metrics=['pearson']):\n",
    "    # Initialize the results dictionary\n",
    "    results_dict = {metric: {} for metric in metrics}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        # Create a DataFrame with all the variables\n",
    "        df = pd.DataFrame({var: ds[var].values.flatten() for var in variables})\n",
    "        \n",
    "        # Define all pairs of variables\n",
    "        pairs = list(permutations(variables, 2))  # <-- Change here\n",
    "        args = [(df, var1, var2, metrics) for var1, var2 in pairs]\n",
    "\n",
    "        # Use a multiprocessing pool to compute the metrics for all pairs\n",
    "        with Pool() as p:\n",
    "            results = p.map(compute_metrics_for_pair, args)\n",
    "        \n",
    "        # Store the results in the results_dict\n",
    "        for var1, var2, metric_dict in results:\n",
    "            for metric, value in metric_dict.items():\n",
    "                # Ensure the keys exist in the dictionary\n",
    "                results_dict[metric].setdefault(name, {}).setdefault(f'{var1}_{var2}', value)\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1de1242-8b90-44db-903d-6ae01d25b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_yearly_correlations(ds_dict, variable_pairs, start_year=None, end_year=None, corr_type='pearson'):\n",
    "    \"\"\"\n",
    "    Calculates yearly Pearson correlation for the given pairs of variables from the same model.\n",
    "\n",
    "    Parameters:\n",
    "    ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "    variable_pairs (list of tuples): The pairs of variables to calculate the correlation for.\n",
    "    start_year (int): The start year of the period to compute the correlation over.\n",
    "    end_year (int): The end year of the period to compute the correlation over.\n",
    "    corr_type (str): The type of correlation coefficient to compute. Can be either 'pearson', 'spearman', or 'kendall'.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with a DataFrame for each dataset, where each DataFrame contains the yearly Pearson\n",
    "        correlation for each pair of variables.\n",
    "    \"\"\"\n",
    "    # Map for complete metric names and symbols\n",
    "    metric_map = {\n",
    "        'r2_lr': ('Coefficient of Determination', 'R²'),\n",
    "        'pearson': ('Pearson Correlation Coefficient', 'r'),\n",
    "        'spearman': ('Spearman Rank Correlation Coefficient', 'ρ'),\n",
    "        'kendalltau': ('Kendall Rank Correlation Coefficient', 'τ')\n",
    "    }\n",
    "    \n",
    "    # Select period\n",
    "    if start_year is not None and end_year is not None:\n",
    "        select_period(ds_dict, start_year=start_year, end_year=end_year)\n",
    "\n",
    "    yearly_correlations = {}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        yearly_corr_dict = {}\n",
    "\n",
    "        # Resample to yearly data\n",
    "        ds_yearly = ds.resample(time='1Y').mean()\n",
    "        \n",
    "        for var1, var2 in variable_pairs:\n",
    "            # Prepare empty list for yearly correlations\n",
    "            yearly_correlations_values = []\n",
    "            yearly_correlations_years = []\n",
    "\n",
    "            # Get the unique years\n",
    "            years = ds_yearly['time'].dt.year\n",
    "            for year in np.unique(years):\n",
    "                # Select the data for this year\n",
    "                ds_year = ds_yearly.sel(time=f'{year}')\n",
    "                \n",
    "                # Calculate the correlation for this year and append to the list\n",
    "                if corr_type == 'pearson':\n",
    "                    corr_value = xr.corr(ds_year[var1], ds_year[var2], dim=['lon', 'lat'])\n",
    "                    \n",
    "                elif corr_type == 'spearman':\n",
    "                    ds_year_stacked = ds_year.stack(z=('lon', 'lat'))\n",
    "                    df = ds_year_stacked.to_dataframe()\n",
    "                    #df = np.isfinite(df[var1])\n",
    "                    corr_value, _ = spearmanr(df[var1], df[var2])\n",
    "                    corr_value = xr.DataArray(corr_value)\n",
    "                    \n",
    "                elif corr_type == 'kendall':\n",
    "                    ds_year_stacked = ds_year.stack(z=('lon', 'lat'))\n",
    "                    df = ds_year_stacked.to_dataframe()\n",
    "                    corr_value, _ = kendalltau(df[var1], df[var2])\n",
    "                    corr_value = xr.DataArray(corr_value)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid correlation type. Expected 'pearson', 'spearman', or 'kendall'.\")\n",
    "\n",
    "                # Calculate the correlation for this year and append to the list\n",
    "                #corr_value = xr.corr(ds_year[var1], ds_year[var2], dim=['lon', 'lat'])\n",
    "\n",
    "                yearly_correlations_values.append(float(corr_value.values)) # Extract the scalar value\n",
    "                yearly_correlations_years.append(year)\n",
    "\n",
    "            # Store in the yearly_corr_dict\n",
    "            yearly_corr_dict[f'{var1}-{var2}'] = xr.DataArray(yearly_correlations_values, dims='time', coords={'time': yearly_correlations_years})\n",
    "\n",
    "        # Create a Dataset from the yearly_corr_dict and store in the yearly_correlations dict\n",
    "        yearly_correlations[name] = xr.Dataset(yearly_corr_dict)\n",
    "        yearly_correlations[name].attrs = {'Metric': metric_map[corr_type][0],\n",
    "                                           'Metric_sign': metric_map[corr_type][1]\n",
    "                                          }\n",
    "\n",
    "    return yearly_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9381cb-6944-4552-b6c7-aac1b6a3c6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_stats(ds_dict):\n",
    "    \"\"\"\n",
    "    Compute yearly mean of each variable in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds_dict (dict): The input dictionary of xarray.Dataset.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are the dataset names and the values are another dictionary.\n",
    "          This inner dictionary has keys as variable names and values as DataArray of yearly means.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    for model, ds in ds_dict.items():\n",
    "        # Compute the yearly mean\n",
    "        yearly_ds = ds.resample(time='1Y').mean()\n",
    "\n",
    "        stats[model] = {}\n",
    "        for var in yearly_ds.data_vars:\n",
    "            # Compute the spatial mean\n",
    "            spatial_mean = yearly_ds[var].mean(dim=['lat', 'lon'])\n",
    "            \n",
    "            # Store the yearly mean values\n",
    "            stats[model][var] = spatial_mean\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbbc45-b47c-461b-99db-d20d9fa0625d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_yearly_means(ds_dict_region):\n",
    "    yearly_means_dict = {}\n",
    "\n",
    "    # For each dataset, compute the yearly mean over the 'time', 'lat', and 'lon' dimensions\n",
    "    for region, ds_dict in ds_dict_region.items():\n",
    "        yearly_means_dict[region] = {}\n",
    "        for ds_name, ds in ds_dict.items():\n",
    "            # Create weights\n",
    "            weights = np.cos(np.deg2rad(ds.lat))\n",
    "\n",
    "            # Compute the yearly mean\n",
    "            ds_yearly = ds.groupby('time.year').mean('time')\n",
    "\n",
    "            # Apply the weights and calculate the spatial mean\n",
    "            ds_weighted = ds_yearly.weighted(weights)\n",
    "            yearly_means_dict[region][ds_name] = ds_weighted.mean(('lat', 'lon'))\n",
    "\n",
    "    return yearly_means_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53592816-7215-44ef-8f2d-55e90f54e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_coefficients(ds_dict, variables, time_dim='time', yearly_corr=False):\n",
    "    \"\"\"\n",
    "    Compute the correlation coefficients for different variable combinations and\n",
    "    store them in a new dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        variables (list): A list of variables for which to compute the correlation coefficients.\n",
    "        time_dim (str): The name of the time dimension in the datasets. Default is 'time'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary where keys are the same as in the input dictionary, and each value is an xarray Dataset\n",
    "              containing correlation coefficients for different variable combinations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a new dictionary to store the correlation data\n",
    "    ds_dict_corr = {}\n",
    "\n",
    "    # Iterate over all combinations of two variables\n",
    "    for var1, var2 in combinations(variables, 2):\n",
    "        # Iterate over all datasets in the dictionary\n",
    "        for ds_name, ds in ds_dict.items():\n",
    "            # Check if both variables exist in the current dataset\n",
    "            if var1 in ds and var2 in ds:\n",
    "                if yearly_corr:\n",
    "                    # Resample the dataset to yearly data\n",
    "                    ds_corr = ds.resample({time_dim: 'Y'}).mean()\n",
    "                else:\n",
    "                    ds_corr = ds\n",
    "                \n",
    "                # If the dataset is not yet in the new dictionary, create a new xarray Dataset for it\n",
    "                if ds_name not in ds_dict_corr:\n",
    "                    ds_dict_corr[ds_name] = xr.Dataset()\n",
    "\n",
    "                # Compute the correlation coefficients and add them as a new DataArray to the Dataset\n",
    "                ds_dict_corr[ds_name][f'{var1} x {var2}'] = xr.corr(ds_corr[var1], ds_corr[var2], dim=time_dim)\n",
    "                ds_dict_corr[ds_name].attrs = ds_dict[ds_name].attrs\n",
    "                ds_dict_corr[ds_name].attrs['Metric'] = 'Pearson Correlation Coefficient'\n",
    "                ds_dict_corr[ds_name].attrs['Metric_sign'] = 'r'\n",
    "                \n",
    "                if yearly_corr:\n",
    "                    ds_dict_corr[ds_name].attrs['means'] = 'yearly means'\n",
    "                else:\n",
    "                    ds_dict_corr[ds_name].attrs['means'] = 'monthly means'\n",
    "                \n",
    "                \n",
    "    return ds_dict_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e6ced-227e-41b6-972f-79bb4e110c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_change_corr(ds_dict_hist_corr, ds_dict_ssp370_corr):\n",
    "    ds_dict_corr_change = {}\n",
    "\n",
    "    for name, ds in ds_dict_hist_corr.items():\n",
    "        ds_dict_corr_change[name] = ds_dict_ssp370_corr[name] - ds\n",
    "        ds_dict_corr_change[name].attrs = {'period': 'Change Correlation SSP370 - Historical',\n",
    "                                      'statistic': 'mean',\n",
    "                                      'statistic_dimension':  'time',\n",
    "                                      'experiment_id': 'ssp370-historical',\n",
    "                                      'source_id': name,\n",
    "                                      'Metric': 'Pearson Correlation Coefficient',\n",
    "                                      'Metric_sign': 'r'\n",
    "                                    }\n",
    "        if ds_dict_hist_corr[name].attrs['means'] == 'yearly means' and ds_dict_ssp370_corr[name].attrs['means'] == 'yearly means':\n",
    "            ds_dict_corr_change[name].attrs['means'] = 'yearly means'\n",
    "        elif ds_dict_hist_corr[name].attrs['means'] == 'monthly means' and ds_dict_ssp370_corr[name].attrs['means'] == 'monthly means':\n",
    "            ds_dict_corr_change[name].attrs['means'] = 'monthly means'\n",
    "        else:\n",
    "            raise ValueError(f\"Computing change between seasonal and monthly mean data.\")\n",
    "            \n",
    "        return ds_dict_corr_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3356085-14f7-460a-afdd-c52cb2dcb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_change(ds_dict_hist_mean, ds_dict_ssp370_mean, relative_change=False, iqr=False):\n",
    "    ds_dict_change = {}\n",
    "\n",
    "    for name, ds in ds_dict_hist_mean.items():\n",
    "        # Compute either absolute or relative change\n",
    "        if relative_change:\n",
    "            \n",
    "            # Convert temperature back to Kelvin to not have negative and positive values\n",
    "            attrs = ds_dict_hist_mean[name]['tas'].attrs\n",
    "            ds_dict_hist_mean[name]['tas'] = ds_dict_hist_mean[name]['tas'] + 273.15\n",
    "            ds_dict_hist_mean[name]['tas'].attrs = attrs\n",
    "            ds_dict_hist_mean[name]['tas'].attrs['units'] = 'K'\n",
    "            \n",
    "            attrs = ds_dict_ssp370_mean[name]['tas'].attrs\n",
    "            ds_dict_ssp370_mean[name]['tas'] = ds_dict_ssp370_mean[name]['tas'] + 273.15\n",
    "            ds_dict_ssp370_mean[name]['tas'].attrs = attrs\n",
    "            ds_dict_ssp370_mean[name]['tas'].attrs['units'] = 'K'\n",
    "            \n",
    "            ds_f = ds_dict_ssp370_period_metric_rel[name]\n",
    "\n",
    "            # Compute relative change only where ds is not 0\n",
    "            rel_change = ds.where(ds != 0)\n",
    "            rel_change = ((ds_f - rel_change) / abs(rel_change)) * 100\n",
    "\n",
    "            # Where ds was 0, set the corresponding relative change to np.nan\n",
    "            rel_change = rel_change.where(ds != 0)\n",
    "\n",
    "            if iqr:\n",
    "                # Compute the IQR and use it for filtering\n",
    "                q1 = rel_change.quantile(0.25)\n",
    "                q3 = rel_change.quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 4 * iqr\n",
    "                upper_bound = q3 + 4 * iqr\n",
    "\n",
    "                # Apply the condition\n",
    "                condition = (rel_change >= lower_bound) & (rel_change <= upper_bound)\n",
    "                rel_change = rel_change.where(condition)\n",
    "\n",
    "            ds_dict_change[name] = rel_change\n",
    "            \n",
    "        else:\n",
    "            ds_dict_change[name] = ds_dict_ssp370_mean[name] - ds\n",
    "            \n",
    "        ds_dict_change[name].attrs = {'period': 'Change SSP370 - Historical',\n",
    "                                      'statistic': ds_dict_ssp370_mean[name].statistic,\n",
    "                                      'statistic_dimension':  ds_dict_ssp370_mean[name].statistic_dimension,\n",
    "                                      'experiment_id': 'ssp370-historical',\n",
    "                                      'source_id': ds_dict_ssp370_mean[name].source_id,\n",
    "                                      'frequency': ds_dict_ssp370_mean[name].frequency\n",
    "                                    }\n",
    "        for variables in ds:\n",
    "            ds_dict_change[name][variables].attrs = ds_dict_ssp370_mean[name][variables].attrs\n",
    "            if relative_change:\n",
    "                ds_dict_change[name][variables].attrs['units_rel'] = '%'\n",
    "                ds_dict_change[name].attrs['change'] = 'Relative Change'\n",
    "            else:\n",
    "                ds_dict_change[name].attrs['change'] = 'Absolute Change'\n",
    "             \n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354754c-b13c-4650-b31b-7ae9e9e424ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_nbwfp(ds_dict):\n",
    "\n",
    "    for model, ds in ds_dict.items():\n",
    "        nbwfp = (ds['mrro']-ds['tran'])/ds['pr']\n",
    "\n",
    "        # Replace infinite values with NaN\n",
    "        nbwfp = xr.where(np.isinf(nbwfp), float('nan'), nbwfp)\n",
    "\n",
    "        # Set all values above 2 and below -2 to NaN\n",
    "        nbwfp = xr.where(nbwfp > 2, float('nan'), nbwfp)\n",
    "        nbwfp = xr.where(nbwfp < -2, float('nan'), nbwfp)\n",
    "\n",
    "        ds_dict[model]['nbwfp'] = nbwfp\n",
    "        ds_dict[model]['nbwfp'].attrs = {'long_name': 'Net Blue Water Flux / Precipitation',\n",
    "                             'units': ''}\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c6728-aa37-48ed-88ee-2bca84fa1521",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39238c09-3874-41ae-a859-a782bb16917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_correlations(yearly_correlations, variable_pairs, target_variable):\n",
    "    \"\"\"\n",
    "    Plots the time series of correlations for each variable pair that includes the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    yearly_correlations (dict): The output from calculate_yearly_correlations.\n",
    "    variable_pairs (list of tuples): The pairs of variables that the correlations were calculated for.\n",
    "    target_variable (str): The variable that must be included in a pair for it to be plotted.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the number of plots\n",
    "    n_plots = sum([var1 == target_variable or var2 == target_variable for var1, var2 in variable_pairs])\n",
    "\n",
    "    # Calculate the dimensions of the grid of subplots\n",
    "    grid_size = math.ceil(math.sqrt(n_plots))\n",
    "    \n",
    "    # Create the figure\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15), sharex=True, sharey=True)\n",
    "\n",
    "    # Flatten the axes\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Create an index for the current plot\n",
    "    i_plot = 0\n",
    "\n",
    "    # Prepare a list to store handles and labels for the legend\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for var1, var2 in variable_pairs:\n",
    "        if var1 == target_variable or var2 == target_variable:\n",
    "            # Select the current axes\n",
    "            ax = axs[i_plot]\n",
    "            \n",
    "            # Construct the correlation variable name\n",
    "            corr_var = f'{var1}-{var2}'\n",
    "\n",
    "            # Prepare a list to store correlations of all models\n",
    "            all_corrs = []\n",
    "\n",
    "            for name, ds in yearly_correlations.items():\n",
    "                # Check if this variable exists in the Dataset\n",
    "                if corr_var in ds:\n",
    "                    # Plot the time series of the correlation\n",
    "                    line, = ax.plot(ds['time'], ds[corr_var], label=name)\n",
    "\n",
    "                    # Append to all_corrs\n",
    "                    all_corrs.append(ds[corr_var])\n",
    "\n",
    "                    # Append to handles and labels if not already present\n",
    "                    if name not in labels:\n",
    "                        handles.append(line)\n",
    "                        labels.append(name)\n",
    "\n",
    "            # Compute the mean correlation across all models\n",
    "            mean_corr = xr.concat(all_corrs, dim='model').mean(dim='model')\n",
    "            mean_line, = ax.plot(mean_corr['time'], mean_corr, color='black', linestyle='--')\n",
    "\n",
    "            ax.set_xlabel('Year')\n",
    "            ax.set_ylabel('Correlation')\n",
    "            ax.set_title(f'{var1} vs {var2}')\n",
    "            ax.grid(True)\n",
    "\n",
    "            # Increment the plot index\n",
    "            i_plot += 1\n",
    "\n",
    "    # Add 'Mean' to the legend\n",
    "    handles.append(mean_line)\n",
    "    labels.append('Mean')\n",
    "\n",
    "    # Show the figure with a legend\n",
    "    fig.legend(handles, labels, loc='center right', bbox_to_anchor=(1.1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10db210-8757-4858-ae1e-4f89e7949440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_mean_correlations(correlations_hist, correlations_ssp370, variable_pairs, target_variable, scale_axis=False, variable_captions=None):\n",
    "    \"\"\"\n",
    "    Plots the mean correlations for each variable pair that includes the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    correlations_hist (dict): The output from calculate_correlations for the historical period.\n",
    "    correlations_ssp370 (dict): The output from calculate_correlations for the SSP370 scenario.\n",
    "    variable_pairs (list of tuples): The pairs of variables that the correlations were calculated for.\n",
    "    target_variable (str): The variable that must be included in a pair for it to be plotted.\n",
    "    scale_axis (bool): Whether to scale the y-axis according to metric value ranges. Default is False.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get info\n",
    "    metric = correlations_hist[list(correlations_hist.keys())[0]].Metric\n",
    "    metric_sign = correlations_hist[list(correlations_hist.keys())[0]].Metric_sign\n",
    "    \n",
    "    # Filter variable pairs\n",
    "    variable_pairs = [(var1, var2) for var1, var2 in variable_pairs if var1 == target_variable or var2 == target_variable]\n",
    "\n",
    "    # Calculate the number of plots and dimensions of the grid of subplots\n",
    "    n_plots = len(variable_pairs)\n",
    "    n_cols = min(n_plots, 3)  # Maximum 3 plots in a row\n",
    "    n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "    # Create the figure\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows), squeeze=False)\n",
    "    axs = axs.flatten()  # Flatten the axes array\n",
    "    \n",
    "    for i, (var1, var2) in enumerate(variable_pairs):\n",
    "        # Prepare lists to store yearly correlation values\n",
    "        yearly_corr_hist, yearly_corr_ssp370 = [], []\n",
    "\n",
    "        for name in correlations_hist.keys():\n",
    "            # Extract the yearly mean values for the historical period\n",
    "            yearly_corr_hist.append(correlations_hist[name][f'{var1}-{var2}'].values)\n",
    "\n",
    "            # Extract the yearly mean values for the SSP370 scenario\n",
    "            yearly_corr_ssp370.append(correlations_ssp370[name][f'{var1}-{var2}'].values)\n",
    "\n",
    "        # Compute the box plot positions\n",
    "        positions = np.arange(len(correlations_hist.keys()))\n",
    "\n",
    "        # Select the current axes\n",
    "        ax = axs[i]\n",
    "\n",
    "        # Define an offset for x-values to place box plots for different periods side by side\n",
    "        offset = 0.15\n",
    "\n",
    "        # Plot the box plots for the historical period\n",
    "        ax.boxplot(yearly_corr_hist, positions=positions-offset, widths=0.3, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='cornflowerblue'), medianprops=dict(color='black'), \n",
    "                   showfliers=True)\n",
    "\n",
    "        # Plot the box plots for the SSP370 scenario\n",
    "        ax.boxplot(yearly_corr_ssp370, positions=positions+offset, widths=0.3, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='sandybrown'), medianprops=dict(color='black'), \n",
    "                   showfliers=True)\n",
    "\n",
    "        # Set the x-ticks labels and the title\n",
    "        ax.set_ylabel(f'{metric_sign}')\n",
    "        ax.set_title(f'{variable_captions.get(var1, var1)} x {variable_captions.get(var2, var2)}', fontsize=9)\n",
    "        ax.set_xticks(positions)\n",
    "        ax.set_xticklabels(list(correlations_hist.keys()), rotation=90)\n",
    "        \n",
    "    fig.suptitle(f'Yearly {metric} for Historical (1985-2014) and SSP370 (2071-2100) Period', fontsize=12, y=1.0)\n",
    "    \n",
    "    # Set a legend\n",
    "    axs[0].legend([Patch(facecolor='cornflowerblue'), Patch(facecolor='sandybrown')], ['Historical', 'SSP370'])\n",
    "\n",
    "    # Handle empty subplots in case n_plots is less than n_rows * n_cols\n",
    "    for i in range(n_plots, n_rows*n_cols):\n",
    "        fig.delaxes(axs[i])\n",
    "    \n",
    "    # Handle y-axis scaling\n",
    "    if scale_axis:\n",
    "        for ax in axs:\n",
    "            ax.set_ylim([-1, 1] if metric != 'r2_lr' else [0, 1])\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    suffix = \"_scaled_axis\" if scale_axis else \"\"\n",
    "    filename = f\"{metric}_changes_{target_variable}{suffix}.png\"\n",
    "    \n",
    "    savepath = f'../../results/CMIP6/yearly_metrics_comparison'\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    filepath = os.path.join(savepath, filename)\n",
    "\n",
    "    fig.savefig(filepath, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555de63-1237-4ed3-b496-48f9f835eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_corr_change_plot(ds_dict, target_variable, full_var_names_and_unit, cmap='viridis', save_fig=False, file_format='png'):\n",
    "    \"\"\"\n",
    "    Plots a map of the specified statistic of the given variable for the Ensemble_mean dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        target_variable (str): The target variable to plot correlations with.\n",
    "        cmap (str): The name of the colormap to use for the plot. Default is 'viridis'.\n",
    "        save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "        file_format (str): The format of the saved figure. Default is 'png'.\n",
    "    \"\"\"\n",
    "    # Info\n",
    "    experiment_id = ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    metric = ds_dict[list(ds_dict.keys())[0]].Metric\n",
    "    metric_sign = ds_dict[list(ds_dict.keys())[0]].Metric_sign\n",
    "    means = ds_dict[list(ds_dict.keys())[0]].attrs['means']\n",
    "    target_var_long_name = full_var_names_and_unit[target_variable][0]\n",
    "    \n",
    "    # Create a figure\n",
    "    n_cols = 3  # Set number of columns to 3\n",
    "    n_rows = 3  # Set number of rows to 3\n",
    "    \n",
    "    fig = plt.figure(figsize=[12 * n_cols, 6.5 * n_rows])  # Start with a blank figure, without subplots\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Loop over datasets and plot the requested statistic\n",
    "    subplot_counter = 0\n",
    "\n",
    "    # Get the Ensemble_mean dataset\n",
    "    ensemble_ds = ds_dict.get(\"Ensemble_mean\", None)\n",
    "\n",
    "    if ensemble_ds is None:\n",
    "        print(\"Ensemble_mean dataset not found.\")\n",
    "        return None\n",
    "\n",
    "    for variable in ensemble_ds.variables:\n",
    "        if not (f'{target_variable} x ' in variable or f' x {target_variable}' in variable):\n",
    "            continue\n",
    "\n",
    "        # Add a new subplot with a cartopy projection\n",
    "        ax = fig.add_subplot(n_rows, n_cols, subplot_counter+1, projection=ccrs.Robinson())\n",
    "\n",
    "        data_to_plot = ensemble_ds[variable]\n",
    "        im = data_to_plot.plot(ax=ax, cmap=cmap, vmin = -1, vmax = 1, transform=ccrs.PlateCarree(), add_colorbar=False)  # Added a cartopy transform to the plot and cmap parameter\n",
    "        \n",
    "        if f'{target_variable} x ' in variable:\n",
    "            corr_var = variable.replace(f'{target_variable} x ', '')\n",
    "        elif f' x {target_variable}' in variable:\n",
    "            corr_var = variable.replace(f' x {target_variable}', '')\n",
    "        else:\n",
    "            continue\n",
    "        corr_var_long_name = full_var_names_and_unit[corr_var][0]\n",
    "        ax.set_title(f'{target_var_long_name} x {corr_var_long_name}', fontsize=18)  # Use the long names in the title\n",
    "        ax.coastlines()  # Adds lines around the continents\n",
    "\n",
    "        subplot_counter += 1\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # Add a common colorbar at the bottom of the plots\n",
    "    cbar = fig.colorbar(im, ax=fig.axes, orientation='horizontal', fraction=0.02, pad=0.04, aspect=75, shrink=0.4)\n",
    "    \n",
    "\n",
    "    # Set colorbar ticks\n",
    "    cbar.set_ticks([-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75])\n",
    "    \n",
    "    \n",
    "    # Set tick size\n",
    "    cbar.ax.tick_params(labelsize=20)  # Adjust size as needed\n",
    "    \n",
    "    # Set colorbar label\n",
    "    cbar.set_label(f'{metric_sign} change', size=26)  # Adjust size as needed\n",
    "    \n",
    "    # Set figure title with first and last year of dataset \n",
    "    if experiment_id == 'historical' or experiment_id == 'ssp370':\n",
    "        fig.suptitle(f\"{metric} ({experiment_id}) of Ensemble Mean for Variable Combinations with {target_var_long_name} ({means})\", fontsize=20, y=0.9)\n",
    "    elif experiment_id == 'ssp370-historical':\n",
    "        fig.suptitle(f\"{metric} Change ({experiment_id}) of Ensemble Mean for Variable Combinations with {target_var_long_name} ({means})\", fontsize=20, y=0.9)\n",
    "    \n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        if experiment_id == 'historical' or experiment_id == 'ssp370':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename = f'ensmean_{target_variable}_correlations.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "        elif experiment_id == 'ssp370-historical':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename =f'ensmean_{target_variable}_correlations_change.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c30d8-7d61-497f-9fd6-689ad3634fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_maps(ds_dict, target_variable_combination, cmap='viridis', save_fig=False, file_format='png'):\n",
    "    \"\"\"\n",
    "    Plots a map of the specified variable combination for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        target_variable_combination (str): The target variable combination to plot.\n",
    "        cmap (str): The name of the colormap to use for the plot. Default is 'viridis'.\n",
    "        save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "        file_format (str): The format of the saved figure. Default is 'png'.\n",
    "    \"\"\"\n",
    "    # Info\n",
    "    experiment_id = ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    metric = ds_dict[list(ds_dict.keys())[0]].Metric\n",
    "    metric_sign = ds_dict[list(ds_dict.keys())[0]].Metric_sign\n",
    "    means = ds_dict[list(ds_dict.keys())[0]].attrs['means']\n",
    "    \n",
    "    # Create a figure\n",
    "    n_datasets_with_var = sum([1 for ds in ds_dict.values() if target_variable_combination in ds.variables])\n",
    "    n_cols = 3  # Set number of columns to 3\n",
    "    n_rows = math.ceil(n_datasets_with_var / n_cols)  # Calculate rows\n",
    "    \n",
    "    fig = plt.figure(figsize=[12 * n_cols, 6.5 * n_rows])  # Start with a blank figure, without subplots\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Loop over datasets and plot the requested variable combination\n",
    "    subplot_counter = 0\n",
    "    for i, (name, ds) in enumerate(ds_dict.items()):\n",
    "        if target_variable_combination not in ds.variables:\n",
    "            continue\n",
    "\n",
    "        # Add a new subplot with a cartopy projection\n",
    "        ax = fig.add_subplot(n_rows, n_cols, subplot_counter+1, projection=ccrs.Robinson())\n",
    "        \n",
    "        data_to_plot = ds[target_variable_combination]\n",
    "        im = data_to_plot.plot(ax=ax, cmap=cmap, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "        ax.set_title(name, fontsize=18)\n",
    "        ax.coastlines()  # Adds lines around the continents\n",
    "\n",
    "        subplot_counter += 1\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # Add a common colorbar at the bottom of the plots\n",
    "    cbar = fig.colorbar(im, ax=fig.axes, orientation='horizontal', fraction=0.02, pad=0.04, aspect=75, shrink=0.4)\n",
    "    \n",
    "    # Set tick size\n",
    "    cbar.ax.tick_params(labelsize=14)  # Adjust size as needed\n",
    "    \n",
    "    # Set colorbar label\n",
    "    cbar.set_label(metric_sign, size=18)  # Adjust size as needed\n",
    "    \n",
    "    # Set figure title with first and last year of dataset \n",
    "    if experiment_id == 'historical' or 'ssp370':\n",
    "        fig.suptitle(f\"{metric} ({experiment_id}) for Variable Combination {target_variable_combination} ({means})\", fontsize=20, y=0.9)\n",
    "    elif experiment_id == 'ssp370-historical':\n",
    "        fig.suptitle(f\"{metric} Change ({experiment_id}) for Variable Combination {target_variable_combination} ({means})\", fontsize=20, y=0.9)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        if experiment_id == 'historical' or 'ssp370':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename = f'{target_variable_combination}_correlation.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "        elif experiment_id == 'ssp370-historical':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', 'comparison', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename =f'{target_variable_combination}_correlation_change.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173177e7-1327-46be-9952-4ba5d9d45155",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343c372-5fab-4276-9c97-171ecae7340b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "variables=['tas', 'pr', 'vpd', 'evspsbl', 'mrro', 'lmrso_1m', 'lmrso_2m', 'tran', 'lai', 'gpp', 'EI', 'wue']\n",
    "experiment_id = 'historical'\n",
    "source_id = ['TaiESM1', 'BCC-CSM2-MR',  'CanESM5', 'CNRM-CM6-1', 'CNRM-ESM2-1', 'IPSL-CM6A-LR', 'UKESM1-0-LL', 'MPI-ESM1-2-LR', 'CESM2-WACCM', 'NorESM2-MM', 'Ensemble mean', 'Ensemble median'] #\n",
    "folder='preprocessed'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# Create dictionary using a dictionary comprehension and Dask\n",
    "ds_dict_hist = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, variables) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f749cba-efb8-4532-9e4b-a7b880530fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= Have a look into the data ==============\n",
    "print(ds_dict_hist.keys())\n",
    "ds_dict_hist[list(ds_dict_hist.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97cb32-6320-4eb2-ac31-96755b267fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lat/lon bounds for each region\n",
    "regions = {\n",
    "        'Greenland': {'lat': slice(59, 89), 'lon': slice(-65, -12)},\n",
    "        'Northern North America': {'lat': slice(40, 73), 'lon': slice(-170, -53)},\n",
    "        'Southern North America': {'lat': slice(15, 40), 'lon': slice(-130, -55)},\n",
    "        'Northern South America': {'lat': slice(-25, 15), 'lon': slice(-85, -30)},\n",
    "        'Southern South America': {'lat': slice(-59, -25), 'lon': slice(-80, -40)},\n",
    "        'Northern Europe': {'lat': slice(45, 89), 'lon': slice(-12, 40)},\n",
    "        'Mediterranean and Middle East': {'lat': slice(30, 45), 'lon': slice(-12, 55)},\n",
    "        'Sahara': {'lat': slice(12, 30), 'lon': slice(-20, 55)},\n",
    "        'Sub-Sahara Africa': {'lat': slice(-35, 12), 'lon': slice(-20, 55)},\n",
    "        'Northern Asia': {'lat': slice(45, 89), 'lon': slice(40, 179)},\n",
    "        'Southwest Asia': {'lat': slice(0, 45), 'lon': slice(55, 90)},\n",
    "        'Southeast Asia': {'lat': slice(-11, 45), 'lon': slice(90, 165)},\n",
    "        'Oceania': {'lat': slice(-50, -11), 'lon': slice(110, 180)}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7ff62-b9f0-4add-b1a0-0883fc1a4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_regions = slice_to_regions(ds_dict_hist, regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2357b23-f907-455e-beaf-e6181069b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_regions = slice_to_regions(ds_dict_ssp370, regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f417b3-2b33-4081-b1d8-6d3a1c21d269",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29262fe-a12e-44ff-894e-6d851abac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_period = select_period(ds_dict_hist, start_year=1985, end_year=2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a12260-1df3-4232-bf91-196fb8144269",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period = select_period(ds_dict_ssp370, start_year=2071, end_year=2100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de9ead-0d36-4f06-9430-9efeae7cc9bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b6390-f4b2-41d0-951d-c6bae78ce192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_hist_period_metric = compute_statistic(ds_dict_hist_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a9c21-8094-4700-b972-36e3f720d629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_ssp370_period_metric = compute_statistic(ds_dict_ssp370_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9e57e-df64-4f98-9bbc-9b7ea9fb6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Plot timeseries for each variable combination\n",
    "plot_time_series_correlations(yearly_correlations_hist, variable_pairs, 'pr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3216b-6614-4a17-9090-900573c2e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_hist = compute_stats(ds_dict_hist)\n",
    "stats_ssp370 = compute_stats(ds_dict_ssp370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3abcc5-ec61-4ce5-a384-b282534580db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_values(stats_hist, stats_ssp370, scaling_both=False, scaling_hist=True, full_variable_names=full_var_names_and_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54daa3a6-168f-414f-94f9-aa4e8aa2bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_consecutive_periods(ds_dict_hist, ds_dict_ssp370, save_fig=False, fig_name='plot.png'):\n",
    "    # Define the order of the variables\n",
    "    variable_order = ['pr', 'vpd', 'mrro', 'evspsbl', 'lmrso_1m', 'lmrso_2m', 'tran', 'gpp', 'lai']\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n",
    "    axes = axes.flatten()  # Flatten the axes array to easily iterate over\n",
    "    fig.suptitle('Variable Change for Historical and SSP370 Period', fontsize=14)\n",
    "\n",
    "    for i, variable in enumerate(variable_order):\n",
    "        ax = axes[i]  # No need to subtract 1 because we're iterating over variable_order list\n",
    "        \n",
    "        # Check if variable exists in the dictionary\n",
    "        if variable not in ds_dict_hist['Ensemble_mean'].variables:\n",
    "            print(f\"Variable {variable} not found in dataset. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Get the data for each period\n",
    "        data_hist = [ds[variable] for ds in ds_dict_hist.values() if ds != 'Ensemble_mean']\n",
    "        data_ssp370 = [ds[variable] for ds in ds_dict_ssp370.values() if ds != 'Ensemble_mean']\n",
    "        \n",
    "        # Extract ensemble mean from the dictionaries\n",
    "        mean_hist = ds_dict_hist['Ensemble_mean'][variable]\n",
    "        mean_ssp370 = ds_dict_ssp370['Ensemble_mean'][variable]\n",
    "\n",
    "        # Calculate the standard deviation for each period\n",
    "        std_dev_hist = np.std(data_hist, ddof=1)\n",
    "        std_dev_ssp370 = np.std(data_ssp370, ddof=1)\n",
    "\n",
    "        # Plot the means with shaded areas for the model spread\n",
    "        ax.plot(mean_hist.year, mean_hist, label='Historical')\n",
    "        ax.fill_between(mean_hist.year, mean_hist - std_dev_hist, mean_hist + std_dev_hist, alpha=0.3)\n",
    "        ax.plot(mean_ssp370.year, mean_ssp370, label='SSP370')\n",
    "        ax.fill_between(mean_ssp370.year, mean_ssp370 - std_dev_ssp370, mean_ssp370 + std_dev_ssp370, alpha=0.3)\n",
    "\n",
    "        # Get units and long_name, or use '' if not present\n",
    "        unit = ds_dict_hist['Ensemble_mean'][variable].attrs.get('units', '')\n",
    "        long_name = ds_dict_hist['Ensemble_mean'][variable].attrs.get('long_name', '')\n",
    "\n",
    "        # Add labels and legend\n",
    "        ax.set_xlabel('Year')\n",
    "        ax.set_ylabel(unit)\n",
    "        ax.set_title(long_name)\n",
    "        ax.legend()\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for i in range(len(variable_order), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # adjust subplot positions so the title doesn't overlap\n",
    "\n",
    "    # Save the figure if specified\n",
    "    if save_fig:\n",
    "        # Define save path and filename\n",
    "        statistic_dim = 'space'\n",
    "        statistic = ds_dict_hist[list(ds_dict_hist.keys())[0]].statistic\n",
    "        savepath = f'../../results/CMIP6/ssp370-historical/{statistic_dim}/{statistic}/line_plot_variable_change'\n",
    "        filename = f'Line_plot_variable_change.png'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59884e43-c839-436c-9b70-e3acc8c750d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "plot_consecutive_periods(ds_dict_hist_mean, ds_dict_ssp370_mean, save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55645167-aa99-486a-a796-83be2ac5e22c",
   "metadata": {},
   "source": [
    "### Compute Ensemble metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342dd4a4-a82c-4fe1-a3d9-206a3b1826c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period = compute_ensemble(ds_dict_hist_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a9167-24b6-4ce4-a6ee-49d7e0b97bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period = compute_ensemble(ds_dict_ssp370_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6bc0a-0837-4359-bd61-a6ad7408853e",
   "metadata": {},
   "source": [
    "### Compute NBWFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247db2db-95c1-4d0d-859f-a6b6db0f8583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_hist_period = compute_nbwfp(ds_dict_hist_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df338702-8f75-4350-8308-09a8fe071e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period = compute_nbwfp(ds_dict_ssp370_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913e646-e441-4989-b975-a12023b0baeb",
   "metadata": {},
   "source": [
    "### Select regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5ecfe-2ae2-499a-b332-b8e3b3166531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c392732-7246-48eb-af0d-b3cbfe312bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3354582-dd2c-420d-82ae-ca2753500dfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Regionmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32997f-4006-4e34-a222-abd1b075f732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c67c08-863d-41a2-8863-8ba91250667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regionmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3e199-c2f8-41e0-8ed2-ea6730ac29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_regions = regionmask.defined_regions.ar6.land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40447a6-197b-4bb1-90a2-c01fd5186bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_regions.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d9e504-9f04-4125-9991-992fef246654",
   "metadata": {},
   "outputs": [],
   "source": [
    "tas = ds_dict_hist_period['TaiESM1'].tas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c74a8-1cfa-491d-b8ec-78d248504ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_2D = land_regions.mask_3D(tas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fae44-4861-472f-a560-158423105e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_2D.isel(region=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85e29e-c8cc-40ef-854d-50a148c55dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = ccrs.Robinson()\n",
    "f, ax = plt.subplots(subplot_kw=dict(projection=proj))\n",
    "\n",
    "h = mask_2D.plot.pcolormesh(ax=ax, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "\n",
    "ax.coastlines()\n",
    "\n",
    "land_regions.plot_regions(line_kws=dict(lw=0.5), add_label=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af602d8e-fb1e-490d-a08b-40c2c44fcf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABSOLUTE CHANGE\n",
    "ds_change = ds_ssp - ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56054ba-3381-4f58-a79b-589b8f7c8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELATIVE CHANGE\n",
    "ds_change_rel = ((ds_ssp - ds)/ds) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cf5d6-bd06-4e2c-a6c8-57237c43e63c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds['NBWF'].plot(cmap=cmap, vmin=-0.5, vmax=0.5)\n",
    "fig.figure.text(0.5, 0.9, \"Historical Net Blue Water Flux (NBWF)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141318d2-5f35-491a-8216-613402ae1cba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_ssp['NBWF'].plot(cmap=cmap, vmin=-0.5, vmax=0.5)\n",
    "fig.figure.text(0.5, 0.9, \"Future Net Blue Water Flux (NBWF)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd680cd-319e-4f2e-8b46-8fd40de2fa29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change['NBWF'].plot(cmap=cmap, vmin=-0.5, vmax=0.5)\n",
    "fig.figure.text(0.5, 0.9, \"Net Blue Water Flux Absolute Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c7428-d4fb-4177-b3b9-f2798fe8532d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change_rel['NBWF'].plot(cmap=cmap, vmin=-100, vmax=100)\n",
    "fig.figure.text(0.5, 0.9, \"Net Blue Water Flux Relative Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2002315a-dd76-4986-9162-a694ac906033",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Historical:\")\n",
    "print(f\"Global Mean of Historical NBWF: {ds['NBWF'].mean().values} mm/day\")\n",
    "print(f\"Global Median of Historical NBWF: {ds['NBWF'].median().values} mm/day\")\n",
    "print(f\"Global Standard Deviation of Historical NBWF: {ds['NBWF'].std().values} mm/day\")\n",
    "print(f\"Global Minimum of Historical NBWF: {ds['NBWF'].min().values} mm/day\")\n",
    "print(f\"Global Maximum of Historical NBWF: {ds['NBWF'].max().values} mm/day\\n\")\n",
    "\n",
    "print(\"SSP370:\")\n",
    "print(f\"Global Mean of SSP370 NBWF: {ds_ssp['NBWF'].mean().values} mm/day\")\n",
    "print(f\"Global Median of SSP370 NBWF: {ds_ssp['NBWF'].median().values} mm/day\")\n",
    "print(f\"Global Standard Deviation of SSP370 NBWF: {ds_ssp['NBWF'].std().values} mm/day\")\n",
    "print(f\"Global Minimum of SSP370 NBWF: {ds_ssp['NBWF'].min().values} mm/day\")\n",
    "print(f\"Global Maximum of SSP370 NBWF: {ds_ssp['NBWF'].max().values} mm/day\\n\")\n",
    "\n",
    "print(\"Absolute Change:\")\n",
    "print(f\"Global Mean Absolute Change of NBWF: {ds_change['NBWF'].mean().values} mm/day\")\n",
    "print(f\"Global Median Absolute Change of NBWF: {ds_change['NBWF'].median().values} mm/day\")\n",
    "print(f\"Global Standard Deviation of NBWF Absolute Change: {ds_change['NBWF'].std().values} mm/day\")\n",
    "print(f\"Global Minimum of NBWF Absolute Change: {ds_change['NBWF'].min().values} mm/day\")\n",
    "print(f\"Global Maximum of NBWF Absolute Change: {ds_change['NBWF'].max().values} mm/day\\n\")\n",
    "\n",
    "print(\"Relative Change:\")\n",
    "print(f\"Global Mean Relative Change of NBWF: {ds_change_rel['NBWF'].mean().values} %\")\n",
    "print(f\"Global Median Relative Change of NBWF: {ds_change_rel['NBWF'].median().values} %\")\n",
    "print(f\"Global Standard Deviation of NBWF Relative Change: {ds_change_rel['NBWF'].std().values} %\")\n",
    "print(f\"Global Minimum of NBWF Relative Change: {ds_change_rel['NBWF'].min().values} %\")\n",
    "print(f\"Global Maximum of NBWF Relative Change: {ds_change_rel['NBWF'].max().values} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661df8af-0797-4521-aa14-60952a190a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds['NBWF/P'].plot(cmap=cmap, vmin=-0.5, vmax=0.5)\n",
    "fig.figure.text(0.5, 0.9, \"Historical Net Blue Water Flux / Precipitation (NBWF/P)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ff1db-279e-451e-9bd8-25142ba9efcb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_ssp['NBWF/P'].plot(cmap=cmap, vmin=-0.5, vmax=0.5)\n",
    "fig.figure.text(0.5, 0.9, \"SSP370 Net Blue Water Flux / Precipitation (NBWF/P)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4c73a-a915-4371-97ef-f83f6b261f36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change['NBWF/P'].plot(cmap=cmap, vmin=-0.2, vmax=0.2)\n",
    "fig.figure.text(0.5, 0.9, \"Net Blue Water Flux / Precipitation (NBWF/P) Absolute Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560acb6-149e-4e40-812c-dbbcd6f13a7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change_rel['NBWF/P'].plot(cmap=cmap, vmin=-100, vmax=100)\n",
    "fig.figure.text(0.5, 0.9, \"Net Blue Water Flux / Precipitation (NBWF/P) Relative Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd928b-abca-4d27-bd65-fd9e1874e2f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Historical:\")\n",
    "print(f\"Global Mean of Historical BWFB: {ds['BWFB'].mean().values}\")\n",
    "print(f\"Global Median of Historical BWFB: {ds['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of Historical BWFB: {ds['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of Historical BWFB: {ds['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of Historical BWFB: {ds['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"SSP370:\")\n",
    "print(f\"Global Mean of SSP370 BWFB: {ds_ssp['BWFB'].mean().values}\")\n",
    "print(f\"Global Median of SSP370 BWFB: {ds_ssp['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of SSP370 BWFB: {ds_ssp['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of SSP370 BWFB: {ds_ssp['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of SSP370 BWFB: {ds_ssp['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"Absolute Change:\")\n",
    "print(f\"Global Mean Absolute Change of BWFB: {ds_change['BWFB'].mean().values}\")\n",
    "print(f\"Global Median Absolute Change of BWFB: {ds_change['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of BWFB Absolute Change: {ds_change['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of BWFB Absolute Change: {ds_change['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of BWFB Absolute Change: {ds_change['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"Relative Change:\")\n",
    "print(f\"Global Mean Relative Change of BWFB: {ds_change_rel['BWFB'].mean().values} %\")\n",
    "print(f\"Global Median Relative Change of BWFB: {ds_change_rel['BWFB'].median().values} %\")\n",
    "print(f\"Global Standard Deviation of BWFB Relative Change: {ds_change_rel['BWFB'].std().values} %\")\n",
    "print(f\"Global Minimum of BWFB Relative Change: {ds_change_rel['BWFB'].min().values} %\")\n",
    "print(f\"Global Maximum of BWFB Relative Change: {ds_change_rel['BWFB'].max().values} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f34bf-e317-4c0b-93c0-773507af71e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds['BWFB'].plot(cmap=cmap, vmin=0, vmax=5)\n",
    "fig.figure.text(0.5, 0.9, \"Historical Blue Water Flux Balance (BWFB)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0a25d-8376-41a8-a4ad-b0c2f757f772",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_ssp['BWFB'].plot(cmap=cmap, vmin=0, vmax=5)\n",
    "fig.figure.text(0.5, 0.9, \"SSP370 Blue Water Flux Balance (BWFB)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ba321-049e-4a59-aef7-b71fd35eba71",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change['BWFB'].plot(cmap=cmap, vmin=0, vmax=5)\n",
    "fig.figure.text(0.5, 0.9, \"Blue Water Flux Balance (BWFB) Absolute Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3f07b-faf6-42b0-b2fb-0cbae69090ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change_rel['BWFB'].plot(cmap=cmap, vmin=-100, vmax=100)\n",
    "fig.figure.text(0.5, 0.9, \"Blue Water Flux Balance (BWFB) Relative Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7673743-a6ff-4b76-a0b7-ca93266acd99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Historical:\")\n",
    "print(f\"Global Mean of Historical BWFB: {ds['BWFB'].mean().values}\")\n",
    "print(f\"Global Median of Historical BWFB: {ds['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of Historical BWFB: {ds['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of Historical BWFB: {ds['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of Historical BWFB: {ds['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"SSP370:\")\n",
    "print(f\"Global Mean of SSP370 BWFB: {ds_ssp['BWFB'].mean().values}\")\n",
    "print(f\"Global Median of SSP370 BWFB: {ds_ssp['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of SSP370 BWFB: {ds_ssp['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of SSP370 BWFB: {ds_ssp['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of SSP370 BWFB: {ds_ssp['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"Absolute Change:\")\n",
    "print(f\"Global Mean Absolute Change of BWFB: {ds_change['BWFB'].mean().values}\")\n",
    "print(f\"Global Median Absolute Change of BWFB: {ds_change['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of BWFB Absolute Change: {ds_change['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of BWFB Absolute Change: {ds_change['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of BWFB Absolute Change: {ds_change['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"Relative Change:\")\n",
    "print(f\"Global Mean Relative Change of BWFB: {ds_change_rel['BWFB'].mean().values} %\")\n",
    "print(f\"Global Median Relative Change of BWFB: {ds_change_rel['BWFB'].median().values} %\")\n",
    "print(f\"Global Standard Deviation of BWFB Relative Change: {ds_change_rel['BWFB'].std().values} %\")\n",
    "print(f\"Global Minimum of BWFB Relative Change: {ds_change_rel['BWFB'].min().values} %\")\n",
    "print(f\"Global Maximum of BWFB Relative Change: {ds_change_rel['BWFB'].max().values} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95cbc7-0430-4675-a771-9b69defd0beb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Timeline and historical ensmean map of variables for regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c36800-2473-408f-9095-12f9bf6a6bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_yearly = compute_yearly_means(ds_dict_hist_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9d031-0fc0-443d-9ad5-78018d5d3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_yearly = compute_yearly_means(ds_dict_ssp370_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ae6a5-b157-43f1-8816-ef31eb4cde6f",
   "metadata": {},
   "source": [
    "### Plot Regional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379c534-9b15-4357-b709-8180d3a351c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_with_regions(ds_dict_metric, ds_dict_hist_yearly, ds_dict_ssp370_yearly, regions, model, metric='mean', target_var='pr', save_fig=False):\n",
    "    \n",
    "    fig = plt.figure(figsize=(22, 10)) \n",
    "\n",
    "    ax_main = fig.add_axes([0.25, 0.25, 0.7, 0.7], projection=ccrs.PlateCarree())  # Define location and size of main plot\n",
    "    \n",
    "    # Compute anomalies\n",
    "    ds_anomaly = ds_dict_metric[model][target_var].copy()\n",
    "    #global_mean = ds_anomaly.mean()\n",
    "    #ds_anomaly = ds_anomaly - global_mean\n",
    "    \n",
    "    # Define vmin vmax\n",
    "    if target_var == 'lmrso_1m' or target_var == 'lmrso_2m' or target_var == 'WUE':\n",
    "        vmin = -50\n",
    "        vmax = 50\n",
    "    else:\n",
    "        vmin = -100\n",
    "        vmax = 100\n",
    "    \n",
    "    # Plot the global data\n",
    "    img = ds_anomaly.plot(ax=ax_main, vmin=vmin, vmax=vmax, cmap='bwr', transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "    \n",
    "    # Add gridlines\n",
    "    gridlines = ax_main.gridlines(draw_labels=True, color='black', alpha=0.2, linestyle='--')\n",
    "    gridlines.top_labels = gridlines.right_labels = False  # Only draw labels on bottom and left side\n",
    "    \n",
    "    long_name = {\n",
    "        'Precipitation': 'Precipitation',\n",
    "        'Total Runoff': 'Total Runoff',\n",
    "        'Vapor Pressure Deficit': 'Vapor Pressure Deficit',\n",
    "        'Evaporation Including Sublimation and Transpiration': 'Evapotranspiration',\n",
    "        'Transpiration': 'Transpiration',\n",
    "        'Leaf Area Index': 'Leaf Area Index',\n",
    "        'Carbon Mass Flux out of Atmosphere Due to Gross Primary Production on Land [kgC m-2 s-1]': 'Gross Primary Production',\n",
    "        'Total Liquid Soil Moisture Content of 1 m Column': '1 m Soil Moisture',\n",
    "        'Total Liquid Soil Moisture Content of 2 m Column': '2 m Soil Moisture',\n",
    "        'Water Use Efficiency': 'Water Use Efficiency' \n",
    "    }\n",
    "\n",
    "\n",
    "    # Define the locations for each line plot\n",
    "    line_plot_locations = {\n",
    "        'Greenland': [0.25, 1.0, 0.2, 0.2],\n",
    "        'Northern North America': [0, 1.0, 0.2, 0.2],\n",
    "        'Southern North America':[0, 0.66, 0.2, 0.2],\n",
    "        'Northern South America':[0, 0.33, 0.2, 0.2],\n",
    "        'Southern South America':[0, 0, 0.2, 0.2],\n",
    "        'Northern Europe':[0.75, 1.0, 0.2, 0.2],\n",
    "        'Mediterranean and Middle East':[0.5, 1.0, 0.2, 0.2],\n",
    "        'Sahara':[0.25, 0, 0.2, 0.2],\n",
    "        'Sub-Sahara Africa':[0.75, 0, 0.2, 0.2],\n",
    "        'Northern Asia': [1.0, 1.0, 0.2, 0.2],\n",
    "        'Southwest Asia': [1.0, 0.33, 0.2, 0.2],\n",
    "        'Southeast Asia': [1.0, 0.66, 0.2, 0.2],\n",
    "        'Oceania': [1.0, 0, 0.2, 0.2]\n",
    "    }\n",
    "    \n",
    "    # Define custom legend handles\n",
    "    handle1_hist = mpatches.Patch(color='blue', alpha=0.2, label='10th-90th Percentile Historical')\n",
    "    handle2_hist = mpatches.Patch(color='blue', alpha=0.1, label='25th-75th Percentile Historical')\n",
    "    handle1_ssp370 = mpatches.Patch(color='orange', alpha=0.2, label='10th-90th Percentile SSP370')\n",
    "    handle2_ssp370 = mpatches.Patch(color='orange', alpha=0.1, label='25th-75th Percentile SSP370')\n",
    "    legend_handles = [handle1_hist, handle2_hist, handle1_ssp370, handle2_ssp370]\n",
    "    legend_labels = ['10th-90th Percentile Historical', '25th-75th Percentile Historical', '10th-90th Percentile SSP370', '25th-75th Percentile SSP370']\n",
    "    \n",
    "    # Define a list of identifiers\n",
    "    identifiers = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "    # Before plotting the subplots, initialize an iterator over the identifiers\n",
    "    identifiers_iter = iter(identifiers)\n",
    "    \n",
    "    offset_lon = 1  # Define an offset in longitude units. You may need to adjust this.\n",
    "    offset_lat = -1  # Define an offset in latitude units. You may need to adjust this.\n",
    "    \n",
    "    ds_percentiles = {}\n",
    "\n",
    "    # Loop over the regions and plot each one\n",
    "    for region, coord in regions.items():\n",
    "        \n",
    "        identifier = next(identifiers_iter)\n",
    "        \n",
    "        lat_min = coord['lat'].start\n",
    "        lat_max = coord['lat'].stop\n",
    "        lon_min = coord['lon'].start\n",
    "        lon_max = coord['lon'].stop\n",
    "        \n",
    "        # Calculate the center of the box\n",
    "        lat_center = (lat_min + lat_max) / 2\n",
    "        lon_center = (lon_min + lon_max) / 2\n",
    "        \n",
    "        # Draw the box\n",
    "        ax_main.plot([lon_min, lon_max, lon_max, lon_min, lon_min],\n",
    "                [lat_min, lat_min, lat_max, lat_max, lat_min],\n",
    "                color='black', transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # Add identifier to main plot\n",
    "        ax_main.text(lon_min + offset_lon, lat_max + offset_lat, identifier, \n",
    "                     horizontalalignment='left', verticalalignment='top', \n",
    "                     transform=ccrs.PlateCarree(), fontsize=14, color='black', weight='bold')\n",
    "\n",
    "        # Add title for map\n",
    "        ax_main.set_title(f\"{model} ({metric})\")\n",
    "        \n",
    "        # Add coastlines\n",
    "        ax_main.coastlines()\n",
    "        \n",
    "        \n",
    "        # Get all the hist models data together for each region without the Ensemble\n",
    "        all_hist_models_data = xr.concat([ds_dict_hist_yearly[region][models][target_var]\n",
    "                                          for models in ds_dict_hist_yearly[region] if models != f'Ensemble_{metric}'], dim='models')\n",
    "         # Calculate percentiles\n",
    "        percentile_10_hist = all_hist_models_data.quantile(0.1, dim='models')\n",
    "        percentile_25_hist = all_hist_models_data.quantile(0.25, dim='models')\n",
    "        percentile_75_hist = all_hist_models_data.quantile(0.75, dim='models')\n",
    "        percentile_90_hist = all_hist_models_data.quantile(0.9, dim='models')\n",
    "        \n",
    "        # Get all the ssp370 models data together for each region without the ensemble\n",
    "        all_ssp370_models_data = xr.concat([ds_dict_ssp370_yearly[region][models][target_var]\n",
    "                                          for models in ds_dict_ssp370_yearly[region] if models != f'Ensemble_{metric}'], dim='models')\n",
    "        \n",
    "        # Calculate percentiles\n",
    "        percentile_10_ssp370 = all_ssp370_models_data.quantile(0.1, dim='models')\n",
    "        percentile_25_ssp370 = all_ssp370_models_data.quantile(0.25, dim='models')\n",
    "        percentile_75_ssp370 = all_ssp370_models_data.quantile(0.75, dim='models')\n",
    "        percentile_90_ssp370 = all_ssp370_models_data.quantile(0.9, dim='models')\n",
    "\n",
    "        \n",
    "        # Plot the regional data at the specified location\n",
    "        ax2 = plt.axes(line_plot_locations[region])\n",
    "        \n",
    "        # Add identifier to subplots\n",
    "        ax2.text(0.05, 0.95, identifier,  # Position is 10% from the left and 10% from the top\n",
    "                 horizontalalignment='left', verticalalignment='top', \n",
    "                 transform=ax2.transAxes, fontsize=14, color='black', weight='bold')\n",
    "        \n",
    "        # Add shading for the 10th and 90th percentiles\n",
    "        ax2.fill_between(ds_dict_hist_yearly[region][model][target_var].year,\n",
    "                         percentile_10_hist,\n",
    "                         percentile_90_hist,\n",
    "                         color='blue', alpha=0.1)\n",
    "        \n",
    "        # Add shading for the 25th and 75th percentiles\n",
    "        ax2.fill_between(ds_dict_hist_yearly[region][model][target_var].year,\n",
    "                         percentile_25_hist,\n",
    "                         percentile_75_hist,\n",
    "                         color='blue', alpha=0.2)\n",
    "        \n",
    "        # Add shading for the 10th and 90th percentiles\n",
    "        ax2.fill_between(ds_dict_ssp370_yearly[region][model][target_var].year,\n",
    "                         percentile_10_ssp370,\n",
    "                         percentile_90_ssp370,\n",
    "                         color='orange', alpha=0.1)\n",
    "        \n",
    "        # Add shading for the 25th and 75th percentiles\n",
    "        ax2.fill_between(ds_dict_ssp370_yearly[region][model][target_var].year,\n",
    "                         percentile_25_ssp370,\n",
    "                         percentile_75_ssp370,\n",
    "                         color='orange', alpha=0.2)\n",
    "        \n",
    "        # Plot the lines\n",
    "        ax2.plot(ds_dict_hist_yearly[region][model][target_var].year,\n",
    "                 ds_dict_hist_yearly[region][model][target_var],\n",
    "                 label='Historical')\n",
    "        ax2.plot(ds_dict_ssp370_yearly[region][model][target_var].year,\n",
    "                 ds_dict_ssp370_yearly[region][model][target_var],\n",
    "                 label='SSP370')\n",
    "        \n",
    "        ax2.set_title(region)\n",
    "        ax2.set_ylabel(f\"{ds_dict_metric[f'Ensemble_{metric}'][target_var].units}\")#, fontsize=8)\n",
    "\n",
    "        \n",
    "    # Get line handles and labels from your plot\n",
    "    line_handles, line_labels = ax2.get_legend_handles_labels()\n",
    "    \n",
    "    # Add line handles and labels to your custom handles and labels\n",
    "    line_handles += legend_handles\n",
    "    line_labels.extend(legend_labels)\n",
    "    \n",
    "    # Add legend for subplots, use legend_handles and legend_labels instead of getting them from the last plot\n",
    "    fig.legend(handles=line_handles, labels=line_labels, loc='center', bbox_to_anchor=(0.6, 0.1), ncol=2)\n",
    "\n",
    "    # Move the colorbar to the bottom\n",
    "    cbar_ax = fig.add_axes([0.475, 0.23, 0.25, 0.02]) #left, bottom, width, height\n",
    "    cbar = fig.colorbar(img, cax=cbar_ax, orientation='horizontal')\n",
    "    \n",
    "    # Add a label to the colorbar\n",
    "    cbar.set_label(f\"Relative Change (SSP370-historical) of {long_name[ds_dict_metric[f'Ensemble_{metric}'][target_var].long_name]} [{ds_dict_metric[f'Ensemble_{metric}'][target_var].units_rel}]\", fontsize=12)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Safe figure\n",
    "    if save_fig:\n",
    "        savepath = os.path.join('../..', 'results', 'CMIP6', 'ssp370-historical', 'time-space', metric, f'var_line_plots_and_anomaly_of_{metric}')\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        filename = f'{model}.{target_var}.line_plot.relative_change.png'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        fig.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0f5a1-263e-4b1f-9f9d-5b49cd0ea42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_with_regions(ds_dict_change_rel, ds_dict_hist_yearly, ds_dict_ssp370_yearly, regions, model='TaiESM1', metric='median', target_var='lai', save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4344d-4203-4772-b222-b6cd358fcdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Compute Correlations ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe1fb6-f8c9-4cd5-81c5-c867c1feed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_corr = compute_correlation_coefficients(ds_dict_hist, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ea77e-4541-42c4-956c-64a368ec0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_corr = compute_correlation_coefficients(ds_dict_ssp370, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b9bb4-0b22-4eb5-8df6-bd437220135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Compute Change ========\n",
    "ds_dict_corr_change = compute_change_corr(ds_dict_hist_corr, ds_dict_ssp370_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec26d9-42d3-49d5-80ba-685bbc7eaa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Create unique variable pairs ========\n",
    "variable_pairs = [(variables[i], variables[j]) for i in range(len(variables)) for j in range(i+1, len(variables))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e84fdb-70c6-4aaa-8cc9-21ccd68d20e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======== Create Plots for all variable pairs\n",
    "for var in variable_pairs:\n",
    "    corr_maps(ds_dict_corr_change, f'{var[0]} x {var[0+1]}', 'coolwarm', save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd0692-cec1-4b1d-b501-eebc9b05b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_corr_change_plot(ds_dict_corr_change, 'mrro', full_var_names_and_unit, 'coolwarm', save_fig=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1 Python 3 (based on the module python3/2023.01)",
   "language": "python",
   "name": "python3_2023_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
