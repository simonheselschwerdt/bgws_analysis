{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7716ffb9-a564-44c7-87d4-2d4caeecdb1c",
   "metadata": {},
   "source": [
    "# CMIP6 Statistics and Plots\n",
    "\n",
    "**Following steps are included in this script:**\n",
    "\n",
    "1. Load netCDF files\n",
    "2. Compute statistics\n",
    "3. Plot statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5a4f3-40bb-459a-88f0-f180c8b3ee8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854af429-43a1-489d-8513-dca8c1466663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Packages ==========\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import dask\n",
    "import matplotlib.cm\n",
    "from matplotlib import rcParams\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from cftime import DatetimeNoLeap\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams[\"mathtext.default\"] = 'regular'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5daa16-4811-4773-b98d-6b69626ff6cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb57b43-bbbe-414b-8075-8cda5d1ea4ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9326b-048c-48c2-84e0-5af87d06a120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Helper function to open the dataset ========\n",
    "def open_dataset(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ca849-c1f4-431c-ae0f-e6a3fb6ba92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to open and merge datasets\n",
    "def open_and_merge_datasets(folder, model, experiment_id, variables):\n",
    "    filepaths = []\n",
    "    for var in variables:\n",
    "        path = f'../../data/CMIP6/{experiment_id}/{folder}/{var}'\n",
    "        fp = glob.glob(os.path.join(path, f'CMIP.{model}.{experiment_id}.{var}_regridded.nc'))\n",
    "        if fp:\n",
    "            filepaths.append(fp[0])\n",
    "        else:\n",
    "            #print(f\"No file found for variable '{var}' in model '{model}'.\")\n",
    "            print(fp)\n",
    "\n",
    "    datasets = [xr.open_dataset(fp) for fp in filepaths]\n",
    "    ds = xr.merge(datasets)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9148cef-f033-40ac-aeef-dec5ccdb277d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79176a9d-1bde-438d-a226-9c749fc98042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_period(ds_dict, start_year=None, end_year=None):\n",
    "    '''\n",
    "    Helper function to select periods.\n",
    "    \n",
    "    Parameters:\n",
    "    ds_dict (dict): Dictionary with xarray datasets.\n",
    "    start_year (int): The start year of the period.\n",
    "    end_year (int): The end year of the period.\n",
    "    \"\"\"\n",
    "    '''\n",
    "    start_year = DatetimeNoLeap(start_year, 1, 16, 12, 0, 0, 0,has_year_zero=True) # 16th of January of start year\n",
    "    end_year = DatetimeNoLeap(end_year, 12, 16, 12, 0, 0, 0, has_year_zero=True) # 16th of December of end year\n",
    "    ds_dict = {k: v.sel(time=slice(start_year, end_year)) for k, v in ds_dict.items()}\n",
    "    \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a690d-ac2f-4875-96dc-66e8424a4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Standardize ========\n",
    "def standardize(ds_dict):\n",
    "    '''\n",
    "    Helper function to standardize datasets of a dictionary\n",
    "    '''\n",
    "    ds_dict_stand = {}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        attrs = ds.attrs\n",
    "        ds_stand = (ds - ds.mean()) / ds.std()\n",
    "\n",
    "        # Preserve variable attributes from the original dataset\n",
    "        for var in ds.variables:\n",
    "            if var in ds_stand.variables:\n",
    "                ds_stand[var].attrs = ds[var].attrs\n",
    "\n",
    "        ds_stand.attrs = attrs\n",
    "        ds_dict_stand[name] = ds_stand\n",
    "        \n",
    "    return ds_dict_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb19561-1cb9-4d0d-8565-3fa7c45988e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_args_and_get_info(ds_dict, variable):\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if not isinstance(variable, str):\n",
    "        raise TypeError('variable must be a string.')\n",
    "        \n",
    "    # Dictionary to store plot titles for each statistic\n",
    "    titles = {\"mean\": \"Mean\", \"std\": \"Standard deviation of yearly means\", \"min\": \"Minimum\", \"max\": \"Maximum\", \"median\": \"Median\", \"time\": \"Time\", \"space\": \"Space\"}\n",
    "    \n",
    "    long_name = {\n",
    "        'Precipitation': 'Precipitation',\n",
    "        'Total Runoff': 'Total Runoff',\n",
    "        'Vapor Pressure Deficit': 'Vapor Pressure Deficit',\n",
    "        'Evaporation Including Sublimation and Transpiration': 'Evapotranspiration',\n",
    "        'Transpiration': 'Transpiration',\n",
    "        'Leaf Area Index': 'Leaf Area Index',\n",
    "        'Carbon Mass Flux out of Atmosphere Due to Gross Primary Production on Land [kgC m-2 s-1]': 'Gross Primary Production',\n",
    "        'Total Liquid Soil Moisture Content of 1 m Column': '1 m Soil Moisture',\n",
    "        'Total Liquid Soil Moisture Content of 2 m Column': '2 m Soil Moisture',\n",
    "        'Runoff - Precipitation': 'Runoff - Precipitation',\n",
    "        'Transpiration - Precipitation': 'Transpiration - Precipitation',\n",
    "        '(Runoff + Transpiration) - Precipitation':  '(Runoff + Transpiration) - Precipitation'\n",
    "    }\n",
    "    \n",
    "    # Data information\n",
    "    var_long_name = long_name[ds_dict[list(ds_dict.keys())[0]][variable].long_name]\n",
    "    period = f\"{ds_dict[list(ds_dict.keys())[0]].attrs['period']}\"\n",
    "    experiment_id =  ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    unit = ds_dict[list(ds_dict.keys())[0]][variable].units\n",
    "    statistic_dim = ds_dict[list(ds_dict.keys())[0]].statistic_dimension\n",
    "    statistic = ds_dict[list(ds_dict.keys())[0]].attrs['statistic']\n",
    "\n",
    "    return var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090c127-204b-4d69-9e33-5769b28e4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_args_and_get_info(ds_dict, variable):\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if not isinstance(variable, str):\n",
    "        raise TypeError('variable must be a string.')\n",
    "        \n",
    "    # Dictionary to store plot titles for each statistic\n",
    "    titles = {\"mean\": \"Mean\", \"std\": \"Standard deviation of yearly means\", \"min\": \"Minimum\", \"max\": \"Maximum\", \"median\": \"Median\", \"time\": \"Time\", \"space\": \"Space\"}\n",
    "    freq = {\"mon\": \"Monthly\"}\n",
    "    \n",
    "    long_name = {\n",
    "        'Precipitation': 'Precipitation',\n",
    "        'Total Runoff': 'Total Runoff',\n",
    "        'Vapor Pressure Deficit': 'Vapor Pressure Deficit',\n",
    "        'Evaporation Including Sublimation and Transpiration': 'Evapotranspiration',\n",
    "        'Transpiration': 'Transpiration',\n",
    "        'Leaf Area Index': 'Leaf Area Index',\n",
    "        'Carbon Mass Flux out of Atmosphere Due to Gross Primary Production on Land [kgC m-2 s-1]': 'Gross Primary Production',\n",
    "        'Total Liquid Soil Moisture Content of 1 m Column': '1 m Soil Moisture',\n",
    "        'Total Liquid Soil Moisture Content of 2 m Column': '2 m Soil Moisture',\n",
    "        'Runoff - Precipitation': 'Runoff - Precipitation',\n",
    "        'Transpiration - Precipitation': 'Transpiration - Precipitation',\n",
    "        '(Runoff + Transpiration) - Precipitation':  '(Runoff + Transpiration) - Precipitation',\n",
    "        'ET - Precipitation':  'ET - Precipitation', \n",
    "        'Negative Runoff': 'Negative Runoff',\n",
    "    }\n",
    "   \n",
    "    # Data information\n",
    "    var_long_name = ds_dict[list(ds_dict.keys())[0]][variable].long_name\n",
    "    period = f\"{ds_dict[list(ds_dict.keys())[0]].attrs['period'][0]}-{ds_dict[list(ds_dict.keys())[0]].attrs['period'][1]}\"\n",
    "    experiment_id =  ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    unit = ds_dict[list(ds_dict.keys())[0]][variable].units\n",
    "    statistic_dim = ds_dict[list(ds_dict.keys())[0]].statistic_dimension\n",
    "    statistic = ds_dict[list(ds_dict.keys())[0]].attrs['statistic']\n",
    "    frequency = freq[ds_dict[list(ds_dict.keys())[0]].frequency]\n",
    "\n",
    "    return var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54437cf1-8268-4619-b224-e6be3684894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_to_regions(ds_dict, regions):\n",
    "\n",
    "    ds_dict_region = {region: {} for region in regions.keys()}\n",
    "\n",
    "    # For each dataset, slice to each region and save in new dict\n",
    "    for ds_name, ds in ds_dict.items():\n",
    "        for region, bounds in regions.items():\n",
    "            ds_dict_region[region][ds_name] = ds.sel(lat=bounds['lat'], lon=bounds['lon'])\n",
    "            \n",
    "    return ds_dict_region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a2736-eb0f-466e-b547-4d293e8a5af5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766cd9d-393f-4db9-b0a3-744388ad88c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic_single(ds, statistic, dimension, yearly_mean=True):\n",
    "    if dimension == \"time\":\n",
    "        stat_ds = getattr(ds, statistic)(\"time\", keep_attrs=True, skipna=True)\n",
    "        stat_ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "    if dimension == \"space\":\n",
    "        # Assign the period attribute before grouping by year\n",
    "        ds.attrs['period'] = [str(ds.time.dt.year[0].values), str(ds.time.dt.year[-1].values)]\n",
    "        \n",
    "        if yearly_mean:\n",
    "            ds = ds.groupby('time.year').mean('time', keep_attrs=True, skipna=True)\n",
    "            ds.attrs['mean'] = 'yearly mean'\n",
    "            \n",
    "        \n",
    "        #get the weights, apply on data, and compute statistic\n",
    "        weights = np.cos(np.deg2rad(ds.lat))\n",
    "        weights.name = \"weights\"\n",
    "        ds_weighted = ds.weighted(weights)\n",
    "        stat_ds = getattr(ds_weighted, statistic)((\"lon\", \"lat\"), keep_attrs=True, skipna=True)\n",
    "    \n",
    "    stat_ds.attrs['statistic'] = statistic\n",
    "    stat_ds.attrs['statistic_dimension'] = dimension\n",
    "\n",
    "    return stat_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadad33e-375e-4f6f-94b1-918dce7e2f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistic(ds_dict, statistic, dimension, start_year=None, end_year=None, yearly_mean=True):\n",
    "    \"\"\"\n",
    "    Computes the specified statistic for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        statistic (str): The statistic to compute, which can be one of 'mean', 'std', 'min', 'var', or 'median'.\n",
    "        dimension (str): The dimension to compute over, which can be 'time' or 'space'.\n",
    "        start_year (str, optional): The start year of the period to compute the statistic over.\n",
    "        end_year (str, optional): The end year of the period to compute the statistic over.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with computed statistic for each dataset.\n",
    "    \"\"\"\n",
    "    # Check the validity of input arguments\n",
    "    if not isinstance(ds_dict, dict):\n",
    "        raise TypeError(\"ds_dict must be a dictionary of xarray datasets.\")\n",
    "    if not all(isinstance(ds, xr.Dataset) for ds in ds_dict.values()):\n",
    "        raise TypeError(\"All values in ds_dict must be xarray datasets.\")\n",
    "    if statistic not in [\"mean\", \"std\", \"min\", \"max\", \"var\", \"median\"]:\n",
    "        raise ValueError(f\"Invalid statistic '{statistic}' specified.\")\n",
    "    if dimension not in [\"time\", \"space\"]:\n",
    "        raise ValueError(f\"Invalid dimension '{dimension}' specified.\")\n",
    "\n",
    "    # Select period\n",
    "    if start_year is not None and end_year is not None:\n",
    "        select_period(ds_dict, start_year=start_year, end_year=end_year)\n",
    "        \n",
    "        \n",
    "    # Use multiprocessing to compute the statistic for each dataset in parallel\n",
    "    with mp.Pool() as pool:\n",
    "        results = pool.starmap(compute_statistic_single, [(ds, statistic, dimension, yearly_mean) for ds in ds_dict.values()])\n",
    "\n",
    "    return dict(zip(ds_dict.keys(), results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55955a2d-2e0a-41ba-bdd8-d0d8b8211506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from itertools import combinations\n",
    "\n",
    "# Define a function to compute the metrics\n",
    "def compute_metrics_for_pair(args):\n",
    "    df, var1, var2, metrics = args\n",
    "    valid_values = np.logical_and(np.isfinite(df[var1]), np.isfinite(df[var2]))\n",
    "\n",
    "    # Compute metrics\n",
    "    X = df[var1][valid_values].values.reshape(-1, 1)\n",
    "    y = df[var2][valid_values].values\n",
    "    metric_dict = {}\n",
    "\n",
    "    if 'rmse_rf' in metrics:\n",
    "        rf = RandomForestRegressor()\n",
    "        rf.fit(X, y)\n",
    "        y_pred_rf = rf.predict(X)\n",
    "        rmse_rf = np.sqrt(mean_squared_error(y, y_pred_rf))\n",
    "        metric_dict['rmse_rf'] = rmse_rf\n",
    "    \n",
    "    if 'rmse_lr' or 'r2_lr' in metrics:\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X, y)\n",
    "        y_pred_lr = lr.predict(X)\n",
    "        if 'rmse_lr' in metrics:\n",
    "            rmse_lr = np.sqrt(mean_squared_error(y, y_pred_lr))\n",
    "            metric_dict['rmse_lr'] = rmse_lr\n",
    "        if 'r2_lr' in metrics:\n",
    "            r2_lr = r2_score(y, y_pred_lr) # compute the R^2 (coefficient of determination)\n",
    "            metric_dict['r2_lr'] = r2_lr\n",
    "\n",
    "    if 'pearson' in metrics:\n",
    "        r_pearson = pearsonr(X.flatten(), y)[0]\n",
    "        metric_dict['pearson'] = r_pearson\n",
    "\n",
    "    if 'spearman' in metrics:\n",
    "        r_spearman = spearmanr(X.flatten(), y)[0]\n",
    "        metric_dict['spearman'] = r_spearman\n",
    "        \n",
    "    if 'kendalltau' in metrics:\n",
    "        tau_kendall = kendalltau(X.flatten(), y)[0]\n",
    "        metric_dict['kendalltau'] = tau_kendall\n",
    "\n",
    "    return (var1, var2, metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d176a36-95a5-4a18-b46f-a311e49243f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def precompute_metrics(ds_dict, variables, metrics=['pearson']):\n",
    "    # Initialize the results dictionary\n",
    "    results_dict = {metric: {} for metric in metrics}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        # Create a DataFrame with all the variables\n",
    "        df = pd.DataFrame({var: ds[var].values.flatten() for var in variables})\n",
    "        \n",
    "        # Define all pairs of variables\n",
    "        pairs = list(permutations(variables, 2))  # <-- Change here\n",
    "        args = [(df, var1, var2, metrics) for var1, var2 in pairs]\n",
    "\n",
    "        # Use a multiprocessing pool to compute the metrics for all pairs\n",
    "        with Pool() as p:\n",
    "            results = p.map(compute_metrics_for_pair, args)\n",
    "        \n",
    "        # Store the results in the results_dict\n",
    "        for var1, var2, metric_dict in results:\n",
    "            for metric, value in metric_dict.items():\n",
    "                # Ensure the keys exist in the dictionary\n",
    "                results_dict[metric].setdefault(name, {}).setdefault(f'{var1}_{var2}', value)\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1de1242-8b90-44db-903d-6ae01d25b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_yearly_correlations(ds_dict, variable_pairs, start_year=None, end_year=None, corr_type='pearson'):\n",
    "    \"\"\"\n",
    "    Calculates yearly Pearson correlation for the given pairs of variables from the same model.\n",
    "\n",
    "    Parameters:\n",
    "    ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "    variable_pairs (list of tuples): The pairs of variables to calculate the correlation for.\n",
    "    start_year (int): The start year of the period to compute the correlation over.\n",
    "    end_year (int): The end year of the period to compute the correlation over.\n",
    "    corr_type (str): The type of correlation coefficient to compute. Can be either 'pearson', 'spearman', or 'kendall'.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with a DataFrame for each dataset, where each DataFrame contains the yearly Pearson\n",
    "        correlation for each pair of variables.\n",
    "    \"\"\"\n",
    "    # Map for complete metric names and symbols\n",
    "    metric_map = {\n",
    "        'r2_lr': ('Coefficient of Determination', 'R²'),\n",
    "        'pearson': ('Pearson Correlation Coefficient', 'r'),\n",
    "        'spearman': ('Spearman Rank Correlation Coefficient', 'ρ'),\n",
    "        'kendalltau': ('Kendall Rank Correlation Coefficient', 'τ')\n",
    "    }\n",
    "    \n",
    "    # Select period\n",
    "    if start_year is not None and end_year is not None:\n",
    "        select_period(ds_dict, start_year=start_year, end_year=end_year)\n",
    "\n",
    "    yearly_correlations = {}\n",
    "    \n",
    "    for name, ds in ds_dict.items():\n",
    "        yearly_corr_dict = {}\n",
    "\n",
    "        # Resample to yearly data\n",
    "        ds_yearly = ds.resample(time='1Y').mean()\n",
    "        \n",
    "        for var1, var2 in variable_pairs:\n",
    "            # Prepare empty list for yearly correlations\n",
    "            yearly_correlations_values = []\n",
    "            yearly_correlations_years = []\n",
    "\n",
    "            # Get the unique years\n",
    "            years = ds_yearly['time'].dt.year\n",
    "            for year in np.unique(years):\n",
    "                # Select the data for this year\n",
    "                ds_year = ds_yearly.sel(time=f'{year}')\n",
    "                \n",
    "                # Calculate the correlation for this year and append to the list\n",
    "                if corr_type == 'pearson':\n",
    "                    corr_value = xr.corr(ds_year[var1], ds_year[var2], dim=['lon', 'lat'])\n",
    "                    \n",
    "                elif corr_type == 'spearman':\n",
    "                    ds_year_stacked = ds_year.stack(z=('lon', 'lat'))\n",
    "                    df = ds_year_stacked.to_dataframe()\n",
    "                    #df = np.isfinite(df[var1])\n",
    "                    corr_value, _ = spearmanr(df[var1], df[var2])\n",
    "                    corr_value = xr.DataArray(corr_value)\n",
    "                    \n",
    "                elif corr_type == 'kendall':\n",
    "                    ds_year_stacked = ds_year.stack(z=('lon', 'lat'))\n",
    "                    df = ds_year_stacked.to_dataframe()\n",
    "                    corr_value, _ = kendalltau(df[var1], df[var2])\n",
    "                    corr_value = xr.DataArray(corr_value)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid correlation type. Expected 'pearson', 'spearman', or 'kendall'.\")\n",
    "\n",
    "                # Calculate the correlation for this year and append to the list\n",
    "                #corr_value = xr.corr(ds_year[var1], ds_year[var2], dim=['lon', 'lat'])\n",
    "\n",
    "                yearly_correlations_values.append(float(corr_value.values)) # Extract the scalar value\n",
    "                yearly_correlations_years.append(year)\n",
    "\n",
    "            # Store in the yearly_corr_dict\n",
    "            yearly_corr_dict[f'{var1}-{var2}'] = xr.DataArray(yearly_correlations_values, dims='time', coords={'time': yearly_correlations_years})\n",
    "\n",
    "        # Create a Dataset from the yearly_corr_dict and store in the yearly_correlations dict\n",
    "        yearly_correlations[name] = xr.Dataset(yearly_corr_dict)\n",
    "        yearly_correlations[name].attrs = {'Metric': metric_map[corr_type][0],\n",
    "                                           'Metric_sign': metric_map[corr_type][1]\n",
    "                                          }\n",
    "\n",
    "    return yearly_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9381cb-6944-4552-b6c7-aac1b6a3c6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_stats(ds_dict):\n",
    "    \"\"\"\n",
    "    Compute yearly mean of each variable in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds_dict (dict): The input dictionary of xarray.Dataset.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are the dataset names and the values are another dictionary.\n",
    "          This inner dictionary has keys as variable names and values as DataArray of yearly means.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    for model, ds in ds_dict.items():\n",
    "        # Compute the yearly mean\n",
    "        yearly_ds = ds.resample(time='1Y').mean()\n",
    "\n",
    "        stats[model] = {}\n",
    "        for var in yearly_ds.data_vars:\n",
    "            # Compute the spatial mean\n",
    "            spatial_mean = yearly_ds[var].mean(dim=['lat', 'lon'])\n",
    "            \n",
    "            # Store the yearly mean values\n",
    "            stats[model][var] = spatial_mean\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbbc45-b47c-461b-99db-d20d9fa0625d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_yearly_means(ds_dict_region):\n",
    "    yearly_means_dict = {}\n",
    "\n",
    "    # For each dataset, compute the yearly mean over the 'time', 'lat', and 'lon' dimensions\n",
    "    for region, ds_dict in ds_dict_region.items():\n",
    "        yearly_means_dict[region] = {}\n",
    "        for ds_name, ds in ds_dict.items():\n",
    "            # Create weights\n",
    "            weights = np.cos(np.deg2rad(ds.lat))\n",
    "\n",
    "            # Compute the yearly mean\n",
    "            ds_yearly = ds.groupby('time.year').mean('time')\n",
    "\n",
    "            # Apply the weights and calculate the spatial mean\n",
    "            ds_weighted = ds_yearly.weighted(weights)\n",
    "            yearly_means_dict[region][ds_name] = ds_weighted.mean(('lat', 'lon'))\n",
    "\n",
    "    return yearly_means_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53592816-7215-44ef-8f2d-55e90f54e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_coefficients(ds_dict, variables, time_dim='time', yearly_corr=False):\n",
    "    \"\"\"\n",
    "    Compute the correlation coefficients for different variable combinations and\n",
    "    store them in a new dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        variables (list): A list of variables for which to compute the correlation coefficients.\n",
    "        time_dim (str): The name of the time dimension in the datasets. Default is 'time'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary where keys are the same as in the input dictionary, and each value is an xarray Dataset\n",
    "              containing correlation coefficients for different variable combinations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a new dictionary to store the correlation data\n",
    "    ds_dict_corr = {}\n",
    "\n",
    "    # Iterate over all combinations of two variables\n",
    "    for var1, var2 in combinations(variables, 2):\n",
    "        # Iterate over all datasets in the dictionary\n",
    "        for ds_name, ds in ds_dict.items():\n",
    "            # Check if both variables exist in the current dataset\n",
    "            if var1 in ds and var2 in ds:\n",
    "                if yearly_corr:\n",
    "                    # Resample the dataset to yearly data\n",
    "                    ds_corr = ds.resample({time_dim: 'Y'}).mean()\n",
    "                else:\n",
    "                    ds_corr = ds\n",
    "                \n",
    "                # If the dataset is not yet in the new dictionary, create a new xarray Dataset for it\n",
    "                if ds_name not in ds_dict_corr:\n",
    "                    ds_dict_corr[ds_name] = xr.Dataset()\n",
    "\n",
    "                # Compute the correlation coefficients and add them as a new DataArray to the Dataset\n",
    "                ds_dict_corr[ds_name][f'{var1} x {var2}'] = xr.corr(ds_corr[var1], ds_corr[var2], dim=time_dim)\n",
    "                ds_dict_corr[ds_name].attrs = ds_dict[ds_name].attrs\n",
    "                ds_dict_corr[ds_name].attrs['Metric'] = 'Pearson Correlation Coefficient'\n",
    "                ds_dict_corr[ds_name].attrs['Metric_sign'] = 'r'\n",
    "                \n",
    "                if yearly_corr:\n",
    "                    ds_dict_corr[ds_name].attrs['means'] = 'yearly means'\n",
    "                else:\n",
    "                    ds_dict_corr[ds_name].attrs['means'] = 'monthly means'\n",
    "                \n",
    "                \n",
    "    return ds_dict_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e6ced-227e-41b6-972f-79bb4e110c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_change_corr(ds_dict_hist_corr, ds_dict_ssp370_corr):\n",
    "    ds_dict_corr_change = {}\n",
    "\n",
    "    for name, ds in ds_dict_hist_corr.items():\n",
    "        ds_dict_corr_change[name] = ds_dict_ssp370_corr[name] - ds\n",
    "        ds_dict_corr_change[name].attrs = {'period': 'Change Correlation SSP370 - Historical',\n",
    "                                      'statistic': 'mean',\n",
    "                                      'statistic_dimension':  'time',\n",
    "                                      'experiment_id': 'ssp370-historical',\n",
    "                                      'source_id': name,\n",
    "                                      'Metric': 'Pearson Correlation Coefficient',\n",
    "                                      'Metric_sign': 'r'\n",
    "                                    }\n",
    "        if ds_dict_hist_corr[name].attrs['means'] == 'yearly means' and ds_dict_ssp370_corr[name].attrs['means'] == 'yearly means':\n",
    "            ds_dict_corr_change[name].attrs['means'] = 'yearly means'\n",
    "        elif ds_dict_hist_corr[name].attrs['means'] == 'monthly means' and ds_dict_ssp370_corr[name].attrs['means'] == 'monthly means':\n",
    "            ds_dict_corr_change[name].attrs['means'] = 'monthly means'\n",
    "        else:\n",
    "            raise ValueError(f\"Computing change between seasonal and monthly mean data.\")\n",
    "            \n",
    "        return ds_dict_corr_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3356085-14f7-460a-afdd-c52cb2dcb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_change(ds_dict_hist_mean, ds_dict_ssp370_mean, relative_change=False):\n",
    "    ds_dict_change = {}\n",
    "\n",
    "    for name, ds in ds_dict_hist_mean.items():\n",
    "        # Compute either absolute or relative change\n",
    "        if relative_change:\n",
    "            # Add a constant to the denominator to avoid division by zero\n",
    "            epsilon = 1\n",
    "            ds_nonzero = ds.where(ds != 0, epsilon)\n",
    "            \n",
    "            ds_dict_change[name] = ((ds_dict_ssp370_mean[name] - ds) / ds_nonzero) * 100\n",
    "            \n",
    "        else:\n",
    "            ds_dict_change[name] = ds_dict_ssp370_mean[name] - ds\n",
    "            \n",
    "        ds_dict_change[name].attrs = {'period': 'Change SSP370 - Historical',\n",
    "                                      'statistic': ds_dict_ssp370_mean[name].statistic,\n",
    "                                      'statistic_dimension':  ds_dict_ssp370_mean[name].statistic_dimension,\n",
    "                                      'experiment_id': 'ssp370-historical',\n",
    "                                      'source_id': ds_dict_ssp370_mean[name].source_id\n",
    "                                    }\n",
    "        for variables in ds:\n",
    "            ds_dict_change[name][variables].attrs = ds_dict_ssp370_mean[name][variables].attrs\n",
    "            if relative_change:\n",
    "                ds_dict_change[name][variables].attrs['units_rel'] = '%'\n",
    "                ds_dict_change[name].attrs['change'] = 'Relative Change'\n",
    "             \n",
    "    return ds_dict_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974fda1a-fbb2-409f-9a9c-4a66e040acb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28584552-cd08-41e8-9cd9-dc573da8c333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_scatter_matrix(ds_dict, variables, results_dict, metrics=['pearson'], save_fig=False, standardized=False):\n",
    "    \"\"\"\n",
    "    Plots a scatter matrix for the given variables from the same model and time period.\n",
    "\n",
    "    Parameters:\n",
    "    ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "    variables: (list of str): The names of the variables to plot.\n",
    "    metrics: (list of str): The metrics to compute. Options are 'pearson', 'spearman', 'rmse_rf', and 'rmse_lr'.\n",
    "    save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "    standardized (bool): If True, data has been standardized beforehand and different scales must be applied.\n",
    "    \"\"\"\n",
    "\n",
    "    # Label mapping\n",
    "    #label_mapping = {'rmse_rf': 'a', 'rmse_lr': 'b', 'r2_lr': 'c', 'pearson': 'd', 'spearman': 'e'}\n",
    "    \n",
    "    label_mapping = {'r2_lr': 'a', 'pearson': 'b', 'spearman': 'c', 'kendalltau': 'd'}\n",
    "\n",
    "    # Define number of Bins for colorbar\n",
    "    # Define bin edges and labels\n",
    "    if standardized:\n",
    "        n_bins = 8\n",
    "        bin_edges = np.arange(-1, 1.25, 0.25)  # 0.25 step from -1 to 1\n",
    "        labels = list(range(1, len(bin_edges)))\n",
    "    else:\n",
    "        n_bins = 12\n",
    "        bin_edges = np.arange(-3, 1.25, 0.25)  # 0.25 step from -3 to 3.25\n",
    "        labels = list(range(1, len(bin_edges)))\n",
    "    \n",
    "    # Create a colormap\n",
    "    cmap = plt.cm.coolwarm_r\n",
    "\n",
    "    for name, ds in ds_dict.items():\n",
    "        # Create a DataFrame with all the variables\n",
    "        df = pd.DataFrame({var: ds[var].values.flatten() for var in variables})\n",
    "        \n",
    "        # Add the Ecohydrological Index to the DataFrame\n",
    "        df['EI'] = ds['EI'].values.flatten()\n",
    "        \n",
    "        # Drop rows with NaN values\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Perform binning with `pd.cut`\n",
    "        df['EI_binned'] = pd.cut(df['EI'], bins=bin_edges, labels=labels, include_lowest=True)\n",
    "\n",
    "        # Normalize the colors to the range of your data\n",
    "        norm = colors.BoundaryNorm(bin_edges, cmap.N)\n",
    "\n",
    "        # Drop 'EI' from the dataframe\n",
    "        df = df.drop(columns='EI')\n",
    "\n",
    "        # If 'EI' is in the variables list, remove it\n",
    "        variables = [var for var in variables if var != 'EI']\n",
    "\n",
    "        # Create a pairplot with scatterplots for off-diagonal elements and histograms\n",
    "        g = sns.pairplot(df, diag_kind='hist', hue='EI_binned', palette=\"coolwarm_r\", plot_kws={'alpha': 0.5}, vars=variables)\n",
    "\n",
    "        # Set the y-labels to include units\n",
    "        for i, var in enumerate(variables):\n",
    "            unit = ds[var].attrs['units'] if 'units' in ds[var].attrs else ''\n",
    "            g.axes[i, 0].set_ylabel(f'{var} [{unit}]')  # Set y-labels for the first Axes in each row\n",
    "\n",
    "        # Set the x-labels to include units\n",
    "        for i, var in enumerate(variables):\n",
    "            unit = ds[var].attrs['units'] if 'units' in ds[var].attrs else ''\n",
    "            g.axes[-1, i].set_xlabel(f'{var} [{unit}]')  # Set x-labels for the last Axes in each column\n",
    "\n",
    "        # Remove the automatically generated legend\n",
    "        g._legend.remove() \n",
    "\n",
    "        # Drop 'EI_binned' now that it is no longer needed\n",
    "        df = df.drop(columns='EI_binned')\n",
    "        \n",
    "        # Create a scalar mappable object with the colormap\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        \n",
    "        if standardized: \n",
    "            sm.set_array(np.arange(-1, 1.25, 0.25))\n",
    "        else:\n",
    "            sm.set_array(np.arange(-3, 1.25, 0.25))\n",
    "\n",
    "        # Define the colorbar axes\n",
    "        cbar_ax = g.fig.add_axes([1, 0.4, 0.005, 0.3])  # [left, bottom, width, height]\n",
    "\n",
    "        # Add the colorbar to the figure\n",
    "        cb = g.fig.colorbar(sm, cax=cbar_ax)\n",
    "        cb.set_label('Ecohydrological Index')  # Set label\n",
    "\n",
    "        # Add metrics to off-diagonal plots\n",
    "        for i, ax in np.ndenumerate(g.axes):\n",
    "            if i[0] != i[1] and i[0] < len(variables) and i[1] < len(variables):  # we're not on the diagonal and the indices are within range\n",
    "                var1 = variables[i[0]]\n",
    "                var2 = variables[i[1]]\n",
    "\n",
    "                # Start with a y-coordinate at the top of the axis and decrement it for each metric\n",
    "                y_coord = 1  # adjust starting point if necessary\n",
    "                y_decrement = 0.1  # adjust decrement value if necessary\n",
    "\n",
    "                for metric in metrics:\n",
    "                    value = results_dict[metric][name][f'{var1}_{var2}']\n",
    "                    ax.text(0.05, y_coord, f'{label_mapping[metric]}: {value:.2f}', transform=ax.transAxes, ha='left', va='top')\n",
    "                    \n",
    "                    y_coord -= y_decrement  # decrement y-coordinate for next metric\n",
    "\n",
    "        # Set the plot title\n",
    "        if standardized:\n",
    "            g.fig.suptitle(f'{ds.source_id} ({ds.period[0]} - {ds.period[1]}) Anomalies', y=1.02)\n",
    "        else:\n",
    "            g.fig.suptitle(f'{ds.source_id} ({ds.period[0]} - {ds.period[1]})', y=1.02)\n",
    "\n",
    "        # Add Legend\n",
    "        legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'{label_mapping[metric]}: {metric}', markersize=5) for metric in metrics]\n",
    "        g.fig.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n",
    "\n",
    "        # Save the figure if specified\n",
    "        if save_fig:\n",
    "            if standardized:\n",
    "                # Define save path and filename\n",
    "                statistic_dim = 'time'\n",
    "                statistic = ds_dict[list(ds_dict.keys())[0]].statistic\n",
    "                savepath = f'../../results/CMIP6/{ds.experiment_id}/{statistic_dim}/{statistic}/scatter_matrix'\n",
    "                filename = f'{name}_{statistic}_scatter_matrix_standardized.png'\n",
    "                filepath = os.path.join(savepath, filename)\n",
    "                os.makedirs(savepath, exist_ok=True)\n",
    "            else:\n",
    "                # Define save path and filename\n",
    "                statistic_dim = 'time'\n",
    "                statistic = ds_dict[list(ds_dict.keys())[0]].statistic\n",
    "                savepath = f'../../results/CMIP6/{ds.experiment_id}/{statistic_dim}/{statistic}/scatter_matrix'\n",
    "                filename = f'{name}_{statistic}_scatter_matrix.png'\n",
    "                filepath = os.path.join(savepath, filename)\n",
    "                os.makedirs(savepath, exist_ok=True)\n",
    "            \n",
    "            g.savefig(filepath, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793bcbe1-d55f-47bf-9d7a-35b919446ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_mean_values(stats_hist, stats_ssp370, scaling_both=False, scaling_hist=False, full_variable_names=None):\n",
    "    \"\"\"\n",
    "    Plots the boxplots for each variable in each model.\n",
    "\n",
    "    Parameters:\n",
    "    stats_hist (dict): The statistics for the historical period.\n",
    "    stats_ssp370 (dict): The statistics for the SSP370 scenario.\n",
    "    scaling (bool): whether to scale y-axis data.\n",
    "    full_variable_names (dict): dictionary of full variable names.\n",
    "    \"\"\"\n",
    "    # Get the list of variables\n",
    "    variables = list(stats_hist[next(iter(stats_hist))].keys())\n",
    "\n",
    "    # Calculate the number of plots and dimensions of the grid of subplots\n",
    "    n_plots = len(variables)\n",
    "    n_cols = min(n_plots, 3)  # Maximum 3 plots in a row\n",
    "    n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "    # Create the figure\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows), squeeze=False)\n",
    "    axs = axs.flatten()  # Flatten the axes array\n",
    "\n",
    "    for i, var in enumerate(variables):\n",
    "        # Prepare lists to store yearly mean values\n",
    "        yearly_means_hist, yearly_means_ssp370 = [], []\n",
    "\n",
    "        for name in stats_hist.keys():\n",
    "            # Extract the yearly mean values for the historical period\n",
    "            yearly_means_hist.append(stats_hist[name][var].values)\n",
    "            \n",
    "            # Extract the yearly mean values for the SSP370 scenario\n",
    "            yearly_means_ssp370.append(stats_ssp370[name][var].values)\n",
    "\n",
    "        # Standardize the data if requested\n",
    "        if scaling_hist:\n",
    "            scaler = StandardScaler()\n",
    "            yearly_means_hist = [scaler.fit_transform(np.array(x).reshape(-1, 1)).flatten() for x in yearly_means_hist]\n",
    "            yearly_means_ssp370 = [scaler.transform(np.array(x).reshape(-1, 1)).flatten() for x in yearly_means_ssp370]\n",
    "            \n",
    "            fig.suptitle(f'Yearly Means of Variables for Historical (1985-2014) and SSP370 (2071-2100) Period Normalized by Historical Mean and Standard Deviation of Historical Data', fontsize=12, y=1.0)\n",
    "            suffix = \"_scaled_historical\"\n",
    "        elif scaling_both:\n",
    "            scaler_hist = StandardScaler()\n",
    "            yearly_means_hist = [scaler_hist.fit_transform(np.array(x).reshape(-1, 1)).flatten() for x in yearly_means_hist]\n",
    "\n",
    "            scaler_ssp370 = StandardScaler()\n",
    "            yearly_means_ssp370 = [scaler_ssp370.fit_transform(np.array(x).reshape(-1, 1)).flatten() for x in yearly_means_ssp370]\n",
    "            \n",
    "            fig.suptitle(f'Yearly Means of Variables for Historical (1985-2014) and SSP370 (2071-2100) Period Normalized by Respective Mean and Standard Deviation', fontsize=12, y=1.0)\n",
    "            suffix = \"_scaled_both\"\n",
    "        else:\n",
    "            fig.suptitle(f'Yearly Means of Variables for Historical (1985-2014) and SSP370 (2071-2100)', fontsize=12, y=1.0)\n",
    "            suffix = \"\"\n",
    "\n",
    "            \n",
    "        # Compute the box plot positions\n",
    "        positions = np.arange(len(stats_hist.keys()))\n",
    "\n",
    "        # Select the current axes\n",
    "        ax = axs[i]\n",
    "\n",
    "        # Define an offset for x-values to place box plots for different periods side by side\n",
    "        offset = 0.15\n",
    "\n",
    "        # Plot the box plots for the historical period\n",
    "        ax.boxplot(yearly_means_hist, positions=positions-offset, widths=0.3, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='cornflowerblue'), medianprops=dict(color='black'), \n",
    "                   showfliers=True)\n",
    "        \n",
    "        # Plot the box plots for the SSP370 scenario\n",
    "        ax.boxplot(yearly_means_ssp370, positions=positions+offset, widths=0.3, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='sandybrown'), medianprops=dict(color='black'), \n",
    "                   showfliers=True)\n",
    "\n",
    "        # Set the x-ticks labels and the title\n",
    "        ax.set_xticks(positions)\n",
    "        ax.set_xticklabels(stats_hist.keys(), rotation=90)\n",
    "        ax.set_ylabel(f'[{full_variable_names[var][1]}]', fontsize=9)\n",
    "        \n",
    "        # Set title with full variable name if provided\n",
    "        if full_variable_names and var in full_variable_names:\n",
    "            ax.set_title(full_variable_names[var][0], fontsize=9)\n",
    "        else:\n",
    "            ax.set_title(var)\n",
    "\n",
    "    # Set a legend\n",
    "    axs[0].legend([Patch(facecolor='cornflowerblue'), Patch(facecolor='sandybrown')], ['Historical', 'SSP370'])\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    filename = f\"Variable_changes{suffix}.png\"\n",
    "    \n",
    "    savepath = f'../../results/CMIP6/yearly_mean_comparison'\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    filepath = os.path.join(savepath, filename)\n",
    "\n",
    "    fig.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391c48e-d011-48de-baa6-ad1d829a54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_change_map(ds_dict, variable, cmap='viridis', save_fig=False, file_format='png'):\n",
    "    \"\"\"\n",
    "    Plots a map of the specified statistic of the given variable for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        variable (str): The name of the variable to plot.\n",
    "        cmap (str): The name of the colormap to use for the plot. Default is 'viridis'.\n",
    "        save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "        file_format (str): The format of the saved figure. Default is 'png'.\n",
    "\n",
    "    Returns:\n",
    "        str: The file path where the figure was saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check arguments and get info\n",
    "    var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles = check_args_and_get_info(ds_dict, variable) \n",
    "    \n",
    "    # Create a figure\n",
    "    n_datasets_with_var = sum([1 for ds in ds_dict.values() if variable in ds])\n",
    "    n_cols = 4  # Set number of columns to 4\n",
    "    n_rows = math.ceil(n_datasets_with_var / n_cols)  # Calculate rows\n",
    "    \n",
    "    fig = plt.figure(figsize=[12 * n_cols, 6.5 * n_rows])  # Start with a blank figure, without subplots\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Loop over datasets and plot the requested statistic\n",
    "    subplot_counter = 0\n",
    "    for i, (name, ds) in enumerate(ds_dict.items()):\n",
    "        if variable not in ds:\n",
    "            print(f\"Variable '{variable}' not found in dataset '{name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Add a new subplot with a cartopy projection\n",
    "        ax = fig.add_subplot(n_rows, n_cols, subplot_counter+1, projection=ccrs.Robinson())\n",
    "        \n",
    "        data_to_plot = ds[variable]\n",
    "        im = data_to_plot.plot(ax=ax, cmap=cmap, transform=ccrs.PlateCarree(), add_colorbar=False)  # Added a cartopy transform to the plot and cmap parameter\n",
    "        ax.set_title(name, fontsize=18)\n",
    "        ax.coastlines()  # Adds lines around the continents\n",
    "\n",
    "        subplot_counter += 1\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # Add a common colorbar at the bottom of the plots\n",
    "    cbar = fig.colorbar(im, ax=fig.axes, orientation='horizontal', fraction=0.02, pad=0.04, aspect=75, shrink=0.4)\n",
    "    \n",
    "    # Set tick size\n",
    "    cbar.ax.tick_params(labelsize=20)  # Adjust size as needed\n",
    "    \n",
    "    # Set colorbar label\n",
    "    cbar.set_label(f\"{var_long_name} [{unit}]\", size=26)  # Adjust size as needed\n",
    "    \n",
    "    # Set figure title with first and last year of dataset \n",
    "    fig.suptitle(f\"{titles[statistic_dim]} {titles[statistic]} of {var_long_name} ({period})\", fontsize=26, y=0.9)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Safe figure\n",
    "    if save_fig:\n",
    "        savepath = os.path.join('../..', 'results', 'CMIP6', 'comparison', 'change_maps')\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        filename = f'{statistic}.{variable}.{ds.experiment_id}.{file_format}'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        fig.savefig(filepath, dpi=300)\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641a772-cd43-4bee-85ab-631cb58eca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_change_map(ds_dict, variable, cmap='viridis', metric='mean', save_fig=False, file_format='png'):\n",
    "    \"\"\"\n",
    "    Plots a map of the specified statistic of the given variable for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        variable (str): The name of the variable to plot.\n",
    "        cmap (str): The name of the colormap to use for the plot. Default is 'viridis'.\n",
    "        save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "        file_format (str): The format of the saved figure. Default is 'png'.\n",
    "\n",
    "    Returns:\n",
    "        str: The file path where the figure was saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check arguments and get info\n",
    "    if ds_dict[list(ds_dict.keys())[0]].attrs['change'] == 'Relative Change':\n",
    "        var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles = check_args_and_get_info(ds_dict, variable)\n",
    "        unit = '%'\n",
    "    else:\n",
    "        var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles = check_args_and_get_info(ds_dict, variable)\n",
    "    \n",
    "    # Create a figure\n",
    "    n_datasets_with_var = sum([1 for ds in ds_dict.values() if variable in ds])\n",
    "    n_cols = 4  # Set number of columns to 4\n",
    "    n_rows = math.ceil(n_datasets_with_var / n_cols)  # Calculate rows\n",
    "    \n",
    "    fig = plt.figure(figsize=[12 * n_cols, 6.5 * n_rows])  # Start with a blank figure, without subplots\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Loop over datasets and plot the requested statistic\n",
    "    subplot_counter = 0\n",
    "    for i, (name, ds) in enumerate(ds_dict.items()):\n",
    "        if variable not in ds:\n",
    "            print(f\"Variable '{variable}' not found in dataset '{name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Add a new subplot with a cartopy projection\n",
    "        ax = fig.add_subplot(n_rows, n_cols, subplot_counter+1, projection=ccrs.Robinson())\n",
    "        \n",
    "        data_to_plot = ds[variable]\n",
    "        im = data_to_plot.plot(ax=ax, vmin=-100, vmax=100, cmap=cmap, transform=ccrs.PlateCarree(), add_colorbar=False)  # Added a cartopy transform to the plot and cmap parameter\n",
    "        ax.set_title(name, fontsize=18)\n",
    "        ax.coastlines()  # Adds lines around the continents\n",
    "\n",
    "        subplot_counter += 1\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # Add a common colorbar at the bottom of the plots\n",
    "    cbar = fig.colorbar(im, ax=fig.axes, orientation='horizontal', fraction=0.02, pad=0.04, aspect=75, shrink=0.4)\n",
    "    \n",
    "    # Set tick size\n",
    "    cbar.ax.tick_params(labelsize=20)  # Adjust size as needed\n",
    "    \n",
    "    # Set colorbar label\n",
    "    cbar.set_label(f\"{var_long_name} Change [{unit}]\", size=26)  # Adjust size as needed\n",
    "    \n",
    "    # Set figure title with first and last year of dataset \n",
    "    if ds_dict[list(ds_dict.keys())[0]].attrs['change'] == 'Relative Change':\n",
    "        fig.suptitle(f\"Relative Change of {var_long_name} {titles[statistic]} ({period})\", fontsize=26, y=0.9)\n",
    "    else:\n",
    "        fig.suptitle(f\"Absolute Change of {var_long_name} {titles[statistic]} ({period})\", fontsize=26, y=0.9)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Safe figure\n",
    "    if save_fig:\n",
    "        savepath = os.path.join('../..', 'results', 'CMIP6', 'ssp370-historical', 'time', metric, 'change_maps')\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        if ds_dict[list(ds_dict.keys())[0]].attrs['change'] == 'Relative Change':\n",
    "            filename = f'relative_change.{statistic}.{variable}.{ds.experiment_id}.{file_format}'\n",
    "        else:\n",
    "            filename = f'absolute_change.{statistic}.{variable}.{ds.experiment_id}.{file_format}'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        fig.savefig(filepath, dpi=300)\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c6728-aa37-48ed-88ee-2bca84fa1521",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4662a-8fef-488c-8c0a-1ea1adcac169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics_with_mean(stats_dict, experiment_id, variables, metric='pearson', scale_axis=False, variable_captions=None):\n",
    "    \"\"\"\n",
    "    Plots the specified metric for variable combinations beginning with each variable in variables for all models. Marks the Ensemble_mean for each variable combination.\n",
    "\n",
    "    Parameters:\n",
    "    stats_dict (dict): A dictionary of computed statistics.\n",
    "    experiment_id (str): Scenario.\n",
    "    variables (list): The names of the variables.\n",
    "    metric (str): The metric to plot. Options are 'r2_lr', 'pearson', 'spearman', 'kendalltau'.\n",
    "    scale_axis (bool): Whether to scale the y-axis according to metric value ranges. Default is False.\n",
    "    \"\"\"\n",
    "    models = list(stats_dict[metric].keys())\n",
    "\n",
    "    # Map for complete metric names and symbols\n",
    "    metric_map = {\n",
    "        'r2_lr': ('Coefficient of Determination', 'R²'),\n",
    "        'pearson': ('Pearson Correlation Coefficient', 'r'),\n",
    "        'spearman': ('Spearman Rank Correlation Coefficient', 'ρ'),\n",
    "        'kendalltau': ('Kendall Rank Correlation Coefficient', 'τ')\n",
    "    }\n",
    "\n",
    "    # Period map\n",
    "    period_map = {\n",
    "        'historical': '1985-2014',\n",
    "        'ssp370': '2071-2100'\n",
    "    }\n",
    "\n",
    "    # Variable name replacement map\n",
    "    variable_name_map = {\n",
    "        'lmrso_1m': 'sm1m',\n",
    "        'lmrso_2m': 'sm2m'\n",
    "    }\n",
    "\n",
    "    # Define markers and colors for each model\n",
    "    markers = ['o', 'v', '^', '<', '>', 's', 'p', '*', 'h', 'H', '+', 'x', 'D', 'd', '|', '_']\n",
    "    colors = sns.color_palette(\"colorblind\", len(models))\n",
    "    model_marker = {model: marker for model, marker in zip(models, markers)}\n",
    "    model_color = {model: color for model, color in zip(models, colors)}\n",
    "\n",
    "    # Define grid size\n",
    "    grid_size = int(np.ceil(np.sqrt(len(variables))))\n",
    "\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(20, 20))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i, variable in enumerate(variables):\n",
    "        variable_combinations = [var_comb for var_comb in stats_dict[metric][models[0]].keys() if var_comb.startswith(variable)]\n",
    "\n",
    "        # Replace variable names in combinations for labeling\n",
    "        formatted_variable_combinations = [var_comb for var_comb in variable_combinations]\n",
    "        for old_name, new_name in variable_name_map.items():\n",
    "            formatted_variable_combinations = [var_comb.replace(old_name, new_name) for var_comb in formatted_variable_combinations]\n",
    "\n",
    "        # Replace underscore with \" x \"\n",
    "        formatted_variable_combinations = [var_comb.replace(\"_\", \" x \") for var_comb in formatted_variable_combinations]\n",
    "\n",
    "        # Scatter plot for each model\n",
    "        for model in models:\n",
    "            # Exclude the ensemble mean from this loop\n",
    "            if model != \"Ensemble_mean\":\n",
    "                values = [stats_dict[metric][model][var_comb] for var_comb in variable_combinations]\n",
    "                axs[i].scatter(formatted_variable_combinations, values, label=model, marker=model_marker[model], color=model_color[model])\n",
    "\n",
    "        # Mark the Ensemble_mean value for each variable combination\n",
    "        ensemble_mean_values = [stats_dict[metric][\"Ensemble_mean\"][var_comb] for var_comb in variable_combinations]\n",
    "        axs[i].scatter(formatted_variable_combinations, ensemble_mean_values, label='Ensemble mean', color='red', marker='x')\n",
    "        axs[i].set_title(variable_captions[variable]) \n",
    "\n",
    "        axs[i].set_ylabel(metric_map[metric][1])\n",
    "\n",
    "        if scale_axis:\n",
    "            if metric == 'r2_lr':\n",
    "                axs[i].set_ylim(0, 1)\n",
    "            else:\n",
    "                axs[i].set_ylim(-1, 1)\n",
    "\n",
    "        axs[i].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    # Delete extra subplots if any\n",
    "    if len(variables) < len(axs):\n",
    "        for j in range(len(variables), len(axs)):\n",
    "            fig.delaxes(axs[j])\n",
    "\n",
    "    # Add a single legend outside of the plots\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1.1, 1), title='Models')\n",
    "    \n",
    "    if experiment_id == 'historical':\n",
    "        fig.suptitle(f'{metric_map[metric][0]} for {experiment_id.capitalize()} Period ({period_map[experiment_id]})', fontsize=16, y=1.0)\n",
    "    else:\n",
    "        fig.suptitle(f'{metric_map[metric][0]} for {experiment_id} Period ({period_map[experiment_id]})', fontsize=16, y=1.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    savepath = f'../../results/CMIP6/{experiment_id}/time/mean/metrics_comparison'\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    if scale_axis:\n",
    "        filename = f'{metric}_comparison_figure_scaled_axis.png'\n",
    "    else:\n",
    "        filename = f'{metric}_comparison_figure.png'\n",
    "    filepath = os.path.join(savepath, filename)\n",
    "\n",
    "    fig.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39238c09-3874-41ae-a859-a782bb16917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_correlations(yearly_correlations, variable_pairs, target_variable):\n",
    "    \"\"\"\n",
    "    Plots the time series of correlations for each variable pair that includes the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    yearly_correlations (dict): The output from calculate_yearly_correlations.\n",
    "    variable_pairs (list of tuples): The pairs of variables that the correlations were calculated for.\n",
    "    target_variable (str): The variable that must be included in a pair for it to be plotted.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the number of plots\n",
    "    n_plots = sum([var1 == target_variable or var2 == target_variable for var1, var2 in variable_pairs])\n",
    "\n",
    "    # Calculate the dimensions of the grid of subplots\n",
    "    grid_size = math.ceil(math.sqrt(n_plots))\n",
    "    \n",
    "    # Create the figure\n",
    "    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15), sharex=True, sharey=True)\n",
    "\n",
    "    # Flatten the axes\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Create an index for the current plot\n",
    "    i_plot = 0\n",
    "\n",
    "    # Prepare a list to store handles and labels for the legend\n",
    "    handles, labels = [], []\n",
    "\n",
    "    for var1, var2 in variable_pairs:\n",
    "        if var1 == target_variable or var2 == target_variable:\n",
    "            # Select the current axes\n",
    "            ax = axs[i_plot]\n",
    "            \n",
    "            # Construct the correlation variable name\n",
    "            corr_var = f'{var1}-{var2}'\n",
    "\n",
    "            # Prepare a list to store correlations of all models\n",
    "            all_corrs = []\n",
    "\n",
    "            for name, ds in yearly_correlations.items():\n",
    "                # Check if this variable exists in the Dataset\n",
    "                if corr_var in ds:\n",
    "                    # Plot the time series of the correlation\n",
    "                    line, = ax.plot(ds['time'], ds[corr_var], label=name)\n",
    "\n",
    "                    # Append to all_corrs\n",
    "                    all_corrs.append(ds[corr_var])\n",
    "\n",
    "                    # Append to handles and labels if not already present\n",
    "                    if name not in labels:\n",
    "                        handles.append(line)\n",
    "                        labels.append(name)\n",
    "\n",
    "            # Compute the mean correlation across all models\n",
    "            mean_corr = xr.concat(all_corrs, dim='model').mean(dim='model')\n",
    "            mean_line, = ax.plot(mean_corr['time'], mean_corr, color='black', linestyle='--')\n",
    "\n",
    "            ax.set_xlabel('Year')\n",
    "            ax.set_ylabel('Correlation')\n",
    "            ax.set_title(f'{var1} vs {var2}')\n",
    "            ax.grid(True)\n",
    "\n",
    "            # Increment the plot index\n",
    "            i_plot += 1\n",
    "\n",
    "    # Add 'Mean' to the legend\n",
    "    handles.append(mean_line)\n",
    "    labels.append('Mean')\n",
    "\n",
    "    # Show the figure with a legend\n",
    "    fig.legend(handles, labels, loc='center right', bbox_to_anchor=(1.1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10db210-8757-4858-ae1e-4f89e7949440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_mean_correlations(correlations_hist, correlations_ssp370, variable_pairs, target_variable, scale_axis=False, variable_captions=None):\n",
    "    \"\"\"\n",
    "    Plots the mean correlations for each variable pair that includes the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    correlations_hist (dict): The output from calculate_correlations for the historical period.\n",
    "    correlations_ssp370 (dict): The output from calculate_correlations for the SSP370 scenario.\n",
    "    variable_pairs (list of tuples): The pairs of variables that the correlations were calculated for.\n",
    "    target_variable (str): The variable that must be included in a pair for it to be plotted.\n",
    "    scale_axis (bool): Whether to scale the y-axis according to metric value ranges. Default is False.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get info\n",
    "    metric = correlations_hist[list(correlations_hist.keys())[0]].Metric\n",
    "    metric_sign = correlations_hist[list(correlations_hist.keys())[0]].Metric_sign\n",
    "    \n",
    "    # Filter variable pairs\n",
    "    variable_pairs = [(var1, var2) for var1, var2 in variable_pairs if var1 == target_variable or var2 == target_variable]\n",
    "\n",
    "    # Calculate the number of plots and dimensions of the grid of subplots\n",
    "    n_plots = len(variable_pairs)\n",
    "    n_cols = min(n_plots, 3)  # Maximum 3 plots in a row\n",
    "    n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "    # Create the figure\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows), squeeze=False)\n",
    "    axs = axs.flatten()  # Flatten the axes array\n",
    "    \n",
    "    for i, (var1, var2) in enumerate(variable_pairs):\n",
    "        # Prepare lists to store yearly correlation values\n",
    "        yearly_corr_hist, yearly_corr_ssp370 = [], []\n",
    "\n",
    "        for name in correlations_hist.keys():\n",
    "            # Extract the yearly mean values for the historical period\n",
    "            yearly_corr_hist.append(correlations_hist[name][f'{var1}-{var2}'].values)\n",
    "\n",
    "            # Extract the yearly mean values for the SSP370 scenario\n",
    "            yearly_corr_ssp370.append(correlations_ssp370[name][f'{var1}-{var2}'].values)\n",
    "\n",
    "        # Compute the box plot positions\n",
    "        positions = np.arange(len(correlations_hist.keys()))\n",
    "\n",
    "        # Select the current axes\n",
    "        ax = axs[i]\n",
    "\n",
    "        # Define an offset for x-values to place box plots for different periods side by side\n",
    "        offset = 0.15\n",
    "\n",
    "        # Plot the box plots for the historical period\n",
    "        ax.boxplot(yearly_corr_hist, positions=positions-offset, widths=0.3, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='cornflowerblue'), medianprops=dict(color='black'), \n",
    "                   showfliers=True)\n",
    "\n",
    "        # Plot the box plots for the SSP370 scenario\n",
    "        ax.boxplot(yearly_corr_ssp370, positions=positions+offset, widths=0.3, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='sandybrown'), medianprops=dict(color='black'), \n",
    "                   showfliers=True)\n",
    "\n",
    "        # Set the x-ticks labels and the title\n",
    "        ax.set_ylabel(f'{metric_sign}')\n",
    "        ax.set_title(f'{variable_captions.get(var1, var1)} x {variable_captions.get(var2, var2)}', fontsize=9)\n",
    "        ax.set_xticks(positions)\n",
    "        ax.set_xticklabels(list(correlations_hist.keys()), rotation=90)\n",
    "        \n",
    "    fig.suptitle(f'Yearly {metric} for Historical (1985-2014) and SSP370 (2071-2100) Period', fontsize=12, y=1.0)\n",
    "    \n",
    "    # Set a legend\n",
    "    axs[0].legend([Patch(facecolor='cornflowerblue'), Patch(facecolor='sandybrown')], ['Historical', 'SSP370'])\n",
    "\n",
    "    # Handle empty subplots in case n_plots is less than n_rows * n_cols\n",
    "    for i in range(n_plots, n_rows*n_cols):\n",
    "        fig.delaxes(axs[i])\n",
    "    \n",
    "    # Handle y-axis scaling\n",
    "    if scale_axis:\n",
    "        for ax in axs:\n",
    "            ax.set_ylim([-1, 1] if metric != 'r2_lr' else [0, 1])\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    suffix = \"_scaled_axis\" if scale_axis else \"\"\n",
    "    filename = f\"{metric}_changes_{target_variable}{suffix}.png\"\n",
    "    \n",
    "    savepath = f'../../results/CMIP6/yearly_metrics_comparison'\n",
    "    os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "    filepath = os.path.join(savepath, filename)\n",
    "\n",
    "    fig.savefig(filepath, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555de63-1237-4ed3-b496-48f9f835eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_corr_change_plot(ds_dict, target_variable, full_var_names_and_unit, cmap='viridis', save_fig=False, file_format='png'):\n",
    "    \"\"\"\n",
    "    Plots a map of the specified statistic of the given variable for the Ensemble_mean dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        target_variable (str): The target variable to plot correlations with.\n",
    "        cmap (str): The name of the colormap to use for the plot. Default is 'viridis'.\n",
    "        save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "        file_format (str): The format of the saved figure. Default is 'png'.\n",
    "    \"\"\"\n",
    "    # Info\n",
    "    experiment_id = ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    metric = ds_dict[list(ds_dict.keys())[0]].Metric\n",
    "    metric_sign = ds_dict[list(ds_dict.keys())[0]].Metric_sign\n",
    "    means = ds_dict[list(ds_dict.keys())[0]].attrs['means']\n",
    "    target_var_long_name = full_var_names_and_unit[target_variable][0]\n",
    "    \n",
    "    # Create a figure\n",
    "    n_cols = 3  # Set number of columns to 3\n",
    "    n_rows = 3  # Set number of rows to 3\n",
    "    \n",
    "    fig = plt.figure(figsize=[12 * n_cols, 6.5 * n_rows])  # Start with a blank figure, without subplots\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Loop over datasets and plot the requested statistic\n",
    "    subplot_counter = 0\n",
    "\n",
    "    # Get the Ensemble_mean dataset\n",
    "    ensemble_ds = ds_dict.get(\"Ensemble_mean\", None)\n",
    "\n",
    "    if ensemble_ds is None:\n",
    "        print(\"Ensemble_mean dataset not found.\")\n",
    "        return None\n",
    "\n",
    "    for variable in ensemble_ds.variables:\n",
    "        if not (f'{target_variable} x ' in variable or f' x {target_variable}' in variable):\n",
    "            continue\n",
    "\n",
    "        # Add a new subplot with a cartopy projection\n",
    "        ax = fig.add_subplot(n_rows, n_cols, subplot_counter+1, projection=ccrs.Robinson())\n",
    "\n",
    "        data_to_plot = ensemble_ds[variable]\n",
    "        im = data_to_plot.plot(ax=ax, cmap=cmap, vmin = -1, vmax = 1, transform=ccrs.PlateCarree(), add_colorbar=False)  # Added a cartopy transform to the plot and cmap parameter\n",
    "        \n",
    "        if f'{target_variable} x ' in variable:\n",
    "            corr_var = variable.replace(f'{target_variable} x ', '')\n",
    "        elif f' x {target_variable}' in variable:\n",
    "            corr_var = variable.replace(f' x {target_variable}', '')\n",
    "        else:\n",
    "            continue\n",
    "        corr_var_long_name = full_var_names_and_unit[corr_var][0]\n",
    "        ax.set_title(f'{target_var_long_name} x {corr_var_long_name}', fontsize=18)  # Use the long names in the title\n",
    "        ax.coastlines()  # Adds lines around the continents\n",
    "\n",
    "        subplot_counter += 1\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # Add a common colorbar at the bottom of the plots\n",
    "    cbar = fig.colorbar(im, ax=fig.axes, orientation='horizontal', fraction=0.02, pad=0.04, aspect=75, shrink=0.4)\n",
    "    \n",
    "\n",
    "    # Set colorbar ticks\n",
    "    cbar.set_ticks([-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75])\n",
    "    \n",
    "    \n",
    "    # Set tick size\n",
    "    cbar.ax.tick_params(labelsize=20)  # Adjust size as needed\n",
    "    \n",
    "    # Set colorbar label\n",
    "    cbar.set_label(f'{metric_sign} change', size=26)  # Adjust size as needed\n",
    "    \n",
    "    # Set figure title with first and last year of dataset \n",
    "    if experiment_id == 'historical' or experiment_id == 'ssp370':\n",
    "        fig.suptitle(f\"{metric} ({experiment_id}) of Ensemble Mean for Variable Combinations with {target_var_long_name} ({means})\", fontsize=20, y=0.9)\n",
    "    elif experiment_id == 'ssp370-historical':\n",
    "        fig.suptitle(f\"{metric} Change ({experiment_id}) of Ensemble Mean for Variable Combinations with {target_var_long_name} ({means})\", fontsize=20, y=0.9)\n",
    "    \n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        if experiment_id == 'historical' or experiment_id == 'ssp370':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename = f'ensmean_{target_variable}_correlations.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "        elif experiment_id == 'ssp370-historical':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename =f'ensmean_{target_variable}_correlations_change.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c30d8-7d61-497f-9fd6-689ad3634fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_maps(ds_dict, target_variable_combination, cmap='viridis', save_fig=False, file_format='png'):\n",
    "    \"\"\"\n",
    "    Plots a map of the specified variable combination for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        target_variable_combination (str): The target variable combination to plot.\n",
    "        cmap (str): The name of the colormap to use for the plot. Default is 'viridis'.\n",
    "        save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "        file_format (str): The format of the saved figure. Default is 'png'.\n",
    "    \"\"\"\n",
    "    # Info\n",
    "    experiment_id = ds_dict[list(ds_dict.keys())[0]].experiment_id\n",
    "    metric = ds_dict[list(ds_dict.keys())[0]].Metric\n",
    "    metric_sign = ds_dict[list(ds_dict.keys())[0]].Metric_sign\n",
    "    means = ds_dict[list(ds_dict.keys())[0]].attrs['means']\n",
    "    \n",
    "    # Create a figure\n",
    "    n_datasets_with_var = sum([1 for ds in ds_dict.values() if target_variable_combination in ds.variables])\n",
    "    n_cols = 3  # Set number of columns to 3\n",
    "    n_rows = math.ceil(n_datasets_with_var / n_cols)  # Calculate rows\n",
    "    \n",
    "    fig = plt.figure(figsize=[12 * n_cols, 6.5 * n_rows])  # Start with a blank figure, without subplots\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Loop over datasets and plot the requested variable combination\n",
    "    subplot_counter = 0\n",
    "    for i, (name, ds) in enumerate(ds_dict.items()):\n",
    "        if target_variable_combination not in ds.variables:\n",
    "            continue\n",
    "\n",
    "        # Add a new subplot with a cartopy projection\n",
    "        ax = fig.add_subplot(n_rows, n_cols, subplot_counter+1, projection=ccrs.Robinson())\n",
    "        \n",
    "        data_to_plot = ds[target_variable_combination]\n",
    "        im = data_to_plot.plot(ax=ax, cmap=cmap, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "        ax.set_title(name, fontsize=18)\n",
    "        ax.coastlines()  # Adds lines around the continents\n",
    "\n",
    "        subplot_counter += 1\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "    # Add a common colorbar at the bottom of the plots\n",
    "    cbar = fig.colorbar(im, ax=fig.axes, orientation='horizontal', fraction=0.02, pad=0.04, aspect=75, shrink=0.4)\n",
    "    \n",
    "    # Set tick size\n",
    "    cbar.ax.tick_params(labelsize=14)  # Adjust size as needed\n",
    "    \n",
    "    # Set colorbar label\n",
    "    cbar.set_label(metric_sign, size=18)  # Adjust size as needed\n",
    "    \n",
    "    # Set figure title with first and last year of dataset \n",
    "    if experiment_id == 'historical' or 'ssp370':\n",
    "        fig.suptitle(f\"{metric} ({experiment_id}) for Variable Combination {target_variable_combination} ({means})\", fontsize=20, y=0.9)\n",
    "    elif experiment_id == 'ssp370-historical':\n",
    "        fig.suptitle(f\"{metric} Change ({experiment_id}) for Variable Combination {target_variable_combination} ({means})\", fontsize=20, y=0.9)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        if experiment_id == 'historical' or 'ssp370':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, 'time', 'mean', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename = f'{target_variable_combination}_correlation.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "        elif experiment_id == 'ssp370-historical':\n",
    "            savepath = os.path.join('../..', 'results', 'CMIP6', 'comparison', 'corr_maps', means)\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            filename =f'{target_variable_combination}_correlation_change.{file_format}'\n",
    "            filepath = os.path.join(savepath, filename)\n",
    "            fig.savefig(filepath, dpi=300)\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173177e7-1327-46be-9952-4ba5d9d45155",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343c372-5fab-4276-9c97-171ecae7340b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "variables=['tas', 'pr', 'vpd', 'evspsbl', 'mrro', 'lmrso_1m', 'lmrso_2m', 'tran', 'lai', 'gpp', 'wue', 'EI', 'nbwfp']\n",
    "experiment_id = 'historical'\n",
    "source_id = ['Ensemble mean']#['TaiESM1', 'BCC-CSM2-MR',  'CanESM5', 'CNRM-CM6-1', 'CNRM-ESM2-1', 'IPSL-CM6A-LR', 'UKESM1-0-LL', 'MPI-ESM1-2-LR', 'CESM2-WACCM', 'NorESM2-MM', 'Ensemble mean', 'Ensemble median'] #\n",
    "folder='preprocessed'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# Create dictionary using a dictionary comprehension and Dask\n",
    "ds_dict_hist = dask.compute({model: open_and_merge_datasets(folder, model, experiment_id, variables) for model in source_id})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f749cba-efb8-4532-9e4b-a7b880530fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============= Have a look into the data ==============\n",
    "print(ds_dict_hist.keys())\n",
    "ds_dict_hist[list(ds_dict_hist.keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee8a73-d991-4fbf-a004-1b2d7c87b88d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Slice regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97cb32-6320-4eb2-ac31-96755b267fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lat/lon bounds for each region\n",
    "regions = {\n",
    "        'Greenland': {'lat': slice(59, 89), 'lon': slice(-65, -12)},\n",
    "        'Northern North America': {'lat': slice(40, 73), 'lon': slice(-170, -53)},\n",
    "        'Southern North America': {'lat': slice(15, 40), 'lon': slice(-130, -55)},\n",
    "        'Northern South America': {'lat': slice(-25, 15), 'lon': slice(-85, -30)},\n",
    "        'Southern South America': {'lat': slice(-59, -25), 'lon': slice(-80, -40)},\n",
    "        'Northern Europe': {'lat': slice(45, 89), 'lon': slice(-12, 40)},\n",
    "        'Mediterranean and Middle East': {'lat': slice(30, 45), 'lon': slice(-12, 55)},\n",
    "        'Sahara': {'lat': slice(12, 30), 'lon': slice(-20, 55)},\n",
    "        'Sub-Sahara Africa': {'lat': slice(-35, 12), 'lon': slice(-20, 55)},\n",
    "        'Northern Asia': {'lat': slice(45, 89), 'lon': slice(40, 179)},\n",
    "        'Southwest Asia': {'lat': slice(0, 45), 'lon': slice(55, 90)},\n",
    "        'Southeast Asia': {'lat': slice(-11, 45), 'lon': slice(90, 165)},\n",
    "        'Oceania': {'lat': slice(-50, -11), 'lon': slice(110, 180)}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7ff62-b9f0-4add-b1a0-0883fc1a4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_regions = slice_to_regions(ds_dict_hist, regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2357b23-f907-455e-beaf-e6181069b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_regions = slice_to_regions(ds_dict_ssp370, regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f417b3-2b33-4081-b1d8-6d3a1c21d269",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29262fe-a12e-44ff-894e-6d851abac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_period = select_period(ds_dict_hist, start_year=1985, end_year=2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a12260-1df3-4232-bf91-196fb8144269",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_period = select_period(ds_dict_ssp370, start_year=2071, end_year=2100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de9ead-0d36-4f06-9430-9efeae7cc9bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b6390-f4b2-41d0-951d-c6bae78ce192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_hist_period_metric = compute_statistic(ds_dict_hist_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a9c21-8094-4700-b972-36e3f720d629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Compute statistic for plot ===============\n",
    "ds_dict_ssp370_period_metric = compute_statistic(ds_dict_ssp370_period, 'mean', 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c67561f-123d-49f6-9a57-eb04f44d71e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499fe44b-08e9-4098-84c3-bda21c7e1c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_stand = standardize(ds_dict_hist_period_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ab8b3-08ef-4ac2-a248-37a22aebe685",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_stand = standardize(ds_dict_ssp370_period_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a221d0c-a4a5-4267-aae1-daefe5ce2b0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot Scatter Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca22d13-3e0c-48ab-a34f-ea6875465d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define statistical metrics\n",
    "metrics=['r2_lr','pearson', 'spearman', 'kendalltau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc8ddd-834f-443b-ba87-1d544ce51e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables=['tas', 'pr', 'vpd', 'evspsbl', 'mrro', 'lmrso_1m', 'lmrso_2m', 'tran', 'lai', 'gpp', 'wue', 'nbwfp']\n",
    "full_var_names_and_unit = {\n",
    "    'pr': ('Precipitation','mm/day'),\n",
    "    'vpd': ('Vapor Pressure Deficit', 'hPa'),\n",
    "    'evspsbl': ('Evapotranspiration', 'mm/day'),\n",
    "    'tran': ('Transpiration', 'mm/day'),\n",
    "    'mrro': ('Total Runoff', 'mm/day'),\n",
    "    'lmrso_1m': ('Soil Moisture 1 m Column', 'kg/m²'),\n",
    "    'lmrso_2m': ('Soil Moisture 2 m Column',  'kg/m²'),\n",
    "    'gpp': ('Gross Primary Production', 'gC/m²/day'),\n",
    "    'lai': ('Leaf Area Index', '')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e57e92-276a-4eef-927b-b6717e41b47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Precompute the metrics\n",
    "results_dict = precompute_metrics(ds_dict_hist_stand, variables, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c032a41-298b-4c4b-8815-84e37e6ce2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da338b0-b8ce-45a0-9a14-eedab9d59456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the scatter matrix using the precomputed metrics\n",
    "plot_scatter_matrix(ds_dict_hist_period_metric, variables, results_dict, metrics, save_fig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c057d851-681a-4ebc-9e38-b8e0e0070dcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot Metrics per Variable Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182cdba-6c30-4eb3-8eb5-df5f2f324d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables=['pr', 'vpd', 'evspsbl', 'tran',  'mrro', 'lmrso_1m', 'lmrso_2m', 'gpp', 'lai']\n",
    "plot_metrics_with_mean(results_dict, experiment_id, variables, 'kendalltau', scale_axis=False, full_var_names_and_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488a2f7-ce69-48ad-bbd4-c7e63e77107b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute Yearly Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43758c12-96ab-41ca-a75c-0e24d6a5bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Create unique variable pairs ========\n",
    "variable_pairs = [(variables[i], variables[j]) for i in range(len(variables)) for j in range(i+1, len(variables))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccca012-7206-4c0b-91bf-feb41563bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Compute yearly correlation =======\n",
    "yearly_correlations_ssp370 = calculate_yearly_correlations(ds_dict_hist, variable_pairs, corr_type='pearson')#, start_year=20, end_year=2100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573ff9a-0b0f-41a8-83be-9f8606109d8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Plot Yearly Correlation Timeline (make this again over the whole period from 1850 to 2100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9e57e-df64-4f98-9bbc-9b7ea9fb6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Plot timeseries for each variable combination\n",
    "plot_time_series_correlations(yearly_correlations_hist, variable_pairs, 'pr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3daf6a5-e25a-4ab0-b064-97af25d88ee4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Plot Yearly Correlation Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb169e4-5c23-467b-a2f4-def0855e86a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_mean_correlations(yearly_correlations_hist, yearly_correlations_ssp370, variable_pairs, 'gpp', scale_axis=False, full_var_names_and_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d03a7-de40-43cf-a562-8d6f14d824df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Grouped Boxplots for Variable Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3216b-6614-4a17-9090-900573c2e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_hist = compute_stats(ds_dict_hist)\n",
    "stats_ssp370 = compute_stats(ds_dict_ssp370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3abcc5-ec61-4ce5-a384-b282534580db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_values(stats_hist, stats_ssp370, scaling_both=False, scaling_hist=True, full_variable_names=full_var_names_and_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1d37ae-b0fd-485b-92b4-8225faa64e48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Timeline of all variables over whole periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54daa3a6-168f-414f-94f9-aa4e8aa2bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_consecutive_periods(ds_dict_hist, ds_dict_ssp370, save_fig=False, fig_name='plot.png'):\n",
    "    # Define the order of the variables\n",
    "    variable_order = ['pr', 'vpd', 'mrro', 'evspsbl', 'lmrso_1m', 'lmrso_2m', 'tran', 'gpp', 'lai']\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\n",
    "    axes = axes.flatten()  # Flatten the axes array to easily iterate over\n",
    "    fig.suptitle('Variable Change for Historical and SSP370 Period', fontsize=14)\n",
    "\n",
    "    for i, variable in enumerate(variable_order):\n",
    "        ax = axes[i]  # No need to subtract 1 because we're iterating over variable_order list\n",
    "        \n",
    "        # Check if variable exists in the dictionary\n",
    "        if variable not in ds_dict_hist['Ensemble_mean'].variables:\n",
    "            print(f\"Variable {variable} not found in dataset. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Get the data for each period\n",
    "        data_hist = [ds[variable] for ds in ds_dict_hist.values() if ds != 'Ensemble_mean']\n",
    "        data_ssp370 = [ds[variable] for ds in ds_dict_ssp370.values() if ds != 'Ensemble_mean']\n",
    "        \n",
    "        # Extract ensemble mean from the dictionaries\n",
    "        mean_hist = ds_dict_hist['Ensemble_mean'][variable]\n",
    "        mean_ssp370 = ds_dict_ssp370['Ensemble_mean'][variable]\n",
    "\n",
    "        # Calculate the standard deviation for each period\n",
    "        std_dev_hist = np.std(data_hist, ddof=1)\n",
    "        std_dev_ssp370 = np.std(data_ssp370, ddof=1)\n",
    "\n",
    "        # Plot the means with shaded areas for the model spread\n",
    "        ax.plot(mean_hist.year, mean_hist, label='Historical')\n",
    "        ax.fill_between(mean_hist.year, mean_hist - std_dev_hist, mean_hist + std_dev_hist, alpha=0.3)\n",
    "        ax.plot(mean_ssp370.year, mean_ssp370, label='SSP370')\n",
    "        ax.fill_between(mean_ssp370.year, mean_ssp370 - std_dev_ssp370, mean_ssp370 + std_dev_ssp370, alpha=0.3)\n",
    "\n",
    "        # Get units and long_name, or use '' if not present\n",
    "        unit = ds_dict_hist['Ensemble_mean'][variable].attrs.get('units', '')\n",
    "        long_name = ds_dict_hist['Ensemble_mean'][variable].attrs.get('long_name', '')\n",
    "\n",
    "        # Add labels and legend\n",
    "        ax.set_xlabel('Year')\n",
    "        ax.set_ylabel(unit)\n",
    "        ax.set_title(long_name)\n",
    "        ax.legend()\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for i in range(len(variable_order), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # adjust subplot positions so the title doesn't overlap\n",
    "\n",
    "    # Save the figure if specified\n",
    "    if save_fig:\n",
    "        # Define save path and filename\n",
    "        statistic_dim = 'space'\n",
    "        statistic = ds_dict_hist[list(ds_dict_hist.keys())[0]].statistic\n",
    "        savepath = f'../../results/CMIP6/ssp370-historical/{statistic_dim}/{statistic}/line_plot_variable_change'\n",
    "        filename = f'Line_plot_variable_change.png'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59884e43-c839-436c-9b70-e3acc8c750d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "plot_consecutive_periods(ds_dict_hist_mean, ds_dict_ssp370_mean, save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95cbc7-0430-4675-a771-9b69defd0beb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Timeline and historical ensmean map of variables for regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c36800-2473-408f-9095-12f9bf6a6bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_yearly = compute_yearly_means(ds_dict_hist_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9d031-0fc0-443d-9ad5-78018d5d3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_yearly = compute_yearly_means(ds_dict_ssp370_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379c534-9b15-4357-b709-8180d3a351c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_with_regions(ds_dict_metric, ds_dict_hist_yearly, ds_dict_ssp370_yearly, regions, model, metric='mean', target_var='pr', save_fig=False):\n",
    "    \n",
    "    fig = plt.figure(figsize=(22, 10)) \n",
    "\n",
    "    ax_main = fig.add_axes([0.25, 0.25, 0.7, 0.7], projection=ccrs.PlateCarree())  # Define location and size of main plot\n",
    "    \n",
    "    # Compute anomalies\n",
    "    ds_anomaly = ds_dict_metric[model][target_var].copy()\n",
    "    #global_mean = ds_anomaly.mean()\n",
    "    #ds_anomaly = ds_anomaly - global_mean\n",
    "    \n",
    "    # Define vmin vmax\n",
    "    if target_var == 'vpd':\n",
    "        vmin = -1\n",
    "        vmax = 1\n",
    "    elif target_var == 'lmrso_1m' or target_var == 'lmrso_2m' or target_var == 'WUE':\n",
    "        vmin = -50\n",
    "        vmax = 50\n",
    "    else:\n",
    "        vmin = -100\n",
    "        vmax = 100\n",
    "    \n",
    "    # Plot the global data\n",
    "    img = ds_anomaly.plot(ax=ax_main, vmin=vmin, vmax=vmax, cmap='bwr', transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "    \n",
    "    # Add gridlines\n",
    "    gridlines = ax_main.gridlines(draw_labels=True, color='black', alpha=0.2, linestyle='--')\n",
    "    gridlines.top_labels = gridlines.right_labels = False  # Only draw labels on bottom and left side\n",
    "    \n",
    "    long_name = {\n",
    "        'Precipitation': 'Precipitation',\n",
    "        'Total Runoff': 'Total Runoff',\n",
    "        'Vapor Pressure Deficit': 'Vapor Pressure Deficit',\n",
    "        'Evaporation Including Sublimation and Transpiration': 'Evapotranspiration',\n",
    "        'Transpiration': 'Transpiration',\n",
    "        'Leaf Area Index': 'Leaf Area Index',\n",
    "        'Carbon Mass Flux out of Atmosphere Due to Gross Primary Production on Land [kgC m-2 s-1]': 'Gross Primary Production',\n",
    "        'Total Liquid Soil Moisture Content of 1 m Column': '1 m Soil Moisture',\n",
    "        'Total Liquid Soil Moisture Content of 2 m Column': '2 m Soil Moisture',\n",
    "        'Water Use Efficiency': 'Water Use Efficiency' \n",
    "    }\n",
    "\n",
    "\n",
    "    # Define the locations for each line plot\n",
    "    line_plot_locations = {\n",
    "        'Greenland': [0.25, 1.0, 0.2, 0.2],\n",
    "        'Northern North America': [0, 1.0, 0.2, 0.2],\n",
    "        'Southern North America':[0, 0.66, 0.2, 0.2],\n",
    "        'Northern South America':[0, 0.33, 0.2, 0.2],\n",
    "        'Southern South America':[0, 0, 0.2, 0.2],\n",
    "        'Northern Europe':[0.75, 1.0, 0.2, 0.2],\n",
    "        'Mediterranean and Middle East':[0.5, 1.0, 0.2, 0.2],\n",
    "        'Sahara':[0.25, 0, 0.2, 0.2],\n",
    "        'Sub-Sahara Africa':[0.75, 0, 0.2, 0.2],\n",
    "        'Northern Asia': [1.0, 1.0, 0.2, 0.2],\n",
    "        'Southwest Asia': [1.0, 0.33, 0.2, 0.2],\n",
    "        'Southeast Asia': [1.0, 0.66, 0.2, 0.2],\n",
    "        'Oceania': [1.0, 0, 0.2, 0.2]\n",
    "    }\n",
    "    \n",
    "    # Define custom legend handles\n",
    "    handle1_hist = mpatches.Patch(color='blue', alpha=0.2, label='10th-90th Percentile Historical')\n",
    "    handle2_hist = mpatches.Patch(color='blue', alpha=0.1, label='25th-75th Percentile Historical')\n",
    "    handle1_ssp370 = mpatches.Patch(color='orange', alpha=0.2, label='10th-90th Percentile SSP370')\n",
    "    handle2_ssp370 = mpatches.Patch(color='orange', alpha=0.1, label='25th-75th Percentile SSP370')\n",
    "    legend_handles = [handle1_hist, handle2_hist, handle1_ssp370, handle2_ssp370]\n",
    "    legend_labels = ['10th-90th Percentile Historical', '25th-75th Percentile Historical', '10th-90th Percentile SSP370', '25th-75th Percentile SSP370']\n",
    "    \n",
    "    # Define a list of identifiers\n",
    "    identifiers = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "    # Before plotting the subplots, initialize an iterator over the identifiers\n",
    "    identifiers_iter = iter(identifiers)\n",
    "    \n",
    "    offset_lon = 1  # Define an offset in longitude units. You may need to adjust this.\n",
    "    offset_lat = -1  # Define an offset in latitude units. You may need to adjust this.\n",
    "    \n",
    "    ds_percentiles = {}\n",
    "\n",
    "    # Loop over the regions and plot each one\n",
    "    for region, coord in regions.items():\n",
    "        \n",
    "        identifier = next(identifiers_iter)\n",
    "        \n",
    "        lat_min = coord['lat'].start\n",
    "        lat_max = coord['lat'].stop\n",
    "        lon_min = coord['lon'].start\n",
    "        lon_max = coord['lon'].stop\n",
    "        \n",
    "        # Calculate the center of the box\n",
    "        lat_center = (lat_min + lat_max) / 2\n",
    "        lon_center = (lon_min + lon_max) / 2\n",
    "        \n",
    "        # Draw the box\n",
    "        ax_main.plot([lon_min, lon_max, lon_max, lon_min, lon_min],\n",
    "                [lat_min, lat_min, lat_max, lat_max, lat_min],\n",
    "                color='black', transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # Add identifier to main plot\n",
    "        ax_main.text(lon_min + offset_lon, lat_max + offset_lat, identifier, \n",
    "                     horizontalalignment='left', verticalalignment='top', \n",
    "                     transform=ccrs.PlateCarree(), fontsize=14, color='black', weight='bold')\n",
    "\n",
    "        # Add title for map\n",
    "        ax_main.set_title(f\"{model} ({metric})\")\n",
    "        \n",
    "        # Add coastlines\n",
    "        ax_main.coastlines()\n",
    "        \n",
    "        \n",
    "        # Get all the hist models data together for each region without the Ensemble\n",
    "        all_hist_models_data = xr.concat([ds_dict_hist_yearly[region][models][target_var]\n",
    "                                          for models in ds_dict_hist_yearly[region] if models != f'Ensemble_{metric}'], dim='models')\n",
    "         # Calculate percentiles\n",
    "        percentile_10_hist = all_hist_models_data.quantile(0.1, dim='models')\n",
    "        percentile_25_hist = all_hist_models_data.quantile(0.25, dim='models')\n",
    "        percentile_75_hist = all_hist_models_data.quantile(0.75, dim='models')\n",
    "        percentile_90_hist = all_hist_models_data.quantile(0.9, dim='models')\n",
    "        \n",
    "        # Get all the ssp370 models data together for each region without the ensemble\n",
    "        all_ssp370_models_data = xr.concat([ds_dict_ssp370_yearly[region][models][target_var]\n",
    "                                          for models in ds_dict_ssp370_yearly[region] if models != f'Ensemble_{metric}'], dim='models')\n",
    "        \n",
    "        # Calculate percentiles\n",
    "        percentile_10_ssp370 = all_ssp370_models_data.quantile(0.1, dim='models')\n",
    "        percentile_25_ssp370 = all_ssp370_models_data.quantile(0.25, dim='models')\n",
    "        percentile_75_ssp370 = all_ssp370_models_data.quantile(0.75, dim='models')\n",
    "        percentile_90_ssp370 = all_ssp370_models_data.quantile(0.9, dim='models')\n",
    "\n",
    "        \n",
    "        # Plot the regional data at the specified location\n",
    "        ax2 = plt.axes(line_plot_locations[region])\n",
    "        \n",
    "        # Add identifier to subplots\n",
    "        ax2.text(0.05, 0.95, identifier,  # Position is 10% from the left and 10% from the top\n",
    "                 horizontalalignment='left', verticalalignment='top', \n",
    "                 transform=ax2.transAxes, fontsize=14, color='black', weight='bold')\n",
    "        \n",
    "        # Add shading for the 10th and 90th percentiles\n",
    "        ax2.fill_between(ds_dict_hist_yearly[region][model][target_var].year,\n",
    "                         percentile_10_hist,\n",
    "                         percentile_90_hist,\n",
    "                         color='blue', alpha=0.1)\n",
    "        \n",
    "        # Add shading for the 25th and 75th percentiles\n",
    "        ax2.fill_between(ds_dict_hist_yearly[region][model][target_var].year,\n",
    "                         percentile_25_hist,\n",
    "                         percentile_75_hist,\n",
    "                         color='blue', alpha=0.2)\n",
    "        \n",
    "        # Add shading for the 10th and 90th percentiles\n",
    "        ax2.fill_between(ds_dict_ssp370_yearly[region][model][target_var].year,\n",
    "                         percentile_10_ssp370,\n",
    "                         percentile_90_ssp370,\n",
    "                         color='orange', alpha=0.1)\n",
    "        \n",
    "        # Add shading for the 25th and 75th percentiles\n",
    "        ax2.fill_between(ds_dict_ssp370_yearly[region][model][target_var].year,\n",
    "                         percentile_25_ssp370,\n",
    "                         percentile_75_ssp370,\n",
    "                         color='orange', alpha=0.2)\n",
    "        \n",
    "        # Plot the lines\n",
    "        ax2.plot(ds_dict_hist_yearly[region][model][target_var].year,\n",
    "                 ds_dict_hist_yearly[region][model][target_var],\n",
    "                 label='Historical')\n",
    "        ax2.plot(ds_dict_ssp370_yearly[region][model][target_var].year,\n",
    "                 ds_dict_ssp370_yearly[region][model][target_var],\n",
    "                 label='SSP370')\n",
    "        \n",
    "        ax2.set_title(region)\n",
    "        ax2.set_ylabel(f\"{ds_dict_metric[f'Ensemble_{metric}'][target_var].units}\")#, fontsize=8)\n",
    "\n",
    "        \n",
    "    # Get line handles and labels from your plot\n",
    "    line_handles, line_labels = ax2.get_legend_handles_labels()\n",
    "    \n",
    "    # Add line handles and labels to your custom handles and labels\n",
    "    line_handles += legend_handles\n",
    "    line_labels.extend(legend_labels)\n",
    "    \n",
    "    # Add legend for subplots, use legend_handles and legend_labels instead of getting them from the last plot\n",
    "    fig.legend(handles=line_handles, labels=line_labels, loc='center', bbox_to_anchor=(0.6, 0.1), ncol=2)\n",
    "\n",
    "    # Move the colorbar to the bottom\n",
    "    cbar_ax = fig.add_axes([0.475, 0.23, 0.25, 0.02]) #left, bottom, width, height\n",
    "    cbar = fig.colorbar(img, cax=cbar_ax, orientation='horizontal')\n",
    "    \n",
    "    # Add a label to the colorbar\n",
    "    cbar.set_label(f\"Relative Change (SSP370-historical) of {long_name[ds_dict_metric[f'Ensemble_{metric}'][target_var].long_name]} [{ds_dict_metric[f'Ensemble_{metric}'][target_var].units_rel}]\", fontsize=12)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Safe figure\n",
    "    if save_fig:\n",
    "        savepath = os.path.join('../..', 'results', 'CMIP6', 'ssp370-historical', 'time-space', metric, f'var_line_plots_and_anomaly_of_{metric}')\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        filename = f'{model}.{target_var}.line_plot.relative_change.png'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        fig.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8d544-b795-4d22-bb1c-e38ad2c24ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute WUE\n",
    "#for name, ds in ds_dict_change_rel.items():\n",
    "#    ds_dict_change_rel[name]['WUE'] =  ds['gpp']/ds['tran']\n",
    "#    ds_dict_change_rel[name]['WUE'].attrs = {'long_name': 'Water Use Efficiency',\n",
    "#                                                'units': '',\n",
    "#                                                'units_rel': '%'\n",
    "#                                                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716c104-7d10-4481-9e5d-e470586cd146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in ds_dict_change_rel.keys():\n",
    "    for var in ['WUE']:\n",
    "        plot_data_with_regions(ds_dict_change_rel, ds_dict_hist_yearly, ds_dict_ssp370_yearly, regions, model=name, metric='median', target_var=var, save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0f5a1-263e-4b1f-9f9d-5b49cd0ea42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_with_regions(ds_dict_change_rel, ds_dict_hist_yearly, ds_dict_ssp370_yearly, regions, model='TaiESM1', metric='median', target_var='lai', save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d0b48-e951-4e38-a856-61a378c2d2dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Plot Variable Change Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0784e01-2a55-4b2c-8519-4da21f777947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Absolute change\n",
    "#ds_dict_change_abs = compute_change(ds_dict_hist_mean, ds_dict_ssp370_mean)\n",
    "\n",
    "# Relative change\n",
    "ds_dict_change_rel = compute_change(ds_dict_hist_median, ds_dict_ssp370_median, relative_change=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f7d7e-f03b-40fc-be3f-7afd90af387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Plot Change ==========\n",
    "plot_mean_change_map(ds_dict_change_rel, 'lmrso_2m', metric='median', cmap='BrBG', save_fig=True, file_format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260bd1f-349e-455c-b50b-4edfe085a7ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Plot Correlation Change Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4344d-4203-4772-b222-b6cd358fcdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Compute Correlations ========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe1fb6-f8c9-4cd5-81c5-c867c1feed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_corr = compute_correlation_coefficients(ds_dict_hist, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ea77e-4541-42c4-956c-64a368ec0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_ssp370_corr = compute_correlation_coefficients(ds_dict_ssp370, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b9bb4-0b22-4eb5-8df6-bd437220135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Compute Change ========\n",
    "ds_dict_corr_change = compute_change_corr(ds_dict_hist_corr, ds_dict_ssp370_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec26d9-42d3-49d5-80ba-685bbc7eaa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Create unique variable pairs ========\n",
    "variable_pairs = [(variables[i], variables[j]) for i in range(len(variables)) for j in range(i+1, len(variables))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e84fdb-70c6-4aaa-8cc9-21ccd68d20e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======== Create Plots for all variable pairs\n",
    "for var in variable_pairs:\n",
    "    corr_maps(ds_dict_corr_change, f'{var[0]} x {var[0+1]}', 'coolwarm', save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8ce99-bf88-4358-9d82-f646b35ab32a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Plot Correlation Change Maps for Ensemble mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd0692-cec1-4b1d-b501-eebc9b05b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_corr_change_plot(ds_dict_corr_change, 'mrro', full_var_names_and_unit, 'coolwarm', save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3354582-dd2c-420d-82ae-ca2753500dfa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Regionmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c67c08-863d-41a2-8863-8ba91250667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regionmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3e199-c2f8-41e0-8ed2-ea6730ac29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_regions = regionmask.defined_regions.ar6.land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40447a6-197b-4bb1-90a2-c01fd5186bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_regions.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d9e504-9f04-4125-9991-992fef246654",
   "metadata": {},
   "outputs": [],
   "source": [
    "tas = filtered_ds.tas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c74a8-1cfa-491d-b8ec-78d248504ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_2D = land_regions.mask_3D(tas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fae44-4861-472f-a560-158423105e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85e29e-c8cc-40ef-854d-50a148c55dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = ccrs.Robinson()\n",
    "f, ax = plt.subplots(subplot_kw=dict(projection=proj))\n",
    "\n",
    "h = mask_2D.plot.pcolormesh(ax=ax, transform=ccrs.PlateCarree(), add_colorbar=False)\n",
    "\n",
    "ax.coastlines()\n",
    "\n",
    "land_regions.plot_regions(line_kws=dict(lw=0.5), add_label=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd33133-4768-481d-9479-7f8f3eebe400",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare Q/Tran and Pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d55a74f-324a-44cc-b668-bc6f73f51641",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb5d43-0dd7-4131-af98-f8731c21278a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_dict = ds_dict_hist_period_metric.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8cfe3-4fdb-4094-b5b8-2975234d3467",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in ds_dict.items():\n",
    "    ds['mrro-pr'] = ds['mrro'] - ds['pr']\n",
    "    \n",
    "    ds['mrro-pr'].attrs = {\n",
    "    \"units\": \"mm/day\",\n",
    "    \"long_name\": \"Runoff - Precipitation\",\n",
    "    }\n",
    "    \n",
    "    ds['tran-pr'] = ds['tran'] - ds['pr']\n",
    "    ds['tran-pr'].attrs = {\n",
    "    \"units\": \"mm/day\",\n",
    "    \"long_name\": \"Transpiration - Precipitation\",\n",
    "    }\n",
    "    \n",
    "    ds['mrro+tran-pr'] = (ds['mrro'] + ds['tran']) - ds['pr']\n",
    "    ds['mrro+tran-pr'].attrs = {\n",
    "    \"units\": \"mm/day\",\n",
    "    \"long_name\": \"(Runoff + Transpiration) - Precipitation\",\n",
    "    }\n",
    "    \n",
    "    ds['evspsbl-pr'] = ds['evspsbl'] - ds['pr']\n",
    "    ds['evspsbl-pr'].attrs = {\n",
    "    \"units\": \"mm/day\",\n",
    "    \"long_name\": \"ET - Precipitation\",\n",
    "    }\n",
    "\n",
    "    ds['mrro_negative'] = xr.where(ds['mrro'] < 0, ds['mrro'], np.nan)\n",
    "    ds['mrro_negative'].attrs = {\n",
    "    \"units\": \"mm/day\",\n",
    "    \"long_name\": \"Negative Runoff\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea491f08-97f4-45bc-bcc4-78180cfccf21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_map_statistic(ds_dict, variable, n_cols=4, cbar_min=0, cbar_max=0.65, cmap='viridis', save_fig=False, log_scale=False, file_format='png'):\n",
    "    \"\"\"\n",
    "    Plots a map of the specified statistic of the given variable for each dataset in the dictionary.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset\n",
    "            and each value is the dataset itself.\n",
    "        variable (str): The name of the variable to plot.\n",
    "        n_cols (int): The number of columns for the subplots. Default is 4.\n",
    "        cbar_min (float): A value to set vmin by multiplying with the variables minimum value across the dataset. Default is 0.\n",
    "        cbar_max (float): A value to set vmax by multiplying with the variables maximum value across the dataset. Default is 0.75.\n",
    "        cmap (str): The name of the colormap to use for the plot. Default is 'viridis'.\n",
    "        save_fig (bool): If True, save the figure to a file. Default is False.\n",
    "        file_format (str): The format of the saved figure. Default is 'png'.\n",
    "\n",
    "    Returns:\n",
    "        str: The file path where the figure was saved.\n",
    "    \"\"\"\n",
    "    # Check arguments and get info\n",
    "    var_long_name, period, unit, statistic_dim, statistic, experiment_id, titles, frequency = check_args_and_get_info(ds_dict, variable) \n",
    "\n",
    "    # Calculate vmin and vmax\n",
    "    temp_dim_ds = xr.concat([ds[variable] for ds in ds_dict.values() if variable in ds], dim='temp_dim', coords='minimal')\n",
    "    vmin = round(float(temp_dim_ds.min())) * cbar_min\n",
    "    vmax = round(float(temp_dim_ds.max()), -int(math.floor(math.log10(abs(float(temp_dim_ds.max())))))) * cbar_max\n",
    "\n",
    "    # Number of datasets that contain the variable\n",
    "    n_datasets_with_var = sum([1 for ds in ds_dict.values() if variable in ds])\n",
    "\n",
    "    # Compute the required number of rows for the subplots\n",
    "    n_rows = np.ceil(n_datasets_with_var / n_cols).astype(int)\n",
    "\n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(12 * n_cols, 6.5 * n_rows))\n",
    "\n",
    "    # Create a GridSpec with specified width and height space\n",
    "    gs = plt.GridSpec(n_rows, n_cols, figure=fig, wspace=0.1, hspace=0.1)\n",
    "\n",
    "    axes = []\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            ax = fig.add_subplot(gs[i, j], projection=ccrs.Robinson())\n",
    "            axes.append(ax)\n",
    "\n",
    "    subplot_counter = 0\n",
    "    # Loop over datasets and plot the requested statistic\n",
    "    for name, ds in ds_dict.items():\n",
    "        if variable not in ds:\n",
    "            print(f\"Variable '{variable}' not found in dataset '{name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        # plot the variable in subplot\n",
    "        ax = axes[subplot_counter]\n",
    "        data_to_plot = ds[variable]\n",
    "        \n",
    "        # Add condition for log_scale colorbar\n",
    "        if log_scale:\n",
    "            norm = colors.LogNorm(vmin=vmin, vmax=vmax)\n",
    "        else:\n",
    "            norm = None\n",
    "            \n",
    "        im = data_to_plot.plot(ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, extend='max', transform=ccrs.PlateCarree(), add_colorbar=False, norm=norm)\n",
    "        ax.set_title(name, fontsize=18)\n",
    "        ax.coastlines()\n",
    "\n",
    "        subplot_counter += 1\n",
    "\n",
    "    # Add a common colorbar at the bottom of the plots\n",
    "    cbar = fig.colorbar(im, ax=axes, orientation='vertical', fraction=0.1, pad=0.03, aspect=20, shrink=0.5)\n",
    "    # Set tick size\n",
    "    cbar.ax.tick_params(labelsize=20)  # Adjust size as needed\n",
    "    # Set colorbar label\n",
    "    cbar.set_label(f\"{var_long_name} [{unit}]\", size=26)  # Adjust size as needed\n",
    "    \n",
    "    # Set figure title \n",
    "    fig.suptitle(f\"{titles[statistic_dim]} {titles[statistic]} of {frequency} {var_long_name} ({period})\", fontsize=30, y=0.95)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    # Save figure\n",
    "    if save_fig:\n",
    "        savepath = os.path.join('../..', 'results', 'CMIP6', experiment_id, statistic_dim, statistic, 'maps')\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        filename = f'{statistic_dim}.{statistic}.{period}.{variable}.{ds.experiment_id}.{file_format}'\n",
    "        filepath = os.path.join(savepath, filename)\n",
    "        fig.savefig(filepath, dpi=300)\n",
    "    else:\n",
    "        filepath = 'Figure not saved. If you want to save the figure add save_fig=True to the function call'\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2813a20-4d2b-4f3b-971b-be0f132b23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Define the colors for the colormap\n",
    "colors = [\n",
    "    (0.9, 0.9, 0.9),  # Light gray for values below 0\n",
    "    (0.95, 0.95, 0.5),  # Light yellow\n",
    "    (1.0, 0.7, 0.2),  # Orange\n",
    "    (0.8, 0.2, 0.2),  # Red\n",
    "    (0.6, 0.0, 0.4)  # Purple\n",
    "]\n",
    "\n",
    "# Define the color positions (ranging from 0 to 1)\n",
    "positions = [-1, 0, 0.25, 0.75, 1]\n",
    "\n",
    "# Create the colormap using LinearSegmentedColormap\n",
    "cmap = LinearSegmentedColormap.from_list('custom_colormap', colors, N=256)\n",
    "\n",
    "# Plot the colormap\n",
    "plt.imshow([[i for i in range(256)]], cmap=cmap, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdef8a1-9f49-4890-88f2-2e6445703d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map_statistic(ds_dict, 'mrro_negative', n_cols=4, cbar_min=1, cbar_max=0, cmap='viridis', save_fig=True, log_scale=False, file_format='png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d881e-2448-4f21-94cc-01d00c9591c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb060e-3de8-4a25-b8bf-d6e033ad29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ds = ds_dict_hist_period_metric['Ensemble median'].where((ds_dict_hist_period_metric['Ensemble median']['nbwfp'] <= 1) & (ds_dict_hist_period_metric['Ensemble median']['nbwfp'] >= -1), drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee680f-ac2c-41b9-8605-c0973fe9b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_2 = ds_dict_hist_period_metric['Ensemble median'].where((ds_dict_hist_period_metric['Ensemble median']['nbwfp'] >= 1) & (ds_dict_hist_period_metric['Ensemble median']['nbwfp'] <= -1), drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ac276-ae04-4d4e-9308-405ad7f931a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict_hist_period_metric['Ensemble median'].nbwfp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aff65a-51bb-46aa-b2e6-eed9cb69665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_2['pr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36026a50-343a-453d-a51c-e12378ae77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ds['nbwfp'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5006f8c-a4e1-4d9a-8bd3-dca024ad2e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = xr.corr(filtered_ds['pr'], filtered_ds['nbwfp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ec3a5-6583-44cf-8a66-a0349205d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(filtered_ds['pr'], filtered_ds['nbwfp'], alpha=0.5)\n",
    "ax.text(0.05, 0.95, f'Correlation: {corr:.2f}', transform=ax.transAxes, verticalalignment='top')\n",
    "plt.xlabel('pr')\n",
    "plt.ylabel('nbwfp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083e9ff-cc5f-4bd9-b100-9336d62c85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ds.nbwfp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a9157-01c9-4306-944b-9a81cb755534",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Analyse BWD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c0f56-133c-4ad7-aac2-7cb78a1e455c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8890313-1608-4f99-be7e-57a4c1ec0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "colors = [\"green\", \"lightgreen\", \"grey\", \"lightblue\", \"blue\"]\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"\", colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48501fcc-465f-435f-a8c7-09b75db7a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"green\", \"lightgreen\", \"lightblue\", \"blue\"]\n",
    "cmap_1 = mcolors.LinearSegmentedColormap.from_list(\"\", colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97050730-0d2f-4057-b99a-5233e33664bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Do computations for different periods and their change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960d58c-8660-4132-a277-7a16d6dd48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISTORICAL\n",
    "ds = ds_dict_hist_period_metric['Ensemble mean']\n",
    "ds['NBWF'] = ds['mrro'] - ds['tran']\n",
    "ds['NBWF/P'] = (ds['mrro'] - ds['tran'])/ds['pr']\n",
    "ds['BWFB'] = ds['mrro'].where(ds['mrro']>10e-6)/ds['tran'].where(ds['tran']>10e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be76001-0a16-429b-b35b-48ca0d619190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUTURE \n",
    "ds_ssp = ds_dict_ssp370_period_metric['Ensemble mean']\n",
    "ds_ssp['NBWF'] = ds_ssp['mrro'] - ds_ssp['tran']\n",
    "ds_ssp['NBWF/P'] = (ds_ssp['mrro'] - ds_ssp['tran'])/ds_ssp['pr']\n",
    "ds_ssp['BWFB'] = ds_ssp['mrro'].where(ds_ssp['mrro']>10e-6)/ds_ssp['tran'].where(ds_ssp['tran']>10e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af602d8e-fb1e-490d-a08b-40c2c44fcf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABSOLUTE CHANGE\n",
    "ds_change = ds_ssp - ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56054ba-3381-4f58-a79b-589b8f7c8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELATIVE CHANGE\n",
    "ds_change_rel = ((ds_ssp - ds)/ds) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb4405a-99de-45c8-a2e6-1fdfba6698d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Net Blue Water Flux (NBWF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cf5d6-bd06-4e2c-a6c8-57237c43e63c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds['NBWF'].plot(cmap=cmap, vmin=-0.5, vmax=0.5)\n",
    "fig.figure.text(0.5, 0.9, \"Historical Net Blue Water Flux (NBWF)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141318d2-5f35-491a-8216-613402ae1cba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_ssp['NBWF'].plot(cmap=cmap, vmin=-0.5, vmax=0.5)\n",
    "fig.figure.text(0.5, 0.9, \"Future Net Blue Water Flux (NBWF)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd680cd-319e-4f2e-8b46-8fd40de2fa29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change['NBWF'].plot(cmap=cmap, vmin=-0.5, vmax=0.5)\n",
    "fig.figure.text(0.5, 0.9, \"Net Blue Water Flux Absolute Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c7428-d4fb-4177-b3b9-f2798fe8532d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change_rel['NBWF'].plot(cmap=cmap, vmin=-100, vmax=100)\n",
    "fig.figure.text(0.5, 0.9, \"Net Blue Water Flux Relative Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2002315a-dd76-4986-9162-a694ac906033",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Historical:\")\n",
    "print(f\"Global Mean of Historical NBWF: {ds['NBWF'].mean().values} mm/day\")\n",
    "print(f\"Global Median of Historical NBWF: {ds['NBWF'].median().values} mm/day\")\n",
    "print(f\"Global Standard Deviation of Historical NBWF: {ds['NBWF'].std().values} mm/day\")\n",
    "print(f\"Global Minimum of Historical NBWF: {ds['NBWF'].min().values} mm/day\")\n",
    "print(f\"Global Maximum of Historical NBWF: {ds['NBWF'].max().values} mm/day\\n\")\n",
    "\n",
    "print(\"SSP370:\")\n",
    "print(f\"Global Mean of SSP370 NBWF: {ds_ssp['NBWF'].mean().values} mm/day\")\n",
    "print(f\"Global Median of SSP370 NBWF: {ds_ssp['NBWF'].median().values} mm/day\")\n",
    "print(f\"Global Standard Deviation of SSP370 NBWF: {ds_ssp['NBWF'].std().values} mm/day\")\n",
    "print(f\"Global Minimum of SSP370 NBWF: {ds_ssp['NBWF'].min().values} mm/day\")\n",
    "print(f\"Global Maximum of SSP370 NBWF: {ds_ssp['NBWF'].max().values} mm/day\\n\")\n",
    "\n",
    "print(\"Absolute Change:\")\n",
    "print(f\"Global Mean Absolute Change of NBWF: {ds_change['NBWF'].mean().values} mm/day\")\n",
    "print(f\"Global Median Absolute Change of NBWF: {ds_change['NBWF'].median().values} mm/day\")\n",
    "print(f\"Global Standard Deviation of NBWF Absolute Change: {ds_change['NBWF'].std().values} mm/day\")\n",
    "print(f\"Global Minimum of NBWF Absolute Change: {ds_change['NBWF'].min().values} mm/day\")\n",
    "print(f\"Global Maximum of NBWF Absolute Change: {ds_change['NBWF'].max().values} mm/day\\n\")\n",
    "\n",
    "print(\"Relative Change:\")\n",
    "print(f\"Global Mean Relative Change of NBWF: {ds_change_rel['NBWF'].mean().values} %\")\n",
    "print(f\"Global Median Relative Change of NBWF: {ds_change_rel['NBWF'].median().values} %\")\n",
    "print(f\"Global Standard Deviation of NBWF Relative Change: {ds_change_rel['NBWF'].std().values} %\")\n",
    "print(f\"Global Minimum of NBWF Relative Change: {ds_change_rel['NBWF'].min().values} %\")\n",
    "print(f\"Global Maximum of NBWF Relative Change: {ds_change_rel['NBWF'].max().values} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e0fcfa-26e1-4c92-ae76-17c1b289647c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Net Blue Water Flux / Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661df8af-0797-4521-aa14-60952a190a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds['NBWF/P'].plot(cmap=cmap, vmin=-0.5, vmax=0.5)\n",
    "fig.figure.text(0.5, 0.9, \"Historical Net Blue Water Flux / Precipitation (NBWF/P)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ff1db-279e-451e-9bd8-25142ba9efcb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_ssp['NBWF/P'].plot(cmap=cmap, vmin=-0.5, vmax=0.5)\n",
    "fig.figure.text(0.5, 0.9, \"SSP370 Net Blue Water Flux / Precipitation (NBWF/P)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4c73a-a915-4371-97ef-f83f6b261f36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change['NBWF/P'].plot(cmap=cmap, vmin=-0.2, vmax=0.2)\n",
    "fig.figure.text(0.5, 0.9, \"Net Blue Water Flux / Precipitation (NBWF/P) Absolute Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560acb6-149e-4e40-812c-dbbcd6f13a7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change_rel['NBWF/P'].plot(cmap=cmap, vmin=-100, vmax=100)\n",
    "fig.figure.text(0.5, 0.9, \"Net Blue Water Flux / Precipitation (NBWF/P) Relative Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd928b-abca-4d27-bd65-fd9e1874e2f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Historical:\")\n",
    "print(f\"Global Mean of Historical BWFB: {ds['BWFB'].mean().values}\")\n",
    "print(f\"Global Median of Historical BWFB: {ds['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of Historical BWFB: {ds['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of Historical BWFB: {ds['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of Historical BWFB: {ds['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"SSP370:\")\n",
    "print(f\"Global Mean of SSP370 BWFB: {ds_ssp['BWFB'].mean().values}\")\n",
    "print(f\"Global Median of SSP370 BWFB: {ds_ssp['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of SSP370 BWFB: {ds_ssp['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of SSP370 BWFB: {ds_ssp['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of SSP370 BWFB: {ds_ssp['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"Absolute Change:\")\n",
    "print(f\"Global Mean Absolute Change of BWFB: {ds_change['BWFB'].mean().values}\")\n",
    "print(f\"Global Median Absolute Change of BWFB: {ds_change['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of BWFB Absolute Change: {ds_change['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of BWFB Absolute Change: {ds_change['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of BWFB Absolute Change: {ds_change['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"Relative Change:\")\n",
    "print(f\"Global Mean Relative Change of BWFB: {ds_change_rel['BWFB'].mean().values} %\")\n",
    "print(f\"Global Median Relative Change of BWFB: {ds_change_rel['BWFB'].median().values} %\")\n",
    "print(f\"Global Standard Deviation of BWFB Relative Change: {ds_change_rel['BWFB'].std().values} %\")\n",
    "print(f\"Global Minimum of BWFB Relative Change: {ds_change_rel['BWFB'].min().values} %\")\n",
    "print(f\"Global Maximum of BWFB Relative Change: {ds_change_rel['BWFB'].max().values} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b471ef-8c50-4cf6-9ffa-719cb5f2f89b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Blue Water Flux Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f34bf-e317-4c0b-93c0-773507af71e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds['BWFB'].plot(cmap=cmap, vmin=0, vmax=5)\n",
    "fig.figure.text(0.5, 0.9, \"Historical Blue Water Flux Balance (BWFB)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0a25d-8376-41a8-a4ad-b0c2f757f772",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_ssp['BWFB'].plot(cmap=cmap, vmin=0, vmax=5)\n",
    "fig.figure.text(0.5, 0.9, \"SSP370 Blue Water Flux Balance (BWFB)\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ba321-049e-4a59-aef7-b71fd35eba71",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change['BWFB'].plot(cmap=cmap, vmin=0, vmax=5)\n",
    "fig.figure.text(0.5, 0.9, \"Blue Water Flux Balance (BWFB) Absolute Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3f07b-faf6-42b0-b2fb-0cbae69090ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = ds_change_rel['BWFB'].plot(cmap=cmap, vmin=-100, vmax=100)\n",
    "fig.figure.text(0.5, 0.9, \"Blue Water Flux Balance (BWFB) Relative Change\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7673743-a6ff-4b76-a0b7-ca93266acd99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Historical:\")\n",
    "print(f\"Global Mean of Historical BWFB: {ds['BWFB'].mean().values}\")\n",
    "print(f\"Global Median of Historical BWFB: {ds['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of Historical BWFB: {ds['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of Historical BWFB: {ds['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of Historical BWFB: {ds['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"SSP370:\")\n",
    "print(f\"Global Mean of SSP370 BWFB: {ds_ssp['BWFB'].mean().values}\")\n",
    "print(f\"Global Median of SSP370 BWFB: {ds_ssp['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of SSP370 BWFB: {ds_ssp['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of SSP370 BWFB: {ds_ssp['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of SSP370 BWFB: {ds_ssp['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"Absolute Change:\")\n",
    "print(f\"Global Mean Absolute Change of BWFB: {ds_change['BWFB'].mean().values}\")\n",
    "print(f\"Global Median Absolute Change of BWFB: {ds_change['BWFB'].median().values}\")\n",
    "print(f\"Global Standard Deviation of BWFB Absolute Change: {ds_change['BWFB'].std().values}\")\n",
    "print(f\"Global Minimum of BWFB Absolute Change: {ds_change['BWFB'].min().values}\")\n",
    "print(f\"Global Maximum of BWFB Absolute Change: {ds_change['BWFB'].max().values}\\n\")\n",
    "\n",
    "print(\"Relative Change:\")\n",
    "print(f\"Global Mean Relative Change of BWFB: {ds_change_rel['BWFB'].mean().values} %\")\n",
    "print(f\"Global Median Relative Change of BWFB: {ds_change_rel['BWFB'].median().values} %\")\n",
    "print(f\"Global Standard Deviation of BWFB Relative Change: {ds_change_rel['BWFB'].std().values} %\")\n",
    "print(f\"Global Minimum of BWFB Relative Change: {ds_change_rel['BWFB'].min().values} %\")\n",
    "print(f\"Global Maximum of BWFB Relative Change: {ds_change_rel['BWFB'].max().values} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "mypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
