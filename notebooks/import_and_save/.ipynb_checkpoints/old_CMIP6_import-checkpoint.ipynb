{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05fcb352-aecb-438c-9c68-b8bed66b5882",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CMIP6 Import Data\n",
    "\n",
    "**Following steps are included in this script:**\n",
    "\n",
    "1. Open dkrz cataloge, save CMIP6 catalog and browse it\n",
    "2. Load important hydroecological data for each model separately as kernel dies otherwise\n",
    "3. Merge data into one file for each model\n",
    "4. Load already saved netcdf file and update with new variables\n",
    "4. Save data to netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0108914-eaef-4832-8fbd-2dc16ae7a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Packages ==========\n",
    "import xarray as xr\n",
    "import intake\n",
    "import dask\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29bae7d0-2612-4355-bac6-86953dbaf211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Run all functions at the bottom ========"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b160a22-6e50-449a-b081-3ab60b4e2e22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Open dkrz catalog and save CMIP6 catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5f2fc7-2cb2-429e-a555-858a5fdfaccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----Open dkrz catalog----\n",
    "cat = intake.open_catalog([\"https://dkrz.de/s/intake\"])\n",
    "\n",
    "# ----Save CMIP6 catalog----\n",
    "cat_cmip6 = cat.dkrz_cmip6_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3529ea-e19f-495a-8702-d63cf783ed70",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Load important hydroecological data\n",
    "\n",
    "\n",
    "#### Attributes\n",
    "| 'source_id' | 'member_id' | 'variable_id' |\n",
    "|:-----------:|:-----------:|:-----------:|\n",
    "| 'BCC-CSM2-MR', 'CESM2', 'CNRM-CM6-1-HR', 'NorESM2-MM', 'SAM0-UNICON', 'TaiESM1'  | 'r1i1p1f1', 'r1i1p1f2' (for CNRM)   | 'pr', 'mrro', 'mrros', 'evspsbl', 'evspsblsoi', 'evspsblveg', 'tran', 'mrso', 'mrsos', 'mrsol', 'huss', 'hurs', 'lai', 'gpp', 'npp' | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad33bbd-9b12-4b90-b4c8-6e3be7cc8264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----Define attributes----\n",
    "attrs = dict(\n",
    "    \n",
    "    experiment_id=\"historical\", #ssp126 historical\n",
    "  #  member_id=['r1i1p1f1' ,'r1i1p1f2', 'r1i1p2f1', 'r1i1p1f3'], #'r1i1p1f2'],,'r1i1p1f2', 'r1i1p2f1', 'r1i1p1f3'\n",
    "   source_id=['CNRM-ESM2-1'], #'TaiESM1', 'AWI-ESM-1-1-LR', 'BCC-CSM2-MR', 'BCC-ESM1', 'CanESM5', 'CNRM-CM6-1', 'CNRM-CM6-1-HR', 'CNRM-ESM2-1','UKESM1-0-LL', 'CESM2', 'CESM2-FV2', 'CESM2-WACCM', 'NorESM2-MM'], \n",
    "    # table_id =['Amon', 'Lmon', 'Emon'], #'Amon', 'Lmon', 'Emon'\n",
    "    variable_id=[\n",
    "              #  'tas',\n",
    "              #  'ps', #surface pressure\n",
    "          #       'pr', # CESM2 has problems loading pr with other Amon data\n",
    "          #      'mrro', \n",
    "               # 'mrros', \n",
    "          #      'evspsbl', \n",
    "               # 'evspsblsoi', \n",
    "               # 'evspsblveg', \n",
    "              #  'tran', \n",
    "               # 'mrso', \n",
    "               # 'mrsos', \n",
    "              #  'mrsol', \n",
    "              #  'huss', \n",
    "               #  'hurs',  # TaiESM1 has hurs only in daily resolution\n",
    "              #  'lai', \n",
    "              # 'gpp', \n",
    "              #  'npp'\n",
    "                'tsl'\n",
    "    #     'sftlf' # land area fraction\n",
    "    ]\n",
    "  #  ,version = ['v20200623', 'v20200624'] #TaiESM1 has two versions for gpp. I select the newer version.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bbcd55-8792-461c-8fba-f0e2c4fc6db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----Save data selection----\n",
    "selection = cat_cmip6.search(**attrs)\n",
    "#selection = cat_cmip6.search(require_all_on=[\"source_id\"], **attrs) #require_all_on defines that source ID must include all important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4484e4-6b3a-45d4-bdb1-646cf255f98d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>variable_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>institution_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CNRM-CERFACS</th>\n",
       "      <th>CNRM-ESM2-1</th>\n",
       "      <th>historical</th>\n",
       "      <td>[tsl]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         variable_id\n",
       "institution_id source_id   experiment_id            \n",
       "CNRM-CERFACS   CNRM-ESM2-1 historical          [tsl]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----Set properties of pandas tables ----\n",
    "pd.set_option('display.max_colwidth', None) #pd.reset_option('display.max_colwidth')\n",
    "pd.set_option('display.max_rows', None) #pd.reset_option('display.max_rows', None)\n",
    "\n",
    "# ----Print table with different attributes of selected data----\n",
    "selection.df.groupby(\n",
    "    [\n",
    "       # \"grid_label\",\n",
    "        \"institution_id\",\n",
    "        \"source_id\",\n",
    "     #   \"version\",\n",
    "    #    'member_id',\n",
    "       # \"time_range\",\n",
    "        'experiment_id',\n",
    "       \n",
    "      #   'table_id'\n",
    "    #    'variable_id'\n",
    "    ]\n",
    ")['variable_id'].unique().apply(list).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6a12c-17fc-460c-852f-88083b7ed7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    }
   ],
   "source": [
    "# ========= Load selection in dictionary ========== (I always have to run the 'define attrs' cell again...)\n",
    "\n",
    "with dask.config.set(**{\"use_cftime\": True, \"decode_times\": True, 'consolidated': True}):\n",
    "    ds_dict = selection.to_dataset_dict(preprocess=pre_preprocessing) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46643743-a9f3-4713-b132-b80664814dad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Merge datasets into one file for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe15e6-9e72-451f-83ff-2e15ce7f18c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========== Drop redundant coordinates and variables ================\n",
    "\n",
    "# Define redundant coordinates and variables\n",
    "drop_list = ['member_id','nbnd', 'bnds', 'height', 'depth', 'lat_bnds', 'lon_bnds', 'time_bnds', 'time_bounds', 'depth_bnds', 'sdepth_bounds', 'depth_bounds', 'hist_interval', 'axis_nbounds'] #depth is not dropped for datasets with variable mrsol\n",
    "\n",
    "# Drop redundant coordinates and variables\n",
    "ds_dict = drop_redundant(ds_dict, drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbdb26-1e64-498c-8dd0-c2b2e31faca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa30eac-eb85-4e7e-8429-9c6629c2c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict[list(ds_dict.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26d5c7-2c86-41f0-b5a8-8a0aba2e91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Merge data if loading all data of one model is not possible ==============\n",
    "# Name of second dictionary ds_dict_\n",
    "\n",
    "# Only use this command if loading at once is not possible\n",
    "\n",
    "#ds_dict[f\"{list(ds_dict_.keys())[0]}_\"]=ds_dict_[list(ds_dict_.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56adc44-b6c3-4094-adb7-6542ffd1f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Merge datasets with different table_id and same source_id ================\n",
    "\n",
    "ds_dict = merge_source_id_data(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88579bfa-4e46-41f0-a2c2-99394ef1cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Compute montly mean for daily datasets (optional: merge with rest of the datasets) =============\n",
    "\n",
    "#ds_dict_monthly = daily_to_monthly(ds_dict) # optionally include ,ds_dict_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eabf7bf-8463-4c01-b58f-46356c36eea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========== Check dictionary for consistency =============\n",
    "ds_dict[list(ds_dict.keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e9618-8715-41c3-bc01-8c1ab53d1665",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Load already saved netcdf file and update with new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3799a7-108f-406a-ba12-6366b0656185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Load model which needs to be updated ==============\n",
    "experiment_id = 'historical'\n",
    "source_id = ['BCC-CSM2-MR']\n",
    "savepath = f'../../data/CMIP6/{experiment_id}/preprocessed'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# ========= Create a helper function to open the dataset ========\n",
    "def open_dataset(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    return ds\n",
    "\n",
    "# ========= Create dictionary using a dictionary comprehension and Dask =======\n",
    "ds_dict_merge, = dask.compute({model: open_dataset(os.path.join(savepath, f'CMIP.{model}.{experiment_id}.nc'))\n",
    "                        for model in source_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d24c25-0f26-43ed-850f-167187988320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========== Check dictionary for consistency =============\n",
    "\n",
    "ds_dict_merge[list(ds_dict_merge.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa9e14-ddd4-45b6-b0d6-f5108430d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Create a dictionary with the computed monthly mean and the loaded model data ==========\n",
    "\n",
    "ds_dict_all = {}\n",
    "ds_dict_all['dataset_one'] = ds_dict_merge[list(ds_dict_merge.keys())[0]]\n",
    "ds_dict_all['dataset_two'] = ds_dict_merged[list(ds_dict_merged.keys())[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c14d8-9ca0-47d8-bb37-73cae975b9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========= Merge data ======================\n",
    "\n",
    "ds_dict_all = merge_source_id_data(ds_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e191e-3a5e-47b1-bb23-c76beacb1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Check dictionary for consistency =============\n",
    "\n",
    "ds_dict_all[list(ds_dict_all.keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad37a4-0a42-40b4-8632-e1cba90922b7",
   "metadata": {},
   "source": [
    "## 5. Store netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86edd4b7-b311-4b51-82f0-472008b5fd65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder='preprocessed'\n",
    "\n",
    "for key in ds_dict_merged.keys():\n",
    "    ds_in = ds_dict_merged[key]\n",
    "    filename = f'CMIP.{ds_in.source_id}.{ds_in.experiment_id}_tsl.nc'\n",
    "    savepath = f'../data/CMIP6/{ds_in.experiment_id}/{folder}'\n",
    "    nc_out = os.path.join(savepath, filename)\n",
    "    os.makedirs(savepath, exist_ok=True) \n",
    "    if os.path.exists(nc_out):\n",
    "        os.remove(nc_out)\n",
    "        print(f\"File  with path: {nc_out} removed\")\n",
    "    # Save to netcdf file\n",
    "    with dask.config.set(scheduler='threads'):\n",
    "        ds_in.to_netcdf(nc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf36f1-63d3-48e4-a66c-d584608a9f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========== Store file and remove any former one ==========\n",
    "\n",
    "nc_out = save_file(ds_dict, folder='raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6a2e8-3e68-464a-a983-1d8bcb2c1316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Check stored file ==============\n",
    "\n",
    "xr.open_dataset(nc_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117e80a-b71f-40de-b026-628705a74ac4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b39bf81-9d53-4d7b-b509-cd75fd4d5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xmip.preprocessing import correct_lon, correct_units, parse_lon_lat_bounds, maybe_convert_bounds_to_vertex, maybe_convert_vertex_to_bounds\n",
    "\n",
    "def pre_preprocessing(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Preprocesses a CMIP6 dataset\n",
    "    \n",
    "    Parameters:\n",
    "    ds (xr.Dataset): Input dataset\n",
    "    \n",
    "    Returns:\n",
    "    xr.Dataset: Preprocessed dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def correct_coordinates(ds: xr.Dataset) -> xr.Dataset:\n",
    "        \"\"\"\n",
    "        Corrects wrongly assigned data_vars to coordinates\n",
    "\n",
    "        Parameters:\n",
    "        ds (xr.Dataset): Input dataset\n",
    "\n",
    "        Returns:\n",
    "        xr.Dataset: Dataset with corrected coordinates\n",
    "        \"\"\"\n",
    "        for co in [\"lon\", \"lat\"]:\n",
    "            if co in ds.variables:\n",
    "                ds = ds.set_coords(co)\n",
    "\n",
    "        return ds.copy(deep=True)\n",
    " \n",
    "    ds = correct_coordinates(ds)\n",
    "    ds = correct_units(ds) \n",
    "    ds = parse_lon_lat_bounds(ds)\n",
    "    ds = maybe_convert_bounds_to_vertex(ds)\n",
    "    ds = maybe_convert_vertex_to_bounds(ds)\n",
    "    return ds.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "314b346c-a5e3-429e-af14-53bb7600ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant(ds_dict, drop_list): \n",
    "    \"\"\"\n",
    "    Remove redundant coordinates and variables from datasets in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    ds_dict (dict): Dictionary containing dataset names as keys and xarray.Dataset objects as values.\n",
    "    drop_list (list): List of redundant coordinate or variable names to be removed from the datasets.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with the same keys as the input ds_dict and modified xarray.Dataset objects with redundant elements removed.\n",
    "    \"\"\"\n",
    "    for ds_name, ds_data in ds_dict.items():\n",
    "        \n",
    "        if 'sdepth' in ds_data.coords:\n",
    "            if 'depth' in ds_data.dims:\n",
    "                ds_data = ds_data.drop('depth')  \n",
    "            ds_data = ds_data.rename({'sdepth': 'depth'})\n",
    "            print(f'sdepth changed to depth for model {ds_data.source_id}')\n",
    "   \n",
    "        \n",
    "        if 'mrsol' in ds_data and 'depth' in drop_list or 'tsl' in ds_data and 'depth' in drop_list:\n",
    "            drop_list.remove('depth')\n",
    "                      \n",
    "        for coord in drop_list:\n",
    "            if coord in ds_data.coords:\n",
    "                ds_data = ds_data.drop(coord).squeeze()\n",
    "                print(f'Dropped coordinate: {coord}')\n",
    "            if coord in ds_data.variables:\n",
    "                ds_data = ds_data.drop_vars(coord).squeeze()\n",
    "                print(f'Dropped variable: {coord}')\n",
    "            # Update the dictionary with the modified dataset\n",
    "            ds_dict[ds_name] = ds_data\n",
    "    \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91459ec4-09e7-4ede-aa0a-b46ee8c49506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_coordinates(new_coords, replace_coords):\n",
    "    \"\"\"\n",
    "    Helper funtion to replace coordinates before merging.\n",
    "    \n",
    "    Args:\n",
    "        new_coords (xr dataset): A dictionary of xarray datasets which gives the new coordinates.\n",
    "        replace_coords (xr dataset): A dictionary of xarray datasets which coordinates will be replaced.\n",
    "\n",
    "    Returns:\n",
    "        replace_coords (xr dataset): The replace dictionary with the new coordinates copied from new_coords.\n",
    "    \"\"\"\n",
    "    \n",
    "    for coord in ['lon', 'lat', 'time']:\n",
    "        if not new_coords[coord].equals(replace_coords[coord]):\n",
    "            replace_coords[coord] = new_coords[coord]\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return replace_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72ae9cc4-ae2e-4100-ba62-12f9ef4bfeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_source_id_data(ds_dict):\n",
    "    \"\"\"\n",
    "    Merge datasets with the same source_id (name of the CMIP6 model) as CMIP6 data is stored in different table id's. This function is mainly used to merge two \n",
    "    different xarray datasets for 'table_id' Amon and Lmon into a single xarray dataset as this makes future investigations easier. Other table_id's\n",
    "    can also be merged; however, be careful when the same variable exists in both datasets.\n",
    "\n",
    "    Args:\n",
    "        ds_dict (dict): A dictionary of xarray datasets, where each key is the name of the dataset \n",
    "                        and each value is the dataset itself.\n",
    "\n",
    "    Returns:\n",
    "        dict: A merged dictionary with a single dataset for each CMIP6 model/source_id.\n",
    "    \"\"\"\n",
    "    \n",
    "    merged_dict = {}\n",
    "    for dataset_name, dataset in ds_dict.items():\n",
    "        source_id = dataset.attrs['source_id']\n",
    "        table_id = dataset.attrs['table_id']\n",
    "        print(f\"Merging dataset '{dataset_name}' with source_id '{source_id}' and table_id '{table_id}'...\")\n",
    "       \n",
    "        if source_id in merged_dict:\n",
    "            if source_id == merged_dict[source_id].attrs['source_id'] and table_id != merged_dict[source_id].attrs['table_id']:\n",
    "                merg_model_name = merged_dict[source_id].attrs['intake_esm_dataset_key']\n",
    "                merg_model_table_id = merged_dict[source_id].attrs['table_id']\n",
    "                 \n",
    "                # Replace coordinates lat, lon, time of dataset only when different to datasets in merged_dict\n",
    "                dataset = replace_coordinates(merged_dict[source_id], dataset)\n",
    "\n",
    "                # Merge data    \n",
    "                with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                    merged_dict[source_id] = xr.merge([merged_dict[source_id], dataset])\n",
    "\n",
    "                if len(list(merged_dict.keys())) == 1:\n",
    "                    print(f\"Datasets '{merg_model_name}' ('{merg_model_table_id}') and '{dataset_name}' ('{table_id}') are merged to 'merged_dict' with key '{source_id}'.\")\n",
    "                else:\n",
    "                    print(f\"Datasets '{dataset_name}' ('{table_id}') is merged with 'merged_dict'.\")\n",
    "\n",
    "        else:\n",
    "            merged_dict[source_id] = dataset\n",
    "            print(f\"Dataset '{dataset_name}' ('{table_id}') is saved in 'merged_dict'.\")\n",
    "\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19c17e8-79cd-480f-9a40-e744dcdb4ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def daily_to_monthly(ds_dict_daily, ds_dict_merged=1):\n",
    "    \"\"\"\n",
    "    Compute mothly data from daily data and merge the variable with the rest of the model. Reason for this function is that the TaiESM1 model has no\n",
    "    monthly data for the variable hurs. Can be applied to any model that has a similar issue.\n",
    "\n",
    "    Args:\n",
    "        ds_dict_daily (dict): Dictionary of xarray dataset with missing variable in daily resolution.\n",
    "        ds_dict_merged (dict, optional): Dictionary of xarray datasets with all variables in monthly resolution except of the missing variable. \n",
    "                                            If no dict is passed to the function, simply the monthly mean is calculated. \n",
    "\n",
    "        \n",
    "    Returns:\n",
    "        ds_dict_monthly: A dictionary with dataarrays in monthly resolution.\n",
    "            \n",
    "            or\n",
    "        \n",
    "        ds_dict_all: A merged dictionary with dataarrays of each variable of the respective model.\n",
    "    \"\"\"\n",
    "    ds_dict_monthly = {}\n",
    "    \n",
    "    if ds_dict_merged==1:\n",
    "        for ds_name, ds_data in ds_dict_daily.items():\n",
    "            source_id = ds_data.attrs['source_id']\n",
    "            ds_data = ds_data.sortby('time')\n",
    "            # Compute monthly values\n",
    "            ds_data_mon = ds_data.resample(time='1MS').mean()\n",
    "            # Put in dict\n",
    "            ds_dict_monthly[source_id] = ds_data_mon\n",
    "        \n",
    "        return ds_dict_monthly\n",
    "    \n",
    "    else:\n",
    "        for ds_name, ds_data in ds_dict_daily.items():\n",
    "            source_id = ds_data.attrs['source_id']\n",
    "            ds_data = ds_data.sortby('time')\n",
    "            # Compute monthly values\n",
    "            ds_data_mon = ds_data.resample(time='1MS').mean()\n",
    "            # Replace coordinates of dataset when different to datasets in ds_dict_merged\n",
    "            ds_data_mon = replace_coordinates(ds_dict_merged[source_id], ds_data_mon)\n",
    "            # Put in dict\n",
    "            ds_dict_monthly[source_id] = ds_data_mon\n",
    "\n",
    "        # Merge computed monthly average with rest of model dict \n",
    "        ds_dict_all = {}\n",
    "\n",
    "        for dataset_name, dataset in ds_dict_monthly.items():\n",
    "            with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                ds_dict_all[dataset_name] = xr.merge([dataset, ds_dict_merged[dataset_name]])\n",
    "        \n",
    "        return ds_dict_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcce18ac-93d7-4ba5-95bd-4be4f0981f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(save_file, folder):\n",
    "    \"\"\"\n",
    "    Save files as netCDF.\n",
    "\n",
    "    Args:\n",
    "        savefile (dict or dataset): Dictionary of xarray datasets or dataset.\n",
    "        folder (string): Name of folder data is saved in.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        nc_out: Path were data is saved in.\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(save_file) == dict:\n",
    "        for key in save_file.keys():\n",
    "            ds_in = save_file[key]\n",
    "            filename = f'CMIP.{ds_in.source_id}.{ds_in.experiment_id}.nc'\n",
    "            savepath = f'../data/CMIP6/{ds_in.experiment_id}/{folder}'\n",
    "            nc_out = os.path.join(savepath, filename)\n",
    "            os.makedirs(savepath, exist_ok=True) \n",
    "            if os.path.exists(nc_out):\n",
    "                os.remove(nc_out)\n",
    "                print(f\"File  with path: {nc_out} removed\")\n",
    "            # Save to netcdf file\n",
    "            with dask.config.set(scheduler='threads'):\n",
    "                ds_in.to_netcdf(nc_out)\n",
    "            \n",
    "    elif type(save_file) == xr.core.dataset.Dataset:\n",
    "            filename = f'CMIP.{save_file.source_id}.{save_file.experiment_id}.nc'\n",
    "            savepath = f'../data/CMIP6/{save_file.experiment_id}/{folder}'\n",
    "            nc_out = os.path.join(savepath, filename)\n",
    "            os.makedirs(savepath, exist_ok=True) \n",
    "            if os.path.exists(nc_out):\n",
    "                os.remove(nc_out)\n",
    "                print(f\"File  with path: {nc_out} removed\")\n",
    "            # Save to netcdf file\n",
    "            with dask.config.set(scheduler='threads'):\n",
    "                ds_in.to_netcdf(nc_out)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dimension '{dimension}' specified.\")\n",
    "        \n",
    "    return nc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed51f42-8628-4321-839b-2b18c38c4879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "mypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
