{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05fcb352-aecb-438c-9c68-b8bed66b5882",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CMIP6 Daily to Monthly and merging\n",
    "\n",
    "**Following steps are included in this script:**\n",
    "\n",
    "1. Open file\n",
    "2. Compute monthly mean and merge with rest of the model data\n",
    "3. Save data to netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0108914-eaef-4832-8fbd-2dc16ae7a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Packages ==========\n",
    "import xarray as xr\n",
    "import dask\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aaea42-d3d3-4c6f-8a4d-b7a9785c43cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19c17e8-79cd-480f-9a40-e744dcdb4ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def daily_to_monthly(ds_dict_daily, ds_dict_merged=1):\n",
    "    \"\"\"\n",
    "    Compute mothly data from daily data and merge the variable with the rest of the model. Reason for this function is that the TaiESM1 model has no\n",
    "    monthly data for the variable hurs. Can be applied to any model that has a similar issue.\n",
    "\n",
    "    Args:\n",
    "        ds_dict_daily (dict): Dictionary of xarray dataset with missing variable in daily resolution.\n",
    "        ds_dict_merged (dict, optional): Dictionary of xarray datasets with all variables in monthly resolution except of the missing variable. \n",
    "                                            If no dict is passed to the function, simply the monthly mean is calculated. \n",
    "\n",
    "        \n",
    "    Returns:\n",
    "        ds_dict_monthly: A dictionary with dataarrays in monthly resolution.\n",
    "            \n",
    "            or\n",
    "        \n",
    "        ds_dict_all: A merged dictionary with dataarrays of each variable of the respective model.\n",
    "    \"\"\"\n",
    "    ds_dict_monthly = {}\n",
    "    \n",
    "    if ds_dict_merged==1:\n",
    "        for ds_name, ds_data in ds_dict_daily.items():\n",
    "            source_id = ds_data.attrs['source_id']\n",
    "            ds_data = ds_data.sortby('time')\n",
    "            # Compute monthly values\n",
    "            ds_data_mon = ds_data.resample(time='1MS').mean()\n",
    "            # Put in dict\n",
    "            ds_dict_monthly[source_id] = ds_data_mon\n",
    "        \n",
    "        return ds_dict_monthly\n",
    "    \n",
    "    else:\n",
    "        for ds_name, ds_data in ds_dict_daily.items():\n",
    "            source_id = ds_data.attrs['source_id']\n",
    "            ds_data = ds_data.sortby('time')\n",
    "            # Compute monthly values\n",
    "            ds_data_mon = ds_data.resample(time='1MS').mean()\n",
    "            # Replace coordinates of dataset when different to datasets in ds_dict_merged\n",
    "            ds_data_mon = replace_coordinates(ds_dict_merged[source_id], ds_data_mon)\n",
    "            # Put in dict\n",
    "            ds_dict_monthly[source_id] = ds_data_mon\n",
    "\n",
    "        # Merge computed monthly average with rest of model dict \n",
    "        ds_dict_all = {}\n",
    "\n",
    "        for dataset_name, dataset in ds_dict_monthly.items():\n",
    "            with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                ds_dict_all[dataset_name] = xr.merge([dataset, ds_dict_merged[dataset_name]])\n",
    "        \n",
    "        return ds_dict_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06a2fc-9cbf-4c50-aef1-4db69cf82aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(save_file, folder, save_var=True):\n",
    "    \"\"\"\n",
    "    Save files as netCDF.\n",
    "\n",
    "    Args:\n",
    "        savefile (dict or dataset): Dictionary of xarray datasets or dataset.\n",
    "        folder (string): Name of folder data is saved in.\n",
    "        save_var (boolean): If True, data is saved separately for each variable. If false, one file is saved with all variables.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        nc_out: Path were data is saved in.\n",
    "    \"\"\"\n",
    "    \n",
    "    if save_var:\n",
    "        for key, ds in ds_dict.items():\n",
    "            for var in ds:\n",
    "                # Variable to keep\n",
    "                variable_to_keep = var\n",
    "                dimensions_to_keep = {'time', 'lat', 'lon'}\n",
    "                coordinates_to_keep = {'time', 'lat', 'lon'}\n",
    "\n",
    "                if any('depth' in ds[var].dims for var in ds.variables):\n",
    "                    dimensions_to_keep.add('depth')\n",
    "                    coordinates_to_keep.add('depth')\n",
    "\n",
    "                # Create a new dataset with only the desired variable\n",
    "                ds_var = ds[[variable_to_keep]]\n",
    "\n",
    "                # Keep only the desired dimensions\n",
    "                ds_var = ds_var.isel({dim: slice(None) for dim in dimensions_to_keep.intersection(ds_var.dims)})\n",
    "\n",
    "                # Set the desired coordinates\n",
    "                coords_to_set = set(ds_var.variables).intersection(coordinates_to_keep)\n",
    "                ds_var = ds_var.set_coords(list(coords_to_set))\n",
    "\n",
    "                savepath = f'../../data/CMIP6/{ds_var.experiment_id}/raw/{var}/'\n",
    "                filename = f'CMIP.{ds_var.source_id}.{ds_var.experiment_id}.{var}.nc'\n",
    "                nc_out = os.path.join(savepath, filename)\n",
    "                os.makedirs(savepath, exist_ok=True) \n",
    "                if os.path.exists(nc_out):\n",
    "                        inp = input(f\"Delete old file {filename} (y/n):\")\n",
    "                        if inp.lower() in [\"y\"]:\n",
    "                            os.remove(nc_out)\n",
    "                            print(f\"File with path: {nc_out} removed\")\n",
    "                        else:\n",
    "                            filename = \"temp_file.nc\"\n",
    "                            nc_out = os.path.join(savepath, filename)\n",
    "                            print(f\"Filename change to {filename}\")\n",
    "\n",
    "                # Save to netcdf file\n",
    "                with dask.config.set(scheduler='threads'):\n",
    "                    ds_var.to_netcdf(nc_out)\n",
    "                    print(f\"File with path: {nc_out} saved\")\n",
    "       \n",
    "    else:\n",
    "        for key in save_file.keys():\n",
    "            ds_in = save_file[key]\n",
    "            filename = f'CMIP.{ds_in.source_id}.{ds_in.experiment_id}.nc'\n",
    "            savepath = f'../data/CMIP6/{ds_in.experiment_id}/{folder}'\n",
    "            nc_out = os.path.join(savepath, filename)\n",
    "            os.makedirs(savepath, exist_ok=True) \n",
    "            if os.path.exists(nc_out):\n",
    "                inp = input(f\"Delete old file {filename} (y/n):\")\n",
    "                if inp.lower() in [\"y\"]:\n",
    "                    os.remove(nc_out)\n",
    "                    print(f\"File  with path: {nc_out} removed\")\n",
    "                else:\n",
    "                    filename = \"temp_file.nc\"\n",
    "                    nc_out = os.path.join(savepath, filename)\n",
    "                    print(f\"Filename change to {filename}\")\n",
    "\n",
    "            # Save to netcdf file\n",
    "            with dask.config.set(scheduler='threads'):\n",
    "                ds_in.to_netcdf(nc_out)\n",
    "\n",
    "    return nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b0ffa-a1b7-4e29-b882-28977897f087",
   "metadata": {},
   "source": [
    "### 1. Open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ebbae-c3cd-47b3-a6b3-6f63ff612241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Define period, models and path ==============\n",
    "variable='mrsol'\n",
    "experiment_id = 'historical'\n",
    "source_id = ['AWI-ESM-1-1-LR', 'BCC-CSM2-MR', 'BCC-ESM1', 'CanESM5', 'CESM2-FV2', 'CESM2-WACCM-FV2', 'CESM2-WACCM', 'CESM2', 'CNRM-CM6-1-HR','CNRM-CM6-1', 'IPSL-CM6A-LR', 'NorESM2-MM', 'SAM0-UNICON', 'TaiESM1', 'UKESM1-0-LL'] # 'AWI-ESM-1-1-LR', 'BCC-CSM2-MR', 'BCC-ESM1', 'CanESM5', 'CESM2-FV2', 'CESM2-WACCM-FV2', 'CESM2-WACCM', 'CESM2', 'CNRM-CM6-1-HR','CNRM-CM6-1', 'IPSL-CM6A-LR', 'NorESM2-MM', SAM0-UNICON, 'TaiESM1', 'UKESM1-0-LL'], \n",
    "savepath = f'../../data/CMIP6/{experiment_id}/raw/{variable}/'\n",
    "\n",
    "# ========= Use Dask to parallelize computations ==========\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "# ========= Create a helper function to open the dataset ========\n",
    "def open_dataset(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    return ds\n",
    "\n",
    "# ========= Create dictionary using a dictionary comprehension and Dask =======\n",
    "ds_dict, = dask.compute({model: open_dataset(os.path.join(savepath, f'CMIP.{model}.{experiment_id}.{variable}.nc'))\n",
    "                        for model in source_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012dbcd-f64d-4b98-83e3-e2cf8cfe8b46",
   "metadata": {},
   "source": [
    "### 2. Mean and Merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88579bfa-4e46-41f0-a2c2-99394ef1cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Compute montly mean for daily datasets (optional: merge with rest of the datasets) =============\n",
    "\n",
    "ds_dict_monthly = daily_to_monthly(ds_dict) # optionally include ,ds_dict_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26d5c7-2c86-41f0-b5a8-8a0aba2e91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Merge data if loading all data of one model is not possible ==============\n",
    "# Name of second dictionary ds_dict_\n",
    "\n",
    "# Only use this command if loading at once is not possible\n",
    "\n",
    "#ds_dict[f\"{list(ds_dict_.keys())[0]}_\"]=ds_dict_[list(ds_dict_.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eabf7bf-8463-4c01-b58f-46356c36eea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========== Check dictionary =============\n",
    "print(ds_dict.keys())\n",
    "ds_dict[list(ds_dict.keys())[8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a620a-6586-4352-a9f1-174402595605",
   "metadata": {},
   "source": [
    "### 3. Save data to netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf36f1-63d3-48e4-a66c-d584608a9f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========== Store file and remove any former one ==========\n",
    "nc_out = save_file(ds_dict, folder='raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6a2e8-3e68-464a-a983-1d8bcb2c1316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Check stored file ==============\n",
    "xr.open_dataset(nc_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy3",
   "language": "python",
   "name": "mypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
